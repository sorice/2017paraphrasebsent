{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph Semantic Text Similarity Corpus (PSTS Corpus)\n",
    "\n",
    "## Transforming PlagDet into a Paraphrase Identification Corpus\n",
    "\n",
    "The objetive of this notebook is to describe the process to convert a classic plagiarism detection corpus (sometimes referred to as *text-reuse corpus*) into a fragment-pairs based paraphrase identification corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plagiarism Detection Corpus\n",
    "\n",
    "The original Plagiarism Detection Corpus of PAN-13 has two parts, the train and test sets.\n",
    "They have the following structure:\n",
    "\n",
    "    PAN-13-text-alignment-corpus\n",
    "        pairs                           -> list of text-names-tuple on susp & src to compare\n",
    "        susp/                           -> susp directory containing all suspicious texts\n",
    "        src/                            -> src directory containing all text reuse source files\n",
    "        01-no-plagiarism/               -> a directory containing an XML file per no-plag case in the pairs file\n",
    "        02-no-obfuscation/              -> a directory containing an XML file per copy-paste case in the pairs file\n",
    "        03-random-obfuscation/          -> a directory containing an XML file per random paraphrase case in the pairs file\n",
    "        04-translation-obfuscation/     -> a directory containing an XML file per cross-lingual text-reuse case in the pairs file\n",
    "        05-summary-obfuscation/         -> a directory containing an XML file per paraphrase case of summary type in the pairs file\n",
    "        \n",
    "Here is an example of the XML structure of a case, [suspicious-document00007-source-document00382.xml](files/data/PAN-PC-2013/orig/03-random-obfuscation/suspicious-document00007-source-document00382.xml):\n",
    "\n",
    "<body>\n",
    "<pre style=\"color:#1f1c1b;background-color:#ffffff;\">\n",
    "<b>&lt;document</b><span style=\"color:#006e28;\"> reference=</span><span style=\"color:#aa0000;\">&quot;suspicious-document00007.txt&quot;</span><b>&gt;</b>\n",
    "<b>&lt;feature</b><span style=\"color:#006e28;\"> name=</span><span style=\"color:#aa0000;\">&quot;plagiarism&quot;</span><span style=\"color:#006e28;\"> obfuscation=</span><span style=\"color:#aa0000;\">&quot;random&quot;</span><span style=\"color:#006e28;\"> obfuscation_degree=</span><span style=\"color:#aa0000;\">&quot;0.4694788492120119&quot;</span><span style=\"color:#006e28;\"> source_length=</span><span style=\"color:#aa0000;\">&quot;453&quot;</span><span style=\"color:#006e28;\"> source_offset=</span><span style=\"color:#aa0000;\">&quot;0&quot;</span><span style=\"color:#006e28;\"> source_reference=</span><span style=\"color:#aa0000;\">&quot;source-document00382.txt&quot;</span><span style=\"color:#006e28;\"> this_length=</span><span style=\"color:#aa0000;\">&quot;453&quot;</span><span style=\"color:#006e28;\"> this_offset=</span><span style=\"color:#aa0000;\">&quot;9449&quot;</span><span style=\"color:#006e28;\"> type=</span><span style=\"color:#aa0000;\">&quot;artificial&quot;</span> <b>/&gt;</b>\n",
    "<b>&lt;/document</b><b>&gt;</b>\n",
    "</pre>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this case refers to two documents (suspicious-document00007.txt, source-document00382.txt) and inside each one, to a fragment. After some processing, both fragments of text can be seen. The xml establishes a *paraphrase* type (also *obfuscation* in this corpus), the boundaries (*offset*,*length*) for both documents, a degree of paraphrase and the way in wich this case was generated.\n",
    "\n",
    "**Note:** some XMLs of this corpus may contain more than one pair of fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width='400' cellpadding='4' cellspacing='0'>\n",
       "        <col width='128*'>\n",
       "        <col width='128*'>\n",
       "        <tr valign='top'>\n",
       "            <td width='45*'>\n",
       "                <p><b>Susp</b></p>\n",
       "            </td>\n",
       "            <td width='45*'>\n",
       "                <p><b>Src</b></p>\n",
       "            </td>\n",
       "        </tr>\n",
       "        <tr valign='top'>\n",
       "            <td width='45*'>\n",
       "                <p> Special@ tamu. edu DJ of Regulation storage tissue and the use from Cadet of\n",
       "crop improvement. Hannapel Plant,\n",
       "Miller Marchetti& Park wd (1985): 700-703 plant of Potato Acid Physiol78 Accumulation by wdpark Tuber.\n",
       "Manipulation Protein  Publications list of gibberellic Patents submitted, am, MA JC, wd Park (1998) release in biotechnology and Jacinto, two long rice grain varieties having pubmed processing quality. Mcclung for Plant Variety Protection  </p>\n",
       "            </td>\n",
       "            <td width='45*'>\n",
       "                <p> wdpark@tamu.edu\n",
       "Manipulation of plant storage tissue and the use of biotechnology in crop improvement.\n",
       "Hannapel DJ, Miller JC & Park WD (1985) : 700-703 Regulation of Potato Tuber Protein Accumulation by Gibberellic Acid. Plant Physiol78\n",
       " \n",
       "Publications list from Pubmed\n",
       "Patents\n",
       "McClung, AM, MA Marchetti, WD Park (1998) Release of Cadet and Jacinto, two long grain rice varieties having special processing quality. Submitted for Plant Variety Protection </p>\n",
       "            </td>\n",
       "        </tr>\n",
       "    </table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run scripts/corpusReader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase Identification Corpus\n",
    "\n",
    "A classic corpus of paraphrase identification may contain different structures of paraphrase cases. Usually the structure could be:\n",
    "\n",
    "    id class sentence-1 sentence-2\n",
    "    \n",
    "And the class could be equal to *0* or *1*, which means *non-paraphrase* and *paraphrase*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Problem of Plagiarism to Paraphrase Corpus Transformation\n",
    "\n",
    "Broadly speaking, in a plagiarism detection issue you must detect (or extract) the two fragments (suspicious and source) by using some approaches (citation measures, word fingerprints, ngrams, etc.); the problem is to find the boundaries of both fragments that are usually inside large documents.\n",
    "\n",
    "But, in a paraphrase detection problem it must be detected if two sentences are paraphrased or not, and a very common technique here is to convert the original structure of a case into a machine learning object: a vector of features based on the *$original_{sentence}$*, the *$paraphased_{sentence}$* and the class _paraphrased/non-paraphrased_ .\n",
    "\n",
    "As you can see on linguist international investigations on paraphrase, there is a wide range of definitions, for that reason we would like to define a concept:\n",
    "\n",
    "**Paraphrase Definition:** *$class = 1$ (paraphrased) if there is some kind of transformation maintaining a high semantic similarity degree [<a href=\"#Vila2014\" title=\"Is This a Paraphrase ? What Kind ? Paraphrase Boundaries and Typology\"> (Vila2014, p. 6)</a>](#Vila2014), and $class = 0$ (non-paraphrased)if both text are dissimilar even if they speak about the same semantic field but differs on meanning in some degree.*\n",
    "\n",
    "After normalization evaluation (see the resultant structure in [Normalization-Alignment-Quality Notebook](02.3-Eval-Normalization-Alignment-Quality.ipynb)) the purpose of this pipeline's step is to obtain a corpus with the following structure:\n",
    "<p><font color='#F84825'>\n",
    " $(case_{id}, text_{fragment_{1}}, text_{fragment_{2}}, binary\\,class)$\n",
    "</font>\n",
    "<p>Then in the next notebooks <font color='#F84825'>this structure</font> will be used to get a data feature vector representation to apply in machine learning.\n",
    "\n",
    "Reminding previous generated structures:\n",
    "\n",
    "* Output structure after alignment subprocess:\n",
    "\n",
    "$(id_K,normalized-sentence_K,original\\,offset_{sentence\\,K},original\\,offset+length_{sentence\\,K})$\n",
    "\n",
    "* Output structure after quality norm subprocess:\n",
    "\n",
    "$(id_{sentence_P\\,susp},offset_{sentence_P},offset+length_{sentence_P},\\%\\,sentence_{P}\\, \\in\\,susp_{fragment\\,X},id_{fragment\\,X})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A New Paraphrase Identification Corpus at Fragment Level\n",
    "\n",
    "### Generating TRUE Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e77a5b32d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPANXml_Reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxmlColecctionPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/orig/xml/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/DATA/Repo/dynwit/01b_paraph_sts_ntb/scripts/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcorpusReader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPANXml_Reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmsrpc_to_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsrpc_to_arff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_msrpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparallel_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/DATA/Repo/dynwit/01b_paraph_sts_ntb/scripts/datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNonBLASDotWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpkg_resources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# setuptools not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_call_aside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3239\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_master_working_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3240\u001b[0m     \"\"\"\n\u001b[1;32m   3241\u001b[0m     \u001b[0mPrepare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmaster\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_call_aside\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3220\u001b[0m \u001b[0;31m# from jaraco.functools 1.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_aside\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3223\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_initialize_master_working_set\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3262\u001b[0m     \u001b[0;31m# (e.g. by calling ``require()``) will get activated as well,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m     \u001b[0;31m# with higher priority (replace=True).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3264\u001b[0;31m     tuple(\n\u001b[0m\u001b[1;32m   3265\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3266\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3263\u001b[0m     \u001b[0;31m# with higher priority (replace=True).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m     tuple(\n\u001b[0;32m-> 3265\u001b[0;31m         \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3266\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3267\u001b[0m     )\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mactivate\u001b[0;34m(self, path, replace)\u001b[0m\n\u001b[1;32m   2776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2777\u001b[0m             \u001b[0mfixup_namespace_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2778\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'namespace_packages.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2779\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m                     \u001b[0mdeclare_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2761\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2762\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2763\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mhas_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metadata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vnlp/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_has\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_isdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scripts import PANXml_Reader\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "origCollectionPath = 'data/orig/'\n",
    "\n",
    "timei = time.time()\n",
    "with open('data/aligned/aligned_pairs') as casePairs:\n",
    "    for docs in casePairs:\n",
    "        #print(docs)\n",
    "        susp, src = docs.split()\n",
    "        #print(susp)\n",
    "        xmlDoc = PANXml_Reader(xmlColecctionPath+susp[:-4]+'-'+src[:-4]+'.xml')\n",
    "        fragmentList = xmlDoc.parser()\n",
    "        newCase = {}\n",
    "        \n",
    "        #Analyse the fragment pairs list in the xml case\n",
    "        if fragmentList != []: #this line filter non-paraphrased XML\n",
    "            \n",
    "            #Load Quality Matrix per case\n",
    "            QM = pd.read_csv(alignedCollectionPath+'quality/'+susp+' '+src,\n",
    "                           names=['sentID','offset','length','percent','FragID'],\n",
    "                           delimiter = '\\t')\n",
    "            \n",
    "            for id, frag in enumerate(fragmentList):\n",
    "                text = {'susp/':'','src/':''}\n",
    "\n",
    "                #For every doc in the pair\n",
    "                for doc,file_type in zip([susp,src],['susp/','src/']):\n",
    "                    targetID = int(str(id+1)+doc[-9:-4])\n",
    "                    docText = open(origCollectionPath+file_type+doc)\n",
    "                    offsetf = len(docText.read())\n",
    "                    docText.close()\n",
    "                    lenf = 0\n",
    "\n",
    "                    #Load aligned matrix for doc\n",
    "                    AM = pd.read_csv(alignedCollectionPath+file_type+doc,\n",
    "                                     names=['id','sent','offset','length'], \n",
    "                                     sep='\\t')\n",
    "                    \n",
    "                    #Join correspondent aligned sentences in a single fragment\n",
    "                    for idx in QM.index:\n",
    "                        if QM.FragID[idx] == targetID:\n",
    "                            offsetf = min(offsetf,QM.offset[idx])\n",
    "                            lenf = max(lenf,QM.length[idx])\n",
    "                            #print(QM.sentID[idx])\n",
    "                            #print(AM.sent[QM.sentID[idx]])\n",
    "                            text[file_type] +=  AM.sent[QM.sentID[idx]]+' '\n",
    "\n",
    "                #Take both created fragment per doc and create a pair fragment case\n",
    "                newCaseID = str(id+1)+susp[-9:-4]+src[-9:-4]\n",
    "                \n",
    "                caseClass = 1\n",
    "                newCase[newCaseID] = ''.join([str(newCaseID),'\\t',text['susp/']+'\\t',\n",
    "                                        text['src/']+'\\t',str(caseClass)+'\\t',\n",
    "                                        frag.suspOffset+'\\t',frag.suspLength+'\\t',\n",
    "                                        frag.srcOffset+'\\t',frag.srcLength+'\\n'])\n",
    "                                        \n",
    "            \n",
    "            #Write the positive cases corpus\n",
    "            paraphCorpus = open('data/true_pairs','a')\n",
    "            for value in newCase.values():\n",
    "                paraphCorpus.write(value)\n",
    "            paraphCorpus.close()\n",
    "print('Total time:', time.time() - timei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Non-Paraphrased Cases Problem\n",
    "\n",
    "The last difficult problem to describe is the emptyness of *non-plagiarism* XML cases (contained in the *data/PAN-PC-2013/orig/01-no-plagiarism/* folder).  Those XMLs have an empty structure, only through the xml file's name you can figure out which texts don't have similarities (See the [suspicious-document00017-source-document00534.xml](files/data/PAN-PC-2013/orig/01-no-plagiarism/suspicious-document00017-source-document00534.xml) example below). How to solve that? \n",
    "\n",
    "<body>\n",
    "<pre style='color:#1f1c1b;background-color:#ffffff;'>\n",
    "<b>&lt;document</b><span style='color:#006e28;'> reference=</span><span style='color:#aa0000;'>&quot;suspicious-document00017.txt&quot;</span><b>&gt;</b>\n",
    "<b>&lt;/document</b><b>&gt;</b>\n",
    "</pre>\n",
    "</body>\n",
    "\n",
    "Once we have both dissimilar texts, we must select two fragments with some shallow properties similar to the positive cases. Why must they share some properties? (E.g. close vocabulary) Because these could help to identify the features with deep semantic similarity identification capacities in the following phases. Regarding machine learning problems modeling properties, a not balanced corpus is proposed, with a 66% of non-paraphrased cases.\n",
    "\n",
    "__Note__: Another approach of *non-paraphrased cases* could be the use of a set of copy-paste cases (similar pairs of text but not paraphrased). For this alternative analysis, or related, the author proposes a set of experiments described in a special notebook not contained in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of Non-Paraphrased Cases Generator Algorithm\n",
    "\n",
    "The list of non paraphrased pair of docs.\n",
    "\n",
    "    data/aligned/false_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slow Generation of Non-Paraphrase Cases Collection\n",
    "\n",
    "This is a solution that consumes a lot of RAM and computing time. It is based on pre-calculating 'all' similarity scores between 'all' possible fragments in every document (joining all consecutive sentences). At the very end this is a misconception of what is right or wrong to avoid some influences from the experiment design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: File `'scripts/02.5_nonParaphrasedCasesGenerationj.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run scripts/02.4_nonParaphrasedCasesGeneration.py \n",
    "                data/PAN-PC-2013/aligned/FALSE_paraph_aligned_pairs \n",
    "                data/PAN-FPC-2017/PAN-True-Paraphrase-Corpus \n",
    "                data/PAN-PC-2013/aligned/susp/ \n",
    "                data/PAN-PC-2013/aligned/src \n",
    "                data/PAN-FPC-2017/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Fast-Generation of Non-Paraphrase Cases Collection\n",
    "\n",
    "This variant is less complex:\n",
    "- Take all false pairs xml files\n",
    "- Select a random true pair\n",
    "- Get two fragments of similar length (%10 of diff)\n",
    "- Write the texts on the PAN-None-Paraphrase-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aligned_text(csv_file):\n",
    "    return pd.read_csv(csv_file,\n",
    "                       names=['id','sent','offset','length'], \n",
    "                       sep='\\t')\n",
    "\n",
    "def get_aligned_frag(csv_file,offset,length):\n",
    "    aligned = read_aligned_text(csv_file)\n",
    "    condition = False\n",
    "    text_result = ''\n",
    "    rlength = 0; roffset = 0\n",
    "    for idx in aligned.index:\n",
    "        if offset >= aligned.offset[idx] \\\n",
    "        and offset < aligned.offset[idx] + aligned.length[idx]:\n",
    "            roffset = aligned.offset[idx]\n",
    "            condition = True\n",
    "        if offset+length < aligned.offset[idx] and condition:\n",
    "            rlength = aligned.offset[idx]-1-roffset\n",
    "            condition = False\n",
    "        if condition == True:\n",
    "            text_result += ''.join(aligned.sent[idx])\n",
    "    return text_result, roffset, rlength\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed cases:  1000\n",
      "Preprocessed cases:  2000\n",
      "Finish-----added:  2991 false cases\n",
      "Total time: 17.078004837036133\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from os.path import isfile\n",
    "\n",
    "dataPath = 'data/aligned/' \n",
    "falseCases = []\n",
    "classValue = '0'\n",
    "\n",
    "timei = time.time()\n",
    "trueCases = pd.read_csv('data/true_pairs',\n",
    "                       names=['id','susp','src','clase','suspOffset','suspLen','srcOffset','srcLen'], \n",
    "                       sep='\\t')\n",
    "\n",
    "pairs = range(len(trueCases))  \n",
    "count=0\n",
    "        \n",
    "with open('data/orig/false_pairs') as falsePairs:\n",
    "    for line in falsePairs:\n",
    "        falseSusp,falseSrc = line.split()\n",
    "        falseSuspText = open('data/norm/susp/'+falseSusp).read()\n",
    "        falseSrcText = open('data/norm/src/'+falseSrc).read()\n",
    "        false_frags = 1\n",
    "        \n",
    "        while(false_frags < 4):#get 3 diff fragm for every true choiced\n",
    "            i = choice(pairs)#get one random true pair\n",
    "            suspLen = int(trueCases.suspLen[i])\n",
    "            srcLen = int(trueCases.srcLen[i])\n",
    "            \n",
    "            #check if false text lengths are grader than trueCase len\n",
    "            if len(falseSuspText) > suspLen and len(falseSrcText)> srcLen:\n",
    "\n",
    "                #get random fragment inside the false susp text\n",
    "                falseSuspOffset = choice(range(len(falseSuspText)-suspLen))\n",
    "                falseSuspLen = choice(range(falseSuspOffset+int(suspLen*0.7),\n",
    "                                            falseSuspOffset+int(suspLen*1.3)))-falseSuspOffset\n",
    "                \n",
    "                #get random fragment inside the false src text\n",
    "                falseSrcOffset = choice(range(len(falseSrcText)-srcLen))\n",
    "                falseSrcLen = choice(range(falseSrcOffset+int(srcLen*0.7),\n",
    "                                           falseSrcOffset+int(srcLen*1.3)))-falseSrcOffset\n",
    "                \n",
    "                #get the current false pairs texts\n",
    "                suspFragText,falseSuspOffset,falseSuspLen = get_aligned_frag('data/aligned/susp/'+\n",
    "                                                                             falseSusp,\n",
    "                                                                             falseSuspOffset,\n",
    "                                                                             falseSuspLen)\n",
    "                srcFragText, falseSrcOffset,falseSrcLen = get_aligned_frag('data/aligned/src/'+\n",
    "                                                                           falseSrc,\n",
    "                                                                           falseSrcOffset,\n",
    "                                                                           falseSrcLen)\n",
    "            \n",
    "                #Make the tuple:\n",
    "                #Take both created fragment per doc and create a pair fragment case\n",
    "                caseID = str(false_frags)+falseSusp[-9:-4]+falseSrc[-9:-4]\n",
    "                falseCases.append(tuple((caseID,\n",
    "                                        suspFragText,srcFragText,\n",
    "                                        classValue,\n",
    "                                        str(falseSuspOffset),str(falseSuspLen),\n",
    "                                        str(falseSrcOffset),str(falseSrcLen))))\n",
    "                false_frags += 1\n",
    "                count+=1\n",
    "                if count%1000 == 0:\n",
    "                    print('Preprocessed cases: ',count)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    with open('data/false_pairs','w') as falsePairs:\n",
    "        for C in falseCases:\n",
    "                falsePairs.write(C[0]+'\\t'+C[1]+'\\t'+C[2]+'\\t'+C[3]+'\\t'+C[4]+'\\t'+C[5]+'\\t'+C[6]+'\\t'+C[7]+'\\n')\n",
    "    print('Finish-----added: ', len(falseCases), 'false cases')\n",
    "print('Total time:', time.time() - timei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating both parts of the Corpus\n",
    "\n",
    "**Note**: Check the corpus or *PAN-Paraphrase-Corpus* file visually, if it's empty then run this code, else just take it and use it, or generate the corpus in a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/PSTSCorpus', 'a') as Corpus:\n",
    "    Corpus.write('id\\tsent1\\tsent2\\tclass\\toffsetSusp\\tlenSusp\\toffsetSrc\\tlenSrc\\n') #inserting first row for further uses\n",
    "    with open('data/false_pairs') as noneCorpus:\n",
    "        for case in noneCorpus:\n",
    "            Corpus.write(case)\n",
    "    with open('data/true_pairs') as trueCorpus:\n",
    "        for case in trueCorpus:\n",
    "            Corpus.write(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Text Similarity Problems\n",
    "\n",
    "Text similarity is a popular field of investigation with many problems very close in meaning but very different in fact. For a better understanding of this notebook, it is shown a short background on the main problems of this area as well as a short definition.\n",
    "\n",
    "- __Semantic Text Similarity__: Given two sentences you must calculate the degree of similarity and classify them. Usually this is a multi-class problem with 6 classes.\n",
    "- __Textual Entailment__: Identify if two texts are related in one direction (A implicates B).\n",
    "- __Text Similarity__: Given two text fragments you must identify if they are semantically related in both directions.\n",
    "- __Text Alignment__: Given two different texts you must match every sentence in text A with its corresponding sentence in text B.\n",
    "- __Paraphrase Identification__: Given two sentences you must classify if they are paraphrased or not (binary classification).\n",
    "- __Text Reuse__: Detect reused fragments in a single text having a text collection as source.\n",
    "- __Plagiarism Detection__ (_Text Reuse + Citation Analysis_): Detect in a text collection pairs of non-quoted fragments with the same meaning.\n",
    "- __Machine Translation__: Align text pairs with same meaning but in a different language.\n",
    "        \n",
    "So the approach presented in this tutorial is a *Text Similarity* problem seen from the perspective of a *Paraphrase Identification* problem.\n",
    "\n",
    "### Corpus of Text Reuse\n",
    "\n",
    "PAN-PC / TNLP / Plagiarism Corpus / \n",
    "\n",
    "### Corpus of Paraphrase Identification\n",
    "\n",
    "MSRPC / STS /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The main objective of this notebook was accomplished:\n",
    "\n",
    "    \"After having the aligned normalized-texts, a new paraphrase corpus (binary cathegory) was generated, based on  chunks extracted from the xmls of PAN-PC corpus.\"\n",
    "    \n",
    "The true cases are generated in the first place. This part of the process is simple and fast, because chunk information is full contained in the xmls of PAN-PC corpus.\n",
    "\n",
    "However, the first version of non paraphrased cases (or false cases) must be constructed mathematically due to the lack of information of non-paraphrased xmls of PAN-PC corpus. The second version constructs almost 3 thousand cases based on random selections of offsets and lens of true pairs. This second version is faster and generates more credible cases.\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "For future experiments, the best way to accomplish this task is to generate non-paraphrased pair of texts manually; that is, humanly designed.\n",
    "\n",
    "The final proposition of this corpus must be to clarify if the selection of fragments is larger than a sentence, is more suitable or has anything to add to the process of plagiarism detection. The possible conclusions after all the machine learning experimentation are:\n",
    "\n",
    "- Paraphrase Detection phase algorithms get almost perfect accuracy results when they have long data to compare. This fact makes us conclude that the _Search Space Reduction_ stage is more important, because it is responsible for defining the offset and length of reused fragments.\n",
    "- Long reused fragments help with paraphrase because the behavior of the detection changes when paraphrase type changes, this is only possible with 3 or more classes of paraphrase inside _PSTSCorpus_. The recommendation for future experiments is to use the strategy of corpus _Plagiarised_Short_Answers_ (based on 4 degrees of rewriting), or to get a derived classification corpus similar to P4P corpus (which offers more cathegories based on linguistic phenomenon of the change: lexical, same polarity, addition-deletion, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "* Analyze the _function_ __getFeatureVector__ and test with other mathematical equations. Make only 100 new cases and analyze the result against the previous one.\n",
    "* Make a parallel version of the algorithm for the generation of non-paraphrased cases.\n",
    "* Analyze the possibility to have a multi class corpus based on Verbatim/Paraphrased/Non-paraphrased cases, taking into acount that every kind of similarity measure will have a high score in both Verbatim & Paraphrased cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "* Vila, Marta & Martí, M Antònia & Rodríguez, Horacio \"Is This a Paraphrase ? What Kind ? Paraphrase Boundaries and Typology\". Open Journal of Modern Linguistics, 2014.\n",
    "<a id='Vila2014'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
