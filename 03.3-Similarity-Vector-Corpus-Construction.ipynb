{
 "metadata": {
  "name": "",
  "signature": "sha256:eab8418c8801b3045da5f20760cac2843504bce0ee6be3484c4230fbd4b90a34"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for Paper *Paraphrase Beyond Sentence*. Source and license info is on [GitHub](https://github.com/sorice/2017paraphrasebsent/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Constructing a Corpus of Similarity Objects\n",
      "\n",
      "After having a good set of text similarity functions, the next urgent resource is the corpus of similarity objects. The next set of cells implement the generation of a corpus of similarity vectors. This cells have been converted into script module functions for future uses in the following notebooks, but as the main objective of this collection is to teach, we let the first successfull implementation for student questions and quiz implementations.\n",
      "\n",
      "**Note:** The above mention functions are *script.datasets.msrpc_to_csv*,*script.datasets.msrpc_to_arff*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('/home/abelm')\n",
      "\n",
      "import pandas as pd\n",
      "from pandas import DataFrame, Series, read_table\n",
      "import textsim\n",
      "from textsim.utils import calc_all\n",
      "import arff\n",
      "import time\n",
      "\n",
      "#Read Paraphrase Corpus\n",
      "df = read_table('data/MSRPC-2004/msr_paraphrase_test.txt',sep='\\t')\n",
      "data = []\n",
      "distances = []\n",
      "exceptions = []\n",
      "ti = time.time()\n",
      "#Open vector similarity feature corpus\n",
      "with open('data/MSRPC-2004/msrpc_test_textsim-42fb.arff','w') as corpus: \n",
      "    corpus.write('@relation paraphrase\\n\\n')\n",
      "    for distance in sorted(textsim.__all_distances__.keys()):\n",
      "        corpus.write('@attribute '+distance+' numeric\\n')\n",
      "        distances.append(distance)\n",
      "    corpus.write('@attribute '+'id'+' integer\\n')\n",
      "    distances.append('id')\n",
      "    corpus.write('@attribute class {yes,no}\\n\\n')\n",
      "    distances.append('class')\n",
      "    corpus.write('@data\\n')\n",
      "    \n",
      "    for row in range(len(df)):\n",
      "        clase, ide1, ide2, sent1, sent2 = df.xs(row)\n",
      "        try:\n",
      "            obj = calc_all(sent1,sent2)[2:]\n",
      "            sec = ''\n",
      "            for item in obj:\n",
      "                if item == 'nan':\n",
      "                    sec += '?,'\n",
      "                else:\n",
      "                    sec += str(item)+','\n",
      "            sec += str(row) #append id for future analysis after classification\n",
      "            obj.append(row)\n",
      "            if clase:\n",
      "                corpus.write(sec+',yes\\n')\n",
      "                obj.append('yes')\n",
      "            else:\n",
      "                corpus.write(sec+',no\\n')\n",
      "                obj.append('no')\n",
      "            data.append(obj)\n",
      "            \n",
      "        except:\n",
      "            exceptions.append(row)\n",
      "\n",
      "    #arff.dump('data/MSRPC-2004/result.arff',data, relation='paraphrase',names=distances)\n",
      "tf = time.time()-ti\n",
      "print('Total time:',tf)\n",
      "print('Exceptions:',len(exceptions),'\\nValues:',exceptions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total time: 415.9744861125946\n",
        "Exceptions: 54 \n",
        "Values: [16, 35, 48, 143, 176, 217, 228, 251, 279, 281, 322, 378, 408, 429, 430, 446, 475, 525, 572, 576, 588, 662, 697, 701, 731, 745, 766, 773, 824, 843, 897, 924, 953, 1003, 1014, 1019, 1039, 1059, 1077, 1151, 1186, 1209, 1216, 1236, 1413, 1425, 1467, 1470, 1527, 1607, 1663, 1666, 1677, 1717]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Runs*\n",
      "\n",
      "1. time = 379.49773502349854, valid_data = 1671\n",
      "2. time = 381, valid_data = 1671\n",
      "3. time = 415.97, valid_data = 1671 (last version that generate a correct MSRP ARFF file)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## CSV Data Generation\n",
      "\n",
      "Sklearn frecuently works with CSV format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def corpus_to_csv(file_path='data/MSRPC-2004/msrpc_test_textsim-42f.csv'):\n",
      "    with open(file_path,'w') as corpus: #Open vector similarity feature corpus\n",
      "        corpus.write(str(len(data))+',')\n",
      "        corpus.write(str(len(distances)-1)+',')\n",
      "        corpus.write('Paraph,Non,')\n",
      "        for distance in distances:\n",
      "            corpus.write(distance+',')\n",
      "        corpus.write('\\n')\n",
      "        for instance in data:\n",
      "            corpus.write(str(instance)[1:-1]+'\\n')\n",
      "    return\n",
      "            \n",
      "corpus_to_csv()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Paralel version of Corpus construction code\n",
      "\n",
      "*Unfinished*\n",
      "(Pendiente, pues no funcion\u00f3 imprimir datos, o comunicarse con el proceso en paralelo, solo hay forma de probarlo usando un set de datos peque\u00f1os). Terminar esto con el ejemplo de la biblioteca tqdm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import parallel\n",
      "clients = parallel.Client()\n",
      "views = clients.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import textsim\n",
      "from textsim.utils import calc_all\n",
      "import arff\n",
      "import time\n",
      "import csv\n",
      "\n",
      "rows = list(range(len(df)))\n",
      "\n",
      "def calc_vec(row):\n",
      "    ide, sent1, sent2, clase = row\n",
      "    print(str(ide)[:-10])\n",
      "    try:\n",
      "        vec = calc_all(sent1,sent2)\n",
      "        vec.append(clase)\n",
      "    except:\n",
      "        print(ide, sent1, sent2, clase)\n",
      "    #count+=1\n",
      "    #if count in inter:\n",
      "    #    print(count)\n",
      "    return vec[2:]\n",
      "\n",
      "#Read Paraphrase Corpus\n",
      "data = []\n",
      "distances = []\n",
      "inter = [1,10,50,100,200,500,1000,1500,2000]\n",
      "count = 0\n",
      "\n",
      "ti = time.time()\n",
      "with open('../PAN-Sim-Vec-Corpus','a') as corpus: #Open vector similarity feature corpus\n",
      "    corpus.write('@relation paraphrase')\n",
      "    for distance in textsim.__all_distances__.keys():\n",
      "        corpus.write('@attribute '+distance)\n",
      "        distances.append(distance)\n",
      "    with open('../PAN-Paraphrase-Corpus',newline='') as csvfile:\n",
      "        casereader = csv.reader(csvfile, delimiter='\\t')\n",
      "        data = list(views.map(calc_vec, casereader))    \n",
      "    arff.dump('result.arff',data, relation='paraphrase',names=distances)\n",
      "tf = time.time()-ti\n",
      "print(tf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Cython Version of Corpus construction code\n",
      "\n",
      "In this case the cython implementation of distances must be inside the library textsim like in **nlpnet** package."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}