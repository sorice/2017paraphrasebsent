{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for Paper *Paraphrase Beyond Sentence*. Source and license info is on [GitHub](https://github.com/sorice/2017paraphrasebsent/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Corpus of Similarity Objects\n",
    "\n",
    "After having a good set of text similarity functions, the next urgent resource is the corpus of similarity objects to use into machine learning sklearn python library. Most frecuently formats inside sklearn are csv and arff. ARFF formats are useful to use in Weka software to compare for example the 10 most important features extracted in python platform versus java platform, also compare times.\n",
    "\n",
    "The next set of cells implement the generation of a corpus of similarity vectors. This cells have been converted into script module functions for future uses in the following notebooks, but as the main objective of this collection is to teach, we let the first successfull implementation for student questions and quiz implementations.\n",
    "\n",
    "**Note:** The above mention functions are *script.datasets.msrpc_to_csv*,*script.datasets.msrpc_to_arff*.\n",
    "\n",
    "**Note:** Textsim package can be download from [github](https://github.com/sorice/textsim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1725/1725 [06:41<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 401.63975954055786\n",
      "Exceptions: 54 \n",
      "Values: [16, 35, 48, 143, 176, 217, 228, 251, 279, 281, 322, 378, 408, 429, 430, 446, 475, 525, 572, 576, 588, 662, 697, 701, 731, 745, 766, 773, 824, 843, 897, 924, 953, 1003, 1014, 1019, 1039, 1059, 1077, 1151, 1186, 1209, 1216, 1236, 1413, 1425, 1467, 1470, 1527, 1607, 1663, 1666, 1677, 1717]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/abelm')\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series, read_table\n",
    "import textsim\n",
    "from textsim.utils import calc_all\n",
    "import arff\n",
    "import time\n",
    "\n",
    "#Read Paraphrase Corpus\n",
    "df = read_table('data/MSRPC-2004/msr_paraphrase_test.txt',sep='\\t')\n",
    "data = []\n",
    "distances = []\n",
    "exceptions = []\n",
    "ti = time.time()\n",
    "#Open vector similarity feature corpus\n",
    "with open('data/MSRPC-2004/msrpc_test_textsim-42fb.arff','w') as corpus: \n",
    "    corpus.write('@relation paraphrase\\n\\n')\n",
    "    for distance in sorted(textsim.__all_distances__.keys()):\n",
    "        corpus.write('@attribute '+distance+' numeric\\n')\n",
    "        distances.append(distance)\n",
    "    corpus.write('@attribute '+'id'+' integer\\n')\n",
    "    distances.append('id')\n",
    "    corpus.write('@attribute class {yes,no}\\n\\n')\n",
    "    distances.append('class')\n",
    "    corpus.write('@data\\n')\n",
    "    \n",
    "    for row in tqdm(range(len(df))):\n",
    "        clase, ide1, ide2, sent1, sent2 = df.xs(row)\n",
    "        try:\n",
    "            obj = calc_all(sent1,sent2)[2:]\n",
    "            sec = ''\n",
    "            for item in obj:\n",
    "                if str(item) == 'nan':\n",
    "                    sec += '?,'\n",
    "                else:\n",
    "                    sec += str(item)+','\n",
    "            sec += str(row) #append id for future analysis after classification\n",
    "            obj.append(row)\n",
    "            if clase:\n",
    "                corpus.write(sec+',yes\\n')\n",
    "                obj.append('yes')\n",
    "            else:\n",
    "                corpus.write(sec+',no\\n')\n",
    "                obj.append('no')\n",
    "            data.append(obj)\n",
    "            \n",
    "        except:\n",
    "            exceptions.append(row)\n",
    "\n",
    "tf = time.time()-ti\n",
    "print('Total time:',tf)\n",
    "print('Exceptions:',len(exceptions),'\\nValues:',exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Runs*\n",
    "\n",
    "1. time = 379.49773502349854, valid_data = 1671\n",
    "2. time = 381, valid_data = 1671\n",
    "3. time = 415.97, valid_data = 1671 (last version that generate a correct MSRP ARFF file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Data Generation\n",
    "\n",
    "Sklearn frecuently works with CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_csv(file_path='data/MSRPC-2004/msrpc_test_textsim-42f.csv'):\n",
    "    with open(file_path,'w') as corpus: #Open vector similarity feature corpus\n",
    "        corpus.write(str(len(data))+',')\n",
    "        corpus.write(str(len(distances)-1)+',')\n",
    "        corpus.write('Paraph,Non,')\n",
    "        for distance in distances:\n",
    "            corpus.write(distance+',')\n",
    "        corpus.write('\\n')\n",
    "        for instance in data:\n",
    "            corpus.write(str(instance)[1:-1]+'\\n')\n",
    "    return\n",
    "            \n",
    "corpus_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paralel version of Corpus construction code\n",
    "\n",
    "Code of parallel_process funct geted from [Dans Shiebler Blog](http://danshiebler.com/2016-09-14-parallel-progress-bar/).\n",
    "This version code only generates ARFF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/abelm')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series, read_table\n",
    "import time\n",
    "import textsim\n",
    "from textsim.utils import calc_all\n",
    "\n",
    "def parallel_process(array, function, n_jobs=4, use_kwargs=False, front_num=3):\n",
    "    \"\"\"\n",
    "        A parallel version of the map function with a progress bar. \n",
    "\n",
    "        Args:\n",
    "            array (array-like): An array to iterate over.\n",
    "            function (function): A python function to apply to the elements of array\n",
    "            n_jobs (int, default=16): The number of cores to use\n",
    "            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of \n",
    "                keyword arguments to function \n",
    "            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job. \n",
    "                Useful for catching bugs\n",
    "        Returns:\n",
    "            [function(array[0]), function(array[1]), ...]\n",
    "    \"\"\"\n",
    "    #We run the first few iterations serially to catch bugs\n",
    "    if front_num > 0:\n",
    "        front = [function(**a) if use_kwargs else function(a) for a in array[:front_num]]\n",
    "    #If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.\n",
    "    if n_jobs==1:\n",
    "        return front + [function(**a) if use_kwargs else function(a) for a in tqdm(array[front_num:])]\n",
    "    #Assemble the workers\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as pool:\n",
    "        #Pass the elements of array into function\n",
    "        if use_kwargs:\n",
    "            futures = [pool.submit(function, **a) for a in array[front_num:]]\n",
    "        else:\n",
    "            futures = [pool.submit(function, a) for a in array[front_num:]]\n",
    "        kwargs = {\n",
    "            'total': len(futures),\n",
    "            'unit': 'it',\n",
    "            'unit_scale': True,\n",
    "            'leave': True\n",
    "        }\n",
    "        #Print out the progress as tasks complete\n",
    "        for f in tqdm(as_completed(futures), **kwargs):\n",
    "            pass\n",
    "    out = []\n",
    "    #Get the results from the futures. \n",
    "    for i, future in tqdm(enumerate(futures)):\n",
    "        try:\n",
    "            out.append(future.result())\n",
    "        except Exception as e:\n",
    "            out.append(e)\n",
    "    return front + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.72K/1.72K [03:58<00:00, 6.37it/s]\n",
      "1722it [00:00, 237898.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 240.59578776359558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def return_obj(row, sent1, sent2, clase):\n",
    "    try:\n",
    "        obj = calc_all(sent1,sent2)[2:]\n",
    "        sec = ''\n",
    "        for item in obj:\n",
    "            if str(item) == 'nan':\n",
    "                sec += '?,'\n",
    "            else:\n",
    "                sec += str(item)+','\n",
    "        sec += str(row) #append id for future analysis after classification\n",
    "        if clase:\n",
    "            with open('data/MSRPC-2004/msrpc_test_textsim-42fb.arff','a') as corpus: \n",
    "                corpus.write(sec+',yes\\n')\n",
    "        else:\n",
    "            with open('data/MSRPC-2004/msrpc_test_textsim-42fb.arff','a') as corpus: \n",
    "                corpus.write(sec+',no\\n')\n",
    "\n",
    "    except:\n",
    "        return row\n",
    "\n",
    "#Read Paraphrase Corpus\n",
    "df = read_table('data/MSRPC-2004/msr_paraphrase_test.txt',sep='\\t')\n",
    "\n",
    "distances = []\n",
    "exceptions = []\n",
    "ti = time.time()\n",
    "#Open vector similarity feature corpus\n",
    "with open('data/MSRPC-2004/msrpc_test_textsim-42fb.arff','w') as corpus: \n",
    "    corpus.write('@relation paraphrase\\n\\n')\n",
    "    for distance in sorted(textsim.__all_distances__.keys()):\n",
    "        corpus.write('@attribute '+distance+' numeric\\n')\n",
    "        distances.append(distance)\n",
    "    corpus.write('@attribute '+'id'+' integer\\n')\n",
    "    distances.append('id')\n",
    "    corpus.write('@attribute class {yes,no}\\n\\n')\n",
    "    distances.append('class')\n",
    "    corpus.write('@data\\n')\n",
    "\n",
    "#Parallel trick for this problem\n",
    "arr = [{'row':i, 'sent1':df.xs(i)[3], 'sent2':df.xs(i)[4], 'clase':df.xs(i)[0]} for i in range(len(df))]\n",
    "exceptions = parallel_process(arr, return_obj, use_kwargs=True)\n",
    "    \n",
    "tf = time.time()-ti\n",
    "print('Total time:',tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptions: 54 \n",
      "Values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>35</th>\n",
       "      <th>48</th>\n",
       "      <th>143</th>\n",
       "      <th>176</th>\n",
       "      <th>217</th>\n",
       "      <th>228</th>\n",
       "      <th>251</th>\n",
       "      <th>279</th>\n",
       "      <th>281</th>\n",
       "      <th>...</th>\n",
       "      <th>1413</th>\n",
       "      <th>1425</th>\n",
       "      <th>1467</th>\n",
       "      <th>1470</th>\n",
       "      <th>1527</th>\n",
       "      <th>1607</th>\n",
       "      <th>1663</th>\n",
       "      <th>1666</th>\n",
       "      <th>1677</th>\n",
       "      <th>1717</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>48</td>\n",
       "      <td>143</td>\n",
       "      <td>176</td>\n",
       "      <td>217</td>\n",
       "      <td>228</td>\n",
       "      <td>251</td>\n",
       "      <td>279</td>\n",
       "      <td>281</td>\n",
       "      <td>...</td>\n",
       "      <td>1413</td>\n",
       "      <td>1425</td>\n",
       "      <td>1467</td>\n",
       "      <td>1470</td>\n",
       "      <td>1527</td>\n",
       "      <td>1607</td>\n",
       "      <td>1663</td>\n",
       "      <td>1666</td>\n",
       "      <td>1677</td>\n",
       "      <td>1717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   16    35    48    143   176   217   228   251   279   281   ...   1413  \\\n",
       "0    16    35    48   143   176   217   228   251   279   281  ...   1413   \n",
       "\n",
       "   1425  1467  1470  1527  1607  1663  1666  1677  1717  \n",
       "0  1425  1467  1470  1527  1607  1663  1666  1677  1717  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_excepts = DataFrame(exceptions)\n",
    "df_excepts = df_excepts.dropna()\n",
    "print('Exceptions:',len(df_excepts),'\\nValues:')\n",
    "df_excepts.T #Transpose only to get a better visualization of exceptions in one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 35, 48, 143, 176, 217, 228, 251, 279, 281, 322, 378, 408, 429, 430, 446, 475, 525, 572, 576, 588, 662, 697, 701, 731, 745, 766, 773, 824, 843, 897, 924, 953, 1003, 1014, 1019, 1039, 1059, 1077, 1151, 1186, 1209, 1216, 1236, 1413, 1425, 1467, 1470, 1527, 1607, 1663, 1666, 1677, 1717]\n"
     ]
    }
   ],
   "source": [
    "print(list(df_excepts.index)) #to get the same values but in int type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython Version of Corpus construction code\n",
    "\n",
    "In this case the cython implementation of distances must be inside the library textsim like in **nlpnet** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* tqdm library is excellent for visualizing bar progress of single core\n",
    "  and parallel code solutions, and let you know exactly how many\n",
    "  instances still pendent in your actual cell running.\n",
    "* The parallel solution speed up 1.7x (6.7 minutes are now 4 minutes).\n",
    "* Cython most improve more than this.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Read pandas.io module to get a strong idea about how to play with in/out operations.\n",
    "* Read scipy.io module to undestand arff format reading implemented iside this library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Make a parallel version of CSV corpus generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "<a id='Scipy2012'></a>\n",
    "* [Scipy2012] Scipy Community, Manual \"SciPy Reference Guide\", 2012."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
