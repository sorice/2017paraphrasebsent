{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Alignment Algorithm Phase\n",
    "\n",
    "The objective of this phase is to get a <font color='#FA0000'>new text structure</font> with normalized sentences and its original related by the offset and length properties. E.g.:\n",
    "\n",
    "<a id='Aligned_Text_Structure'></a>\n",
    "<font color='#FA0000'>**Aligned Text Structure:**</font>\n",
    "If you open a the abstract-name file *suspicious-document00XYZ.txt*, then you would get a text with de following structure in every line:\n",
    "\n",
    "<p><font color='#FA0000'>\n",
    "   $(id_K,normalized\\ sentence_K,original\\_offset_{sentence\\,K},original\\_offset+length_{sentence\\,K})$\n",
    "   </font>\n",
    "\n",
    "*Why we need this?*\n",
    "This is useful for example when you are working in a real plagiarism detection or text-reuse detection applications and you need to show to the users the similarity result between the preprocessed fragments or sentences and its original form (pdf, html, etc). Additionally is very safe to work with duplicated or transformed objects very well related to the originals, after have them you can do subsequent transformations to the copy-object and never loss the original length and position of this sentence or fragment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment Algorithm based on Jaccard Distance\n",
    "\n",
    "The final algorithm needs 4 auxiliar functions:\n",
    "\n",
    "1. A character replacement function in the original text in which Jaccard (and others) fails (*Eg: line breaks*) (**normalize**)\n",
    "2. A function to look over the list of sentences of the preproces text (**getSentA**).\n",
    "3. A function to look over the list of segments of the original-text ending with '.'(**getSentB**).\n",
    "4. Jaccard function to validate Smith-Watermna alignment (**Jaccard**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cell\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Text Normalization Function\n",
    "\n",
    "To improve the precision of Jaccard algorithm is necessary to convert some punctuation situation.\n",
    "The main feature here is that whole regular expressions and functiones used does not change the length of the original text. The [previous](02.1-Normalizing-Text-Corpus.ipynb) normalization process it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.normalize import  add_doc_ending_point, abbreviations\n",
    "from preprocess.normalize import  multipart_words, replace_point_sequence\n",
    "\n",
    "\n",
    "def normalize(text_orig):\n",
    "    replacement_patterns = [(r'[:](?=\\s*?\\n)','##1'),\n",
    "                            (r'\\xc2|\\xa0',' '),\n",
    "                            (r'(\\w\\s*?):(?=\\s+?[A-Z]+?)|(\\w\\s*?):(?=\\s*?\"+?[A-Z]+?)','\\g<1>##2'),\n",
    "                            (r'[?!]','##3'),\n",
    "                            (r'(\\w+?)(\\n)(?=[\"$%()*+&,-/;:¿¡<=>@[\\\\]^`{|}~\\t\\s]*(?=.*[A-Z0-9]))','\\g<1>##4'), # any alphanumeric char\n",
    "                            # follow by \\n follow by any number of point sign follow by a capital letter, replace by alphanumerig+.\n",
    "                            (r'(\\w+?)(\\n)(?=[\"$%()*+&,-/;:¿¡<=>@[\\\\]^`{|}~\\t\\s\\n]*(?=[a-zA-Z0-9]))','\\g<1>##5'),# any alphanumeric char\n",
    "                            # follow by \\n follow by any number of point sign follow by a letter, replace by alphanumerig+.\n",
    "                            (r'[:](?=\\s*?)(?=[\"$%()*+&,-/;:¿¡<=>@[\\\\]^`{|}~\\t\\s]*[A-Z]+?)','##6'),\n",
    "                            (r'(\\w+?\\s*?)\\|','\\g<1>##7'),\n",
    "                            (r'\\n(?=\\s*?[A-Z]+?)','##8'),\n",
    "                            (r'##\\d','apdbx'),\n",
    "                            ]\n",
    "    \n",
    "    for (pattern, repl) in replacement_patterns:\n",
    "            (text_orig, count) = re.subn(pattern, repl, text_orig)\n",
    "    \n",
    "    text_orig = replace_point_sequence(text_orig)\n",
    "    text_orig = multipart_words(text_orig)\n",
    "    text_orig = abbreviations(text_orig)\n",
    "    text_orig = re.sub(r'apdbx+','.', text_orig)\n",
    "    text_orig = add_doc_ending_point(text_orig)#append . final si el último caracter no tiene punto, evita un ciclo infinito al final.\n",
    "    return text_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This minimal normalization function has been created only with the purpouse to delete some troublesome strings without any lenght modification, the normalization demo flow used in preprocess library change the lenght of all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521\n",
      "1521\n"
     ]
    }
   ],
   "source": [
    "orig_text = open('test/test_text.txt').read()\n",
    "print(len(orig_text))\n",
    "norm_text = normalize(orig_text)\n",
    "print(len(norm_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentence A\n",
    "\n",
    "Get all sentences in preprocessed text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentA(text):\n",
    "    offset = 0\n",
    "    for i in re.finditer('\\.',text):\n",
    "        sentA = text[offset:i.end()]\n",
    "        yield sentA, offset, i.end()\n",
    "        offset = i.end()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The is a 5 acre 20 000 m located in the of located in . 0 55\n",
      "1 The zoo is operated by the in partnership with the . 56 108\n",
      "2 Queens Zoo zoo New York City borough Queens Flushing Meadows_Corona Park Wildlife Conservation Society New York City Department of Parks and Recreation . 109 262\n",
      "3 The Zoo opened on with the ceremonial ribbon cut by . 263 316\n",
      "4 The zoo is home mostly to animals native to North America . 317 376\n",
      "5 The Queens Zoo is the only one of the five zoos in New York City to exhibit . 377 454\n",
      "6 The zoo was constructed on the site of the and the zoo is aviary is a designed by and used during the 1964 Fair . 455 568\n",
      "7 October 26 1968 Robert Moses 2 Spectacled Bears 1964 New York World is Fair geodesic dome Buckminster Fuller 1  . 569 682\n"
     ]
    }
   ],
   "source": [
    "#Example of getSentA use\n",
    "preprocessed_text = open('data/norm/susp/suspicious-document00184.txt').read()\n",
    "        \n",
    "for i,(sentA, offset, length) in enumerate(getSentA(preprocessed_text)):\n",
    "    print (i,sentA, offset, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentence B\n",
    "\n",
    "<center><strong>Diagrama simplificado getSentB function.</strong></center></br>\n",
    "<table border=0 cellspacing=10> \n",
    "    <caption align=\"bottom\"> </br><em>Figura 2.3.1: Esquema del Algoritmo Get Sentence B</em>\n",
    "    </caption> \n",
    "<tr align=\"center\">\n",
    "    <th> <img src=\"imgs/getSentB.jpg\" height=800px width=1200px alt=\"*\" \n",
    "        align=\"center\"> </th>\n",
    "    <th> </th>\n",
    "    <td> \n",
    "        <p> El algoritmo comienza a buscar en el texto original el caracter '.'  más cercano al offsetB (*$\\|\\overrightarrow{sB}\\|=0 \\Longrightarrow sentLength = 0$*).\n",
    "        Una vez encontrado denota como punto previo este nuevo punto para una nueva llamada (*$sentLength = \\|\\overrightarrow{sB}\\|+1$*), \n",
    "        luego calcula el segmento, y define el punto siguiente como $Offset + \\|\\overrightarrow{sB}\\|$ *(len segmento definido por el . encontrado)*.\n",
    "        En la siguiente corrida el algoritmo encontrará el '.' más cercano a partir de **sB** \n",
    "        calculando el nuevo punto siguiente(*sB`*). Y así sucesivamente\n",
    "        ante cada llamada en la función *getSentB*. Es en la función de alineación donde se establece la condición \n",
    "        de parada cuando el score de Smith-Waterman es máximo entre el *$\\|\\overrightarrow{sB^{n`}}\\|$ y *length sentA*. \n",
    "        </p>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0 sentB: The is a 5 acre (20,000 m ) located in the of , located in .\n",
      "offsetB: 0 nextPoint: 60 sentLength 60\n",
      "***************\n",
      "i 1 sentB: The is a 5 acre (20,000 m ) located in the of , located in . The zoo is operated by the in partnership with the .\n",
      "offsetB: 0 nextPoint: 113 sentLength 113\n",
      "***************\n",
      "i 2 sentB: The is a 5 acre (20,000 m ) located in the of , located in . The zoo is operated by the in partnership with the .Queens Zoo zoo New York City borough Queens Flushing Meadows-Corona Park Wildlife Conservation Society New York City Department of Parks and Recreation\n",
      "The Zoo opened on , , with the ceremonial ribbon cut by .\n",
      "offsetB: 0 nextPoint: 322 sentLength 322\n",
      "***************\n",
      "i 3 sentB:  The zoo is home mostly to animals native to North America.\n",
      "offsetB: 322 nextPoint: 381 sentLength 59\n",
      "***************\n",
      "i 4 sentB:  The zoo is home mostly to animals native to North America. The Queens Zoo is the only one of the five zoos in New York City to exhibit .\n",
      "offsetB: 322 nextPoint: 459 sentLength 137\n",
      "***************\n",
      "i 5 sentB:  The zoo is home mostly to animals native to North America. The Queens Zoo is the only one of the five zoos in New York City to exhibit . The zoo was constructed on the site of the , and the zoo's aviary is a , designed by and used during the 1964 Fair.\n",
      "offsetB: 322 nextPoint: 575 sentLength 253\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "def getSentB(text2, offsetB, nextPoint,sentLength):\n",
    "    posB = text2[offsetB+sentLength:].find('.')\n",
    "    sentLength += posB+1\n",
    "    sentB = text2[offsetB:offsetB+sentLength]\n",
    "    nextPoint = offsetB + sentLength\n",
    "    return sentB, nextPoint, sentLength\n",
    "\n",
    "text_orig = open('data/orig/susp/suspicious-document00184.txt').read()\n",
    "\n",
    "offsetB = 0;nextPoint = 0;sentLength=0\n",
    "\n",
    "for i in range(text_orig.count('.')):\n",
    "    sentB, nextPoint, sentLength = getSentB(text_orig,offsetB,nextPoint,sentLength)\n",
    "    print('i',i,'sentB:',sentB)\n",
    "    print('offsetB:', offsetB, 'nextPoint:', nextPoint, 'sentLength', sentLength)\n",
    "    print('***************')\n",
    "    if i == 2: #This is a cuting point see the explanation below\n",
    "        offsetB = nextPoint\n",
    "        nextPoint = 0\n",
    "        sentLength = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** as you can prove later in the alingment algorithm, when sentA match with sentB the value of the offset parameter is equalized to *nextPoint*. Then the algorithm start to look for the next sentB from the final position (*last* **nextPoint**) of previous sentB. In the above example 322 is at the same time the las nextPoint(& len) of $sentB_1$, and the offset of $sentB_2$, this one start with the string: *The zoo is home mostly...*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Function\n",
    "\n",
    "Originally Jaccard distance is evaluated as $(A\\cup B- B\\cap A)/B\\cup A)$, which means the fraction between different labels and similar labels inside A & B sets. Here we implement a variation $Jaccard=(B\\cap A)/B\\cup A)$, which means the fraction between common terms and the total different terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(text1,text2):\n",
    "    sentA1 = re.sub(r'[!\"#$%&()\\'*+,-/:;<=>?@\\\\^_`{|}~.\\[\\]]',' ', text1)\n",
    "    sentB1 = re.sub(r'[!\"#$%&()\\'*+,-/:;<=>?@\\\\^_`{|}~.\\[\\]]',' ', text2)\n",
    "    setA = set(sentA1.split())\n",
    "    setB = set(sentB1.split())\n",
    "    total_terms = len(sentB1.split())\n",
    "    return len(setA.intersection(setB))/(total_terms or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment Pipeline\n",
    "\n",
    "* The next code print jaccard score between sentence A & B. \n",
    "* Then print the maximal score finded for a chunck B given sentence A. \n",
    "* Next the main properties of chunck B (offset & length) are printed. \n",
    "* Finally for a visual comparation of result sentence A is printed.\n",
    "\n",
    "**Nota:** To see more details about how process occurs uncomment the *print* orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----from 0 ---- to prevPoint 0\n",
      "-----to nextPoint 60\n",
      "sentB:\n",
      " > > > Home Practice Areas Municipal Law Zoning and Land Use. \n",
      "\n",
      "sentA:  Home Practice Areas Municipal Law Zoning and Land Use . \n",
      "\n",
      "***************\n",
      "jaccard_measure: 1.0\n",
      "i: 0 score: 1.0 maxScore: 0\n",
      "frag-sentA: al Law Zoning and Land Use . \n",
      "frag-sentB: pal Law Zoning and Land Use. \n",
      "\n",
      "-----from 0 ---- to prevPoint 60\n",
      "-----to nextPoint 80\n",
      "sentB:\n",
      " > > > Home Practice Areas Municipal Law Zoning and Land Use.Zoning and Land Use. \n",
      "\n",
      "sentA:  Home Practice Areas Municipal Law Zoning and Land Use . \n",
      "\n",
      "jaccard_measure: 0.6923076923076923\n",
      "i: 0 score: 0.6923076923076923 maxScore: 1.0\n",
      "frag-sentA: al Law Zoning and Land Use . \n",
      "frag-sentB: and Use.Zoning and Land Use. \n",
      "\n",
      "Final 0 \n",
      "jaccard_measure: 1.0\n",
      "Sentence 0 score max: 1.0 offsetB: 0 lengthB: 60 \n",
      "\n",
      "sent original: > > > Home Practice Areas Municipal Law Zoning and Land Use\n",
      " \n",
      "\n",
      "sent process:  Home Practice Areas Municipal Law Zoning and Land Use .\n",
      "\n",
      "***************\n",
      "-----from 60 ---- to prevPoint 60\n",
      "-----to nextPoint 80\n",
      "sentB:\n",
      " Zoning and Land Use. \n",
      "\n",
      "sentA: Zoning and Land Use . \n",
      "\n",
      "***************\n",
      "jaccard_measure: 1.0\n",
      "i: 1 score: 1.0 maxScore: 0\n",
      "frag-sentA: Land Use . \n",
      "frag-sentB:  Land Use. \n",
      "\n",
      "-----from 60 ---- to prevPoint 80\n",
      "-----to nextPoint 410\n",
      "sentB:\n",
      " Zoning and Land Use.Our attorneys work with developers, municipalities and individuals on projects ranging from applications for minor variances to permits for telecommunication facilities, major subdivisions and site plans for office parks, hospitals, golf courses, shopping centers, senior housing and single and multi_family housing developments_. \n",
      "\n",
      "sentA: Zoning and Land Use . \n",
      "\n",
      "jaccard_measure: 0.0851063829787234\n",
      "i: 1 score: 0.0851063829787234 maxScore: 1.0\n",
      "frag-sentA: Land Use . \n",
      "frag-sentB: lopments_. \n",
      "\n",
      "Final 1 \n",
      "jaccard_measure: 1.0\n",
      "Sentence 1 score max: 1.0 offsetB: 60 lengthB: 20 \n",
      "\n",
      "sent original: Zoning and Land Use\n",
      " \n",
      "\n",
      "sent process: Zoning and Land Use .\n",
      "\n",
      "***************\n",
      "-----from 80 ---- to prevPoint 80\n",
      "-----to nextPoint 410\n",
      "sentB:\n",
      " Our attorneys work with developers, municipalities and individuals on projects ranging from applications for minor variances to permits for telecommunication facilities, major subdivisions and site plans for office parks, hospitals, golf courses, shopping centers, senior housing and single and multi_family housing developments_. \n",
      "\n",
      "sentA: Our attorneys work with developers municipalities and individuals on projects ranging from applications for minor variances to permits for telecommunication facilities major subdivisions and site plans for office parks hospitals golf courses shopping centers senior housing and single and multi_family housing developments . \n",
      "\n",
      "jaccard_measure: 0.8604651162790697\n",
      "i: 2 score: 0.8604651162790697 maxScore: 0\n",
      "frag-sentA: ities major subdivisions and site plans for office parks hospitals golf courses shopping centers senior housing and single and multi_family housing developments . \n",
      "frag-sentB: , major subdivisions and site plans for office parks, hospitals, golf courses, shopping centers, senior housing and single and multi_family housing developments_. \n",
      "\n",
      "-----from 80 ---- to prevPoint 410\n",
      "-----to nextPoint 544\n",
      "sentB:\n",
      " Our attorneys work with developers, municipalities and individuals on projects ranging from applications for minor variances to permits for telecommunication facilities, major subdivisions and site plans for office parks, hospitals, golf courses, shopping centers, senior housing and single and multi_family housing developments_.We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals. \n",
      "\n",
      "sentA: Our attorneys work with developers municipalities and individuals on projects ranging from applications for minor variances to permits for telecommunication facilities major subdivisions and site plans for office parks hospitals golf courses shopping centers senior housing and single and multi_family housing developments . \n",
      "\n",
      "jaccard_measure: 0.5967741935483871\n",
      "i: 2 score: 0.5967741935483871 maxScore: 0.8604651162790697\n",
      "frag-sentA: ities major subdivisions and site plans for office parks hospitals golf courses shopping centers senior housing and single and multi_family housing developments . \n",
      "frag-sentB: amily housing developments_.We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals. \n",
      "\n",
      "Final 2 \n",
      "jaccard_measure: 0.8604651162790697\n",
      "Sentence 2 score max: 0.8604651162790697 offsetB: 80 lengthB: 330 \n",
      "\n",
      "sent original: Our attorneys work with developers, municipalities and individuals on projects ranging from applications for minor variances to permits for telecommunication facilities, major subdivisions and site plans for office parks, hospitals, golf courses, shopping centers, senior housing and single and multi-family housing developments.\n",
      " \n",
      "\n",
      "sent process: Our attorneys work with developers municipalities and individuals on projects ranging from applications for minor variances to permits for telecommunication facilities major subdivisions and site plans for office parks hospitals golf courses shopping centers senior housing and single and multi_family housing developments .\n",
      "\n",
      "***************\n",
      "-----from 410 ---- to prevPoint 410\n",
      "-----to nextPoint 544\n",
      "sentB:\n",
      " We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals. \n",
      "\n",
      "sentA: We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals . \n",
      "\n",
      "***************\n",
      "jaccard_measure: 1.0\n",
      "i: 3 score: 1.0 maxScore: 0\n",
      "frag-sentA: alley in processing applications for zoning and land use approvals . \n",
      "frag-sentB: Valley in processing applications for zoning and land use approvals. \n",
      "\n",
      "-----from 410 ---- to prevPoint 544\n",
      "-----to nextPoint 900\n",
      "sentB:\n",
      " We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals. Whether it involves giving a presentation to a local land use board to explain a complex project, working through the regulatory maze of SEQRA regulations, wetlands control boards, the New York DEC, the Connecticut Siting Council or drafting findings for a municipal board, we know how to make a record that will support the outcome our client is seeking. \n",
      "\n",
      "sentA: We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals . \n",
      "\n",
      "jaccard_measure: 0.24358974358974358\n",
      "i: 3 score: 0.24358974358974358 maxScore: 1.0\n",
      "frag-sentA: alley in processing applications for zoning and land use approvals . \n",
      "frag-sentB: o make a record that will support the outcome our client is seeking. \n",
      "\n",
      "Final 3 \n",
      "jaccard_measure: 1.0\n",
      "Sentence 3 score max: 1.0 offsetB: 410 lengthB: 134 \n",
      "\n",
      "sent original: We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals. \n",
      "\n",
      "sent process: We have experience working with communities throughout the Hudson Valley in processing applications for zoning and land use approvals .\n",
      "\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def alignSentences(preproc_text, orig_text):\n",
    "    sentenceList=[]\n",
    "    offsetB = 0\n",
    "    \n",
    "    norm_orig_text = normalize(orig_text)\n",
    "    \n",
    "    #if preproc_text.count('.') > norm_orig_text.count('.'):\n",
    "    #    raise Exception(\"Preprocess Error: number of preproc periods most be less or equal than normalize original text periods.\")\n",
    "        \n",
    "    for i, (sentA, offsetA, lengthA) in enumerate(getSentA(preproc_text)):\n",
    "        maxScore =-1; score = 0\n",
    "        prevPoint = 0 #len(sentA)-2\n",
    "        nextPoint = 0\n",
    "        iqualScore = 0\n",
    "        prevFrag=''\n",
    "        jaccard_measure = 0\n",
    "        prev_jaccard_measure = 1.0\n",
    "        k = 0.5\n",
    "        \n",
    "        #Sí llegamos a la última oración entonces\n",
    "        if i == preproc_text.count('.')-1: \n",
    "            lengMax = len(norm_orig_text)\n",
    "            tuple = (i, sentA, offsetB, lengMax)\n",
    "            sentenceList.append(tuple)\n",
    "            break\n",
    "        \n",
    "        #Sí no es la última oración compara hasta encontrar el score max.\n",
    "        while(score >= maxScore):\n",
    "            prev_jaccard_measure = jaccard_measure\n",
    "            lengMax = nextPoint\n",
    "            maxScore = score\n",
    "            \n",
    "            #Get sentence B and prepare it to calc distances\n",
    "            print('-----from',offsetB,'---- to prevPoint', offsetB+prevPoint)\n",
    "            sentB, nextPoint, prevPoint = getSentB(norm_orig_text, offsetB, nextPoint, prevPoint)\n",
    "            print('-----to nextPoint',nextPoint)\n",
    "            print('sentB:\\n',sentB,'\\n')\n",
    "            print('sentA:',sentA,'\\n')\n",
    "            \n",
    "            #Calc measures Jaccard\n",
    "            jaccard_measure = jaccard ( sentA , sentB) #Second measure only to lookfor errors\n",
    "            print ('jaccard_measure:',jaccard_measure)\n",
    "            \n",
    "            score = jaccard_measure\n",
    "            print('i:',i,'score:',score,'maxScore:',maxScore)\n",
    "    \n",
    "            print('frag-sentA:',sentA[-round(len(sentA)*k):],'\\nfrag-sentB:',sentB[-round(len(sentA)*k):],'\\n')\n",
    "            \n",
    "            #Repeated sentence exception src00014\n",
    "            if prevFrag == sentB[-round(len(sentA)*k):]:\n",
    "                #print ('=================Repeated sentence')\n",
    "                break\n",
    "            #keep the previous fragment to know if the next sent is the same as before. \n",
    "            #The algh move forward to the next sentence.\n",
    "            prevFrag = sentB[-round(len(sentA)*k):] \n",
    "            \n",
    "            #Short sentence exceptions\n",
    "            if len(sentA) < 14:\n",
    "                maxScore = score\n",
    "                lengMax = nextPoint\n",
    "                break\n",
    "                \n",
    "            #Infinite loop exception\n",
    "            if score == maxScore:\n",
    "                iqualScore += 1\n",
    "            if iqualScore == 20:\n",
    "                break\n",
    "            \n",
    "        tuple = (i, sentA, offsetB, lengMax)\n",
    "        sentenceList.append(tuple)\n",
    "        \n",
    "        if i > -1:\n",
    "            print ('Final',i,'\\njaccard_measure:',prev_jaccard_measure)\n",
    "            print('Sentence' ,i, 'score max:',maxScore, 'offsetB:', offsetB, 'lengthB:',lengMax-offsetB,'\\n')\n",
    "            print('sent original:',text_orig[offsetB:lengMax],'\\n')\n",
    "            print('sent process:',sentA)\n",
    "            print('\\n***************')\n",
    "        \n",
    "        offsetB = lengMax\n",
    "\n",
    "    return sentenceList\n",
    "\n",
    "text_orig = open('data/orig/src/source-document01802.txt').read()\n",
    "preproc_text = open('data/norm/src/source-document01802.txt').read()\n",
    "\n",
    "sentenceList = alignSentences(preproc_text,text_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Alignment result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oración preprocesada: Civilians 587 as of 2004 4 .\n",
      "oración original: Civilians\n",
      "587 (as of 2004) [4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('oración preprocesada:', sentenceList[2][1])\n",
    "print ('oración original:', text_orig[sentenceList[2][2]:sentenceList[2][3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Tagged Aligned Text of Whole Collection\n",
    "\n",
    "Here we have again the *file* **pairs** which contains all normalized cases, inside the normalizated collection (../norm), then we need the route to original files and finally the output directory.\n",
    "\n",
    "Rememberíng the [begining definition](#Aligned_Text_Structure) of <font color='#FA0000'>aligned text structure</font> for future phases.\n",
    "\n",
    "E.g. open the file *data/aligned/susp/suspicious-document00007.txt*\n",
    "<i>\n",
    "<p>    0\tAccording to the legend a dish was make by servants of country kings paella were let to take mixed leftovers from the large dinner home in courtly pots .\t0\t154\n",
    "<p>    1\tIt iseafood believed that the Arabic word woulderives from the paella word which means leftovers .\t154\t248\n",
    "<p>    2\tTake spanish dish guides Paella probably a a other rich .\t248\t305\n",
    "<p>    ...\n",
    "</i>\n",
    "\n",
    "$(id_K,normalized-sentence_K,original-offset_{sentence\\,K},original-length_{sentence\\,K})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1445 TRUE: 1\n",
      "2 / 1445 TRUE: 2\n",
      "146 / 1445 TRUE: 146\n",
      "290 / 1445 TRUE: 290\n",
      "291 / 1445 TRUE: 291\n",
      "435 / 1445 TRUE: 435\n",
      "579 / 1445 TRUE: 579\n",
      "580 / 1445 TRUE: 580\n",
      "724 / 1445 TRUE: 724\n",
      "868 / 1445 TRUE: 868\n",
      "869 / 1445 TRUE: 869\n",
      "1013 / 1445 TRUE: 1013\n",
      "1157 / 1445 TRUE: 1157\n",
      "1158 / 1445 TRUE: 1158\n",
      "1302 / 1445 TRUE: 1302\n",
      "1445 / 1445 TRUE: 1445\n",
      "tiempo total:  5.598086833953857\n"
     ]
    }
   ],
   "source": [
    "%run scripts/02.2_alignNormalizedCaseList.py norm/norm_pairs norm/src/ norm/susp/ align/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The objective of this process was accomplished. The obtained transformed text contains the proposed structure. 1361 pairs of text (cases) were obtaibned previously from the 2000 initial normalized list using utils.py. Then we re-aligned thems to show the process of alignment using 02.3_alignNormalizedCaseList.py script.\n",
    "\n",
    "Although utils.py script use the Jaccard alignment algorithm and a 68% of the total cases were good aligned after normalization, it is very easy to test other similarity distances or alignment knowleage and obtain different results and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excersices\n",
    "\n",
    "1. The comparison between alignment text in 2.2_notebook and the algh presented here can show better performance. Implement a different alignment pipeline using another faster technique and less source code.\n",
    "\n",
    "2. Make a list of different punctuation situations that can appear in real text provided by pdftotext conversors and program the regular expresion for both sides the normalization process and for normalize function. Construct a test unit with your ideas and finally test them in the same way we did here.\n",
    "\n",
    "3. Create a different idea to align original and preprocessed sentences. Sugestion, take a look to cross-lingual alignment implemented in nltk.\n",
    "\n",
    "4. In the Smith-Waterman version of this alignment algorithm The next code can be added to the alignment pipeline to improve the alignment of long sentences. Evaluate it and propose the best k to maintain the precision.\n",
    "\n",
    "        #Optimization for very long sentences alignment\n",
    "        if len(sentA) > 500:\n",
    "            k = 0.1\n",
    "        else: k=0.5\n",
    "        \n",
    "5. The pairs list provided for this tutorial(_norm_pairs_utils_) contains 1445 cases in which the Jaccard based aligment makes a perfect score. If you test this algorithm in PAN-PC pairs file the result will be very different, some cases will be bad aligned. Make the necessary changes to the algorithm to write in the preprocessDocList file a \"False\" value if the text is bad aligned. (_Sugestion: study the utils.py file_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
