{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for Paper *Paraphrase Beyond Sentence*. Source and license info is on [GitHub](https://github.com/sorice/2017paraphrasebsent/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Similarity Example with Sklearn\n",
    "\n",
    "The goal of this notebook is to show step-by-step how to calculate similarity between two texts using vectorized texts. The similarity measures used in the examples below are based on tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 From String to Vectors\n",
    "\n",
    "Introduce the strings and transform to vectors using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (1, 19) (1, 19)\n",
      "<class 'numpy.ndarray'>\n",
      "[[1 1 1 1 2 1 0 1 1 1 0 2 1 0 1 1 0 1 1]]\n",
      "[[1 1 1 1 2 0 1 1 0 1 1 2 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sA = \"PCCW's chief operating officer, Mike Butcher, and Alex Arena, the chief financial officer, will report directly to Mr So.\"\n",
    "sB = \"Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So.\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_v = CountVectorizer()\n",
    "tdm = count_v.fit_transform([sA, sB])\n",
    "A = tdm[0]\n",
    "B = tdm[1]\n",
    "print(type(A))\n",
    "print(type(B))\n",
    "\n",
    "print (type(B),B.shape,A.shape)\n",
    "#pairwise_distances_argmin_min(A,Bp,axis=1,metric='jaccard')\n",
    "Bp = B.toarray()\n",
    "Ap = A.toarray()\n",
    "print (type(Bp))\n",
    "print (Bp)\n",
    "print (Ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Padding Vectors When Lengths are Not Equal\n",
    "\n",
    "If the vectors are obtained from a different source to the one shown above, an equal length in both vectors is needed. The similarity metrics in sklearn works well if both vectors have the same length. Different sentences have different sizes, so it is mandatory to pad the shortest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([0,1,0,0,1,1,1,0,0,1,0,0,0,0],dtype=np.int32)\n",
    "B = np.array([0,0,1,0,1,0,1,0,1],dtype=np.int32)\n",
    "\n",
    "Bp = np.pad(B,(0,len(A)-len(B)),'constant') #padding B with value 0 constant at the right side to the A len."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Calculating Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([5.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import *\n",
    "\n",
    "#Reshape the vectos to samples,features\n",
    "Bp = Bp.reshape(1,-1)\n",
    "A = A.reshape(1,-1)\n",
    "\n",
    "# jaccard similarity\n",
    "pairwise_distances_argmin_min(A,Bp,axis=1,metric='manhattan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Constructing a Vector of Features\n",
    "\n",
    "The next step is about building a vector made of some similarity measures.\n",
    "\n",
    "    pairwise_distances_argmin_min(A,Bp,axis=1,metric='dice')\n",
    "    pairwise_distances_argmin_min(A,Bp,axis=1,metric='hamming')\n",
    "    pairwise_distances_argmin_min(A,Bp,axis=1,metric='cityblock')\n",
    "    pairwise_distances_argmin_min(A,Bp,axis=1,metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import * #support sparse matrix inputs\n",
    "from scipy.spatial.distance import * #do not support sparse matrix inputs\n",
    "\n",
    "#Use the next line if your vectors have more than 14 samples, the 'mahalanobis' distance fails\n",
    "    #from sklearn.metrics.pairwise import _VALID_METRICS \n",
    "\n",
    "_VALID_METRICS = ['euclidean', 'l2', 'l1', 'manhattan', 'cityblock',\n",
    "                  'braycurtis', 'canberra', 'chebyshev', 'correlation',\n",
    "                  'cosine', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
    "                  'matching', 'minkowski', 'rogerstanimoto',\n",
    "                  'russellrao', 'seuclidean', 'sokalmichener',\n",
    "                  'sokalsneath', 'sqeuclidean', 'yule',]\n",
    "vector = []\n",
    "\n",
    "for distance in _VALID_METRICS:\n",
    "    index, dist =  pairwise_distances_argmin_min(A,Bp, axis=1, metric=distance)\n",
    "    vector.append(dist[0])\n",
    "\n",
    "_DISTANCE_VECTOR = np.array(vector, dtype = np.float16)\n",
    "_DISTANCE_VECTOR = _DISTANCE_VECTOR.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 23)\n",
      "[[2.236  2.236  5.     5.     5.     0.5557 5.     1.     0.8115 0.5527\n",
      "  0.5557 0.3572 0.7144 0.8945 0.3572 2.236  0.5264 0.857     nan 0.5264\n",
      "  0.8335 5.     0.6   ]]\n"
     ]
    }
   ],
   "source": [
    "print (_DISTANCE_VECTOR.shape)\n",
    "print (_DISTANCE_VECTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Scaling all Features for Normalization\n",
    "\n",
    "All the features must be scaled to the interval [0,1]. In the example below will be used the *MaxAbsScaler* method which takes the maximal existent value as 1. Other techniques are available in the module sklearn.preprocessing.\n",
    "\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "    maxabs = MaxAbsScaler()\n",
    "    _DISTANCE_VECTOR_NORM = maxabs.fit_transform(_DISTANCE_VECTOR)\n",
    "    \n",
    "A setback in this method is that it needs some samples. That is why the samples used are taken from an ARFF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1725,)\n",
      "original MATRIX[0]\n",
      " (0.72500002, 0.48598132, 0.61682242, 0.61682242, 0.73934317, 0.73934317, 0.91112685, 0.89166665, 0.58008659, 0.3888889, 0.438357, 0.4375, 0.78516549, 0.28, 0.36842105, 0.85772765, 0.46666667, 0.58333337, b'yes') \n",
      "type [('NeedlemanWunch', '<f8'), ('SmithWaterman', '<f8'), ('SmithWatermanGotoh', '<f8'), ('SmithWatermanGotohWindowedAffine', '<f8'), ('Jaro', '<f8'), ('JaroWinkler', '<f8'), ('ChapmanMeanLength', '<f8'), ('ChapmanLengthDeviation', '<f8'), ('QGramsDistance', '<f8'), ('BlockDistance', '<f8'), ('CosineSimilarity', '<f8'), ('DiceSimilarity', '<f8'), ('EuclideanDistance', '<f8'), ('JaccardSimilarity', '<f8'), ('MatchingCoefficient', '<f8'), ('MongeElkan', '<f8'), ('OverlapCoefficien', '<f8'), ('Levenshtein', '<f8'), ('class', 'S3')] shape (10,) \n",
      "\n",
      "transformed MATRIX[0]\n",
      " [0.88359377 0.55277496 0.67167594 0.67167594 0.88752249 0.79225245\n",
      " 0.9220618  0.91024303 0.72510823 0.49122807 0.54455337 0.54687499\n",
      " 0.86514849 0.41999999 0.49736843 0.913556   0.51851853 0.77777783] \n",
      "type float64 shape (10, 18)\n",
      "Index(['NeedlemanWunch', 'SmithWaterman', 'SmithWatermanGotoh',\n",
      "       'SmithWatermanGotohWindowedAffine', 'Jaro', 'JaroWinkler',\n",
      "       'ChapmanMeanLength', 'ChapmanLengthDeviation', 'QGramsDistance',\n",
      "       'BlockDistance', 'CosineSimilarity', 'DiceSimilarity',\n",
      "       'EuclideanDistance', 'JaccardSimilarity', 'MatchingCoefficient',\n",
      "       'MongeElkan', 'OverlapCoefficien', 'Levenshtein'],\n",
      "      dtype='object')\n",
      "(10, 18)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "data, meta = arff.loadarff('data/MSRPC-2004/msrpc_test_simmetrics-18f.arff')\n",
    "\n",
    "print(data.shape) # see a (1725,) is important to reduce this for this notebook\n",
    "\n",
    "#Reduce the samples\n",
    "_DISTANCE_MATRIX_a = data[:10] # This matrix contain non numeric values, is important to drop\n",
    "\n",
    "# Using pandas to worki with the distance matrix\n",
    "_DISTANCE_MATRIX_b = pd.DataFrame(_DISTANCE_MATRIX_a)\n",
    "_DISTANCE_MATRIX_c = _DISTANCE_MATRIX_b.drop(['class'], axis=1)\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "maxabs = MaxAbsScaler()\n",
    "_DISTANCE_MATRIX_NORM = maxabs.fit_transform(_DISTANCE_MATRIX_c)\n",
    "print('original MATRIX[0]\\n', _DISTANCE_MATRIX_a[0],'\\ntype',_DISTANCE_MATRIX_a.dtype, 'shape',_DISTANCE_MATRIX_a.shape,'\\n')\n",
    "print('transformed MATRIX[0]\\n', _DISTANCE_MATRIX_NORM[0],'\\ntype',_DISTANCE_MATRIX_NORM.dtype, 'shape',_DISTANCE_MATRIX_NORM.shape)\n",
    "\n",
    "#Converting the result in a DataFrame\n",
    "_NormMatrix_DataFrame = pd.DataFrame(_DISTANCE_MATRIX_NORM,columns=_DISTANCE_MATRIX_c.columns)\n",
    "print(_NormMatrix_DataFrame.columns)\n",
    "print(_NormMatrix_DataFrame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- The *sklearn.metrics.pairwise* method allows to get 23 measures based on tokens.\n",
    "\n",
    "Next notebook *[3.2-Similarity-Features-Elaboration.ipynb](03.2-Text-Similarity-Features-Elaboration-with-textsim-pack.ipynb)* is a complete guide to use the internal documentation notebooks of **textsim** library for the creation of a complete set of features based on different string, token or knowledge distances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
