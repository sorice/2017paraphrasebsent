{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from preprocess.demo import preProcessFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the Text Corpus\n",
    "\n",
    "The objetives of this notebook are:\n",
    "\n",
    "* To explain what is the standard normalization process defined in Natural Language Processing. [(2.1.1)](#Text_Normalization) \n",
    "* 2.1.2. As a second goal to show you some personal(the author of this notebook) protocols designed across the normalization methods. \n",
    "* 2.1.3. To compare some shallow differences between some normalization methods implemented on classic NLP libraries, some of them in python and C++.\n",
    "* 2.1.4. And finally to convert the whole initial text collection y a normalized new one.\n",
    "    \n",
    "## Brief Summary\n",
    "\n",
    "It has to be taken into account that the text collection, to which we are referring in the whole tutorial, contains spell errors and other kinds of errors provenient from the output of pdf-to-text libraries or some automatic text generation algorithms (E.g. multiple line breaks inside a single sentence). So, although humans do not make these mistakes in real text, the automatic conversion or generation process can create them.\n",
    "\n",
    "In this phase we hope to obtain texts without end of sentences dots ambiguities, and the multi-words united by the underscore simbol (_), any other punctuation or non-letter simbol will be deleted.\n",
    "The result of this phase will be used on the [next](02.3-Aligning-Preproc-Sent-to-Original.ipynb) phase to match the normalized sentences with the originals with a 100% precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Text Normalization\n",
    "<a id='Text_Normalization'></a>\n",
    "\n",
    "*\"Text normalization is a related step that involves merging different written forms of a\n",
    "token into a canonical normalized form; for example, a document may contain the equivalent tokens\n",
    "“Mr.”, “Mr”, “mister”, and “Mister” that would all be normalized to a single form.\"* [<a href=\"#Indurkhya2010\" title=\"Handbook of Natural Language Processing\"> (Indurkhya2010) </a>](#Indurkhya2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Non Standard Normalization Protocols\n",
    "\n",
    "As you can see in the former block text normalization only include token transformation. But for future simplicity codes reasons the investigators must have at the end of the normalization phase a a canonical text at all levels: morphological, lexical and sintactic. Is not a good way to tray to simulate the rich-text pattern recognition that brains do (bullets, section or chapter divisions, etc.), faster algorithms needs very plain texts.\n",
    "\n",
    "See an example. Is well known that if you have a cappital letter after dot, this simbol means the end of a sentence. But, wha happen if you have the next composition: *\"H. Albot was a B.A. of Psicology.\"*. As you can see the first dot isn't the end of the first sentence, in fact there is only one sentence. And as a second detail the second point is correct, neither is the end of the sentence. Probably the must usefull string to get at the end of a normalization process could be: *\"Harry Albot was a bachelor of Psicology.\"*. That implicates the analysis of proper names,abbreviation and acronyms, math simbols, etc.\n",
    "\n",
    "The protocols created by the author available in the *preprocess.norm* module inside the tutorial are:\n",
    "\n",
    "- URL analysis                                 -> trated as a contiguous string (http___google_com__)\n",
    "- Rare simbols (including math simbols)        -> converted to a canonical form (u'\\u03c0' = 'Pi')\n",
    "- ... points detection                         -> eliminated\n",
    "- Contiguous string                            -> multi-words are trated as a single token (text-reuse = text_reuse)\n",
    "- Separation of end of sentence dots           -> to avoid ambiguities (\"Hola. Hoy...\" = \"Hola . Hoy...\")\n",
    "- Abbreviation, Acronym and proper names       -> canonical form subtitution\n",
    "- Addition of last sentence end dot            -> dot addition at the end of the last sentence if not\n",
    "- Punctuation chars analysis                   -> a set of regular expressions to solve pdf-text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A real example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************PREPROCESSED TEXT******************************\n",
      "For other optional flags of opencv_createsamples see the official documentation at http___Docs_opencv_org_doc_user_guide_ug_traincascade_html . 99 www_it_ebooks_info . Generating Haar Cascades for Custom 8_4 Targets . Creating cascade by running . opencv_traincascade 3_ anoche . 4 Después . Over 110 recipes to master this full_stack Python web framework 1 . Take your web2py skills to the next level by dipping into delicious usable recipes in this cookbook 2 . Learn advanced web2py usage from building advanced forms to creating PDF reports 3 . Written by developers of the web2py project with plenty of code examples for interesting and comprehensive learning . Mi correo es abelm_uclv_cu . A ver si lo coge . Please check www_PacktPub_com for information on our titles . www_it_ebooks_info . Learning SciPy for Numerical and . Scientific Computing . ISBN 978_1_78216_162_2 . Ahora probaremos la división al final de cada línea . U_S_ es un país desarrollado luego de la guerra la NASA fue creada . Reunieronse pues Lic_ Dr_ Ing_ Ph_D_ y todos los que de una forma u otra poseían títulos honoríficos . Elimina las comillas dobles y simples al final y al principio de la oración . Probando tildes y caracteres de español áéíóú ñÑÁÉÍÓÚ . Jhon is a word with a contraction . Oración cerrada con comillas seguida por otra . Well writed . Oración cerrada con comillas seguida de salto de línea . Oración seguida por otra entre comillas . La otra oración . \n",
      "*****************************HANDMADE PREPROCESSED TEXT******************************\n",
      "For other optional flags of opencv_createsamples see the official documentation at http___Docs_opencv_org_doc_user_guide_ug_traincascade_html .  99 www_it_ebooks_info .  Generating Haar Cascades for Custom 8_4 Targets .  Creating cascade by running .  opencv_traincascade 3_ anoche .  4 Después .  Over 110 recipes to master this full_stack Python web framework 1 .  Take your web2py skills to the next level by dipping into delicious usable recipes in this cookbook 2 .  Learn advanced web2py usage from building advanced forms to creating PDF reports 3 .  Written by developers of the web2py project with plenty of code examples for interesting and comprehensive learning .  Mi correo es abelm_uclv_cu .  A ver si lo coge .  Please check www_PacktPub_com for information on our titles .  www_it_ebooks_info .  Learning SciPy for Numerical and .  Scientific Computing .  ISBN 978_1_78216_162_2 .  Ahora probaremos la división al final de cada línea .   U_S_ es un país desarrollado luego de la guerra la NASA fue creada .  Reunieronse pues Lic_ Dr_ Ing_ Ph_D_ y todos los que de una forma u otra poseían títulos honoríficos .  Elimina las comillas dobles y simples al final y al principio de la oración .  Probando tildes y caracteres de español: áéíóú ñÑÁÉÍÓÚ . Jhon is a word with a contraction .  Oración cerrada con comillas seguida por otra .  Well writed .  Oración cerrada con comillas seguida de salto de línea .  Oración seguida por otra entre comillas.   La otra oración . \n",
      "\n",
      "Automatic end of sentence count of preprocessed text: 28\n",
      "Human end of sentence count of original test text: 28\n"
     ]
    }
   ],
   "source": [
    "text_orig = open('test/test_text.txt').read()\n",
    "preproc_text = preProcessFlow(text_orig)\n",
    "\n",
    "print('*****************************PREPROCESSED TEXT******************************')\n",
    "print(preproc_text)\n",
    "\n",
    "text_human = open('test/test_text_human_analysis.txt').read()\n",
    "text_human2, temp = re.subn(r'\\n',' ',text_human)\n",
    "print('*****************************HANDMADE PREPROCESSED TEXT******************************')\n",
    "print (text_human2)\n",
    "\n",
    "print ('\\nAutomatic end of sentence count of preprocessed text:',preproc_text.count('.'))\n",
    "print ('Human end of sentence count of original test text:', text_human2.count('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Other Normalization Process\n",
    "\n",
    "It is included some original source code of every library to show you the limitations of every normalization methods on each API.\n",
    "\n",
    "### NLTK\n",
    "\n",
    "Python's Natural Language Toolkit (NLTK) is a suite of libraries that has become one of the best tools for prototyping and building natural language processing systems. Is developed under the guide of Stanford Professor Steven Bird (see http://www.nltk.org/).\n",
    "\n",
    "~/nltk/tag/perceptron.py\n",
    "\n",
    "    def normalize(self, word):\n",
    "        '''\n",
    "        Normalization used in pre-processing.\n",
    "        - All words are lower cased\n",
    "        - Digits in the range 1800-2100 are represented as !YEAR;\n",
    "        - Other digits are represented as !DIGITS\n",
    "\n",
    "This function returns a TAG for every type of normalization task but not the normalized word.\n",
    "\n",
    "The others references to normalization that can be found inside NLTK some times are about the mathematical operation to make two different datas comparable, and in a few cases just to put some strings into lower case.\n",
    "\n",
    "### Freeling\n",
    "\n",
    "The FreeLing package consists of a library providing language analysis services (such as morphological analysis, date recognition, PoS tagging, etc.). It was made by TALP Research Center and the Universitat Politecnica de Catalunya (see http://nlp.lsi.upc.edu/freeling).\n",
    "\n",
    "The FreeLing API have the normalization task divided in modules that implement the detection of some patterns: Numbers, Punctuation, Dates, Multiword, Name Entity and Quantity.\n",
    "\n",
    "As you can review in: _~/freeling-3.1/src/include/freeling/morfo/tokenizer.h_ and _~/freeling-3.1/src/libfreeling/tokenizer.cc_ there are som basical rules you can use to analyse the logic of the normalization process inside _FreeLing_. In fact we put some of that rules in here (taken from _~/freeling-3.1/data/en/tokenizer.dat_).\n",
    "\n",
    "    <RegExps>\n",
    "    INDEX_SEQUENCE   0  (\\.{4,}|-{2,}|\\*{2,}|_{2,}|/{2,})\n",
    "    INITIALS1 \t 1  ([A-Z](\\.[A-Z])+)(\\.\\.\\.)\n",
    "    INITIALS2 \t 0  ([A-Z]\\.)+\n",
    "    TIMES            0  (([01]?[0-9]|2[0-4]):[0-5][0-9])\n",
    "    NAMES_CODES\t 0  ({ALPHA}|{SYMNUM})*[0-9]({ALPHA}|[0-9]|{SYMNUM}+{ALPHANUM})*\n",
    "    THREE_DOTS \t 0  (\\.\\.\\.)\n",
    "    QUOTES\t         0  (``|<<|>>|'')\n",
    "    MAILS \t         0  {ALPHANUM}+([\\._]{ALPHANUM}+)*@{ALPHANUM}+([\\._]{ALPHANUM}+)*\n",
    "    URLS1 \t         0  ((mailto:|(news|http|https|ftp|ftps)://)[\\w\\.\\-]+|^(www(\\.[\\w\\-]+)+))\n",
    "    URLS2            1  ([\\w\\.\\-]+\\.(com|org|net))[\\s]\n",
    "    CONTRACT_0a      1  (i'(m|d|ll|ve))({NOALPHANUM}|$) CI\n",
    "    CONTRACT_0b      1  ((you|we|they|who)'(d|ll|ve|re))({NOALPHANUM}|$) CI\n",
    "    CONTRACT_0c      1  ((he|she|it|that|there)'(d|ll|s))({NOALPHANUM}|$) CI\n",
    "    CONTRACT_0d      1  ((let|what|where|how|who)'s)({NOALPHANUM}|$) CI\n",
    "    CONTRACT1        1  ({ALPHA}+)('([sdm]|ll|ve|re)({NOALPHANUM}|$)) CI\n",
    "    CONTRACT2        1  ('([sdm]|ll|ve|re|tween))({NOALPHANUM}|$) CI\n",
    "    KEEP_COMPOUNDS   0  {ALPHA}+(['_\\-\\+]{ALPHA}+)+\n",
    "    *ABREVIATIONS1   0  (({ALPHA}+\\.)+)(?!\\.\\.)\n",
    "    WORD             0  {ALPHANUM}+[\\+]*\n",
    "    OTHERS_C         0  {OTHERS}\n",
    "    </RegExps>\n",
    "\n",
    "In this case the process take into account the same **preprocess** module referenced before. The disadvantage of Freeling is the huge amount of code (+300 Mb) necessary for the complete instalation. On the contrary have a wonderful performance, is implemented in C++, have support for many languages and Spanish is the better one.\n",
    "\n",
    "### Pattern\n",
    "\n",
    "A Belgium application for data science and some task of natural language processing. It was made by the Computational Linguistic and Psycholinguistics Research Center (CLIPS), and have support for various languages as English and Spanish (see http://www.clips.uantwerpen.be/pattern or http://github.com/clips/pattern). Actually its main problem is the support for Python 3. \n",
    "\n",
    "~/pattern/text/en/wordnet/__init__.py:\n",
    "\n",
    "    def normalize(word):\n",
    "       57:     \"\"\" Normalizes the word for synsets() or Sentiwordnet[] by removing accents,\n",
    "       58          since WordNet does not take unicode.\n",
    "       \n",
    "As you can see this function is only to help the English Wordnet implementation to deal with its accents's incompatibilities.\n",
    "\n",
    "### Other python libs in pypi repository (TODO)\n",
    "\n",
    "* __Normalization__:\n",
    "* __Normalizr__:\n",
    "\n",
    "### A curiosity in Sklearn Library\n",
    "\n",
    "You can find a normalize method inside this library.\n",
    "\n",
    "    from sklearn.preprocessing import normalize\n",
    "    \n",
    "But, basically, this is to _\"Normalize samples individually to unit norm.\"_ (taken from ~/sklearn/preprocessing/data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4 Text Normalization Collection\n",
    "\n",
    "Use the _02.1_preprocessDocList.py_ script to complete this task.\n",
    "\n",
    "Details of the above script:\n",
    "\n",
    "- You may past pair file path, src-path, susp-path and out-path as parameters of the script.\n",
    "- It will start reading *preProcessedDocDict* file to hanle the list of previously preprocessed documents.\n",
    "\n",
    "    * in depth: the algh contains a dict named \"preProcessedDocDict\" to optimize the preprocess flow, as we can have the same susp or src doc in many cases, this is preprocess just the first time.\n",
    "\n",
    "- An example stand on the project folder:\n",
    "\n",
    "<font color='#4E2525'>__(virtualenv)$ python3 scripts/02.1_preprocessCaseList.py orig/pairs orig/src/ orig/susp/ norm/__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The script will use as default the data folder as working directory\n",
      "/home/abelma/01b_Paraphrase_Identification_beyond_sentences_the_case_of_Plagiarism_Detection/data\n",
      "Preprocessed cases:  1000 Valid cases:  1000\n",
      "Preprocessed cases:  2000 Valid cases:  2000\n",
      "tiempo total:  0.03257417678833008\n"
     ]
    }
   ],
   "source": [
    "%run scripts/02.1_preprocessCaseList orig/pairs orig/src/ orig/susp/ norm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "First the normalization process was conceptualized. Then it was specified the different rules defined by the author as steps in the normalization process implemented. Different real natural language text situations needs to be analysed to implement different rules that can process them. As was analized after that python classic libraries don't implement a similar normalization process, mainly due to the paradigm of convert text into numerical vectors. Only FreeLing API made a similar normalization process and divide it taken into account every classical situation in real unstructured texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "* Indurkhya, N. & Damerau, F. J. Herbrich, R. & Graepel, T. (Eds.) Handbook of Natural Language Processing CRC Press, 2010.\n",
    "<a id='Indurkhya2010'></a>\n",
    "\n",
    "* FreeLing User Manual, 2013.\n",
    "\n",
    "* Perkins, Jacob. Python 3 Text Processing with NLTK 3 Cookbook, Packt Publishing, 2014.\n",
    "\n",
    "* Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper. 2009. O’Reilly Media, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Many of the grammatical concept list on this tutorial script are not complete, for example Abbreviation is only superficially implemented. Pick up a grammatical concept, analyse the preprocess mudule implementation and try to improve it.\n",
    "\n",
    "2. Date recognition is not implemented in this tutorial scripts, made a quick RegExp based implementation and compared it with FreeLing Date recognition rules.\n",
    "\n",
    "3. The preprocess punctuation_filter method is the first step to correct the effects of some punctuations chars on the normalization process. Use a mathematical pdf book and first convert to text (we suggest using python-pdfminer) and then keep the result using the original implementation and then review all the regular expressions and try new ones and repeat the preprocess. After that compare the results. Save this files for future analysis in the alignment process.\n",
    "\n",
    "4. Try to implement a new normalization flow different to showed here, and compare the results counting the number of sentences. These ones will be usefull in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
