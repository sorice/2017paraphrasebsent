{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from preprocess.demo import preProcessFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Text Corpus\n",
    "\n",
    "The objectives of this notebook are:\n",
    "\n",
    "* To explain what the standard normalization process defined in Natural Language Processing is. [(2.1.1)](#Text_Normalization) \n",
    "* 2.1.2. As a second goal to show you some personal(made by the author) functions designed across the process of data normalization. \n",
    "* 2.1.3. To compare some shallow differences between some normalization methods implemented on classic NLP libraries, some of them on python and C++.\n",
    "* 2.1.4. Finally, to convert the whole initial corpus collection in a normalized new one.\n",
    "\n",
    "The first corpus used in this collection for phase 1 or _Data Preparation_ is the corpus for __Text Alignment__ task of PAN Scientific Event. PAN is a series of scientific events and shared tasks on digital text forensics and stylometry. The corpus used here is the __test corpus of 2013__ available [HERE](https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-13/pan13-data/pan13-text-alignment-test-and-training.zip). More corpus related with the same task could be found in [pan13-data](https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/).\n",
    "\n",
    "Due to the impossibility to process all the metrics (50+) in a single laptop, the resultant Paragraph Semantic Text Similarity Corpus (PSTS Corpus) will be published later with the help of the Wittylytics team. In substitution, phase 2 or _Text Similarity Calculation_ and the last experimental phase _Classifying Paraphrased Cases_ will be shown with [Microsoft Research Paraphrase Corpus](http://research.microsoft.com/en-us/downloads), which is a sentence pair corpus with two classes and shorter than PAN-PC-2013.\n",
    " \n",
    "    \n",
    "## Brief Summary\n",
    "\n",
    "It must be taken into account that the text collection, to which we are referring in the whole tutorial, contains spell errors and other kinds of errors coming from the output of pdf-to-text libraries or some automatic text generation algorithms (E.g. multiple line breaks inside a single sentence). So, although humans do not make these mistakes in real text, the automatic conversion or generation process can create them.\n",
    "\n",
    "In this phase we hope to obtain texts without end of sentences dots ambiguities, and the multi-words united by the underscore symbol `_`. Any other punctuation or non-letter symbol will be deleted.\n",
    "The result of this step will be used on the [next](02.2-Jaccard-Align-Preproc-to-Original-Sent) step to match the normalized sentences with the originals with a 100% precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Text Normalization\n",
    "<a id='Text_Normalization'></a>\n",
    "\n",
    "*\"Text normalization is a related step that involves merging different written forms of a\n",
    "token into a canonical normalized form; for example, a document may contain the equivalent tokens\n",
    "“Mr.”, “Mr”, “mister”, and “Mister” that would all be normalized to a single form.\"* [<a href=\"#Indurkhya2010\" title=\"Handbook of Natural Language Processing\"> (Indurkhya2010) </a>](#Indurkhya2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Non Standard Normalization Protocols\n",
    "\n",
    "As you can see in the former block, text normalization only includes token transformation. To simplify source codes in the next notebooks, the investigators must have at the end of the normalization phase a canonical text at all levels: morphological, lexical and syntactic. Trying to simulate the rich-text pattern recognition that brains do (bullets, section or chapter divisions, etc.) is not a good way. Faster algorithms need very plain texts.\n",
    "\n",
    "Here is an example. It is well known that if you have a capital letter after a dot, this symbol means the end of a sentence. But, what happens if you have the next composition: *\"H. Albot was a B.A. of Psychology.\"*. As you can see, the first dot isn't the end of the first sentence, in fact there is only one sentence. And as a second detail the second dot is correct, and is not the end of the sentence either. Probably the must useful string to get at the end of a normalization process could be: *\"Harry Albot was a Bachelor of Psychology.\"*. This implicates the analysis of proper names, abbreviation and acronyms, math symbols, etc.\n",
    "\n",
    "The protocols created by the author available in the *preprocess.norm* module inside the tutorial are:\n",
    "\n",
    "- URL analysis                                 -> trated as a contiguous string (http___google_com__)\n",
    "- Rare simbols (including math simbols)        -> converted to a canonical form (u'\\u03c0' = 'Pi')\n",
    "- ... points detection                         -> eliminated\n",
    "- Contiguous string                            -> multi-words are trated as a single token (text-reuse = text_reuse)\n",
    "- Separation of end of sentence dots           -> to avoid ambiguities (\"Hola. Hoy...\" = \"Hola . Hoy...\")\n",
    "- Abbreviation, Acronym and proper names       -> canonical form subtitution\n",
    "- Addition of last sentence end dot            -> dot addition at the end of the last sentence if not\n",
    "- Punctuation chars analysis                   -> a set of regular expressions to solve pdf-text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A real example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************PREPROCESSED TEXT******************************\n",
      "For other optional flags of opencv_createsamples see the official documentation at http___Docs_opencv_org_doc_user_guide_ug_traincascade_html . 99 www_it_ebooks_info . . Generating Haar Cascades for Custom 8_4 Targets . Creating cascade by running . opencv_traincascade 3_ anoche . 4 Después . . Over 110 recipes to master this full_stack Python web framework 1 . Take your web2py skills to the next level by dipping into delicious usable recipes in this cookbook 2 . Learn advanced web2py usage from building advanced forms to creating PDF reports 3 . Written by developers of the web2py project with plenty of code examples for interesting and comprehensive learning . . Mi correo es abelm_uclv_cu . A ver si lo coge . Please check www_PacktPub_com for information on our titles . www_it_ebooks_info . . Learning SciPy for Numerical and . Scientific Computing . ISBN 978_1_78216_162_2 . Ahora probaremos la división al final de cada línea . U_S_ es un país desarrollado luego de la guerra la NASA fue creada . Reunieronse pues Lic_ Dr_ Ing_ Ph_D_ y todos los que de una forma u otra poseían títulos honoríficos . . Elimina las comillas dobles y simples al final y al principio de la oración . . Probando tildes y caracteres de español áéíóú ñÑÁÉÍÓÚ . Jhon is a word with a contraction . Oración cerrada con comillas seguida por otra . Well writed . Oración cerrada con comillas seguida de salto de línea . . Oración seguida por otra entre comillas . La otra oración . \n",
      "*****************************HANDMADE PREPROCESSED TEXT******************************\n",
      "For other optional flags of opencv_createsamples see the official documentation at http___Docs_opencv_org_doc_user_guide_ug_traincascade_html .  99 www_it_ebooks_info .  Generating Haar Cascades for Custom 8_4 Targets .  Creating cascade by running .  opencv_traincascade 3_ anoche .  4 Después .  Over 110 recipes to master this full_stack Python web framework 1 .  Take your web2py skills to the next level by dipping into delicious usable recipes in this cookbook 2 .  Learn advanced web2py usage from building advanced forms to creating PDF reports 3 .  Written by developers of the web2py project with plenty of code examples for interesting and comprehensive learning .  Mi correo es abelm_uclv_cu .  A ver si lo coge .  Please check www_PacktPub_com for information on our titles .  www_it_ebooks_info .  Learning SciPy for Numerical and .  Scientific Computing .  ISBN 978_1_78216_162_2 .  Ahora probaremos la división al final de cada línea .   U_S_ es un país desarrollado luego de la guerra la NASA fue creada .  Reunieronse pues Lic_ Dr_ Ing_ Ph_D_ y todos los que de una forma u otra poseían títulos honoríficos .  Elimina las comillas dobles y simples al final y al principio de la oración .  Probando tildes y caracteres de español: áéíóú ñÑÁÉÍÓÚ . Jhon is a word with a contraction .  Oración cerrada con comillas seguida por otra .  Well writed .  Oración cerrada con comillas seguida de salto de línea .  Oración seguida por otra entre comillas.   La otra oración . \n",
      "\n",
      "Automatic end of sentence count of preprocessed text: 35\n",
      "Human end of sentence count of original test text: 28\n"
     ]
    }
   ],
   "source": [
    "text_orig = open('test/test_text.txt').read()\n",
    "preproc_text = preProcessFlow(text_orig)\n",
    "\n",
    "print('*****************************PREPROCESSED TEXT******************************')\n",
    "print(preproc_text)\n",
    "\n",
    "text_human = open('test/test_text_human_analysis.txt').read()\n",
    "text_human2, temp = re.subn(r'\\n',' ',text_human)\n",
    "print('*****************************HANDMADE PREPROCESSED TEXT******************************')\n",
    "print (text_human2)\n",
    "\n",
    "print ('\\nAutomatic end of sentence count of preprocessed text:',preproc_text.count('.'))\n",
    "print ('Human end of sentence count of original test text:', text_human2.count('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Other Normalization Processes\n",
    "\n",
    "Some original source codes of every library are included to show you the limitations of every normalization method on each API.\n",
    "\n",
    "### NLTK\n",
    "\n",
    "Python's Natural Language Toolkit (NLTK) is a suite of libraries that has become one of the best tools for prototyping and building natural language processing systems. It is developed under the guide of Stanford Professor Steven Bird (see http://www.nltk.org/).\n",
    "\n",
    "~/nltk/tag/perceptron.py\n",
    "\n",
    "    def normalize(self, word):\n",
    "        '''\n",
    "        Normalization used in pre-processing.\n",
    "        - All words are lower cased\n",
    "        - Digits in the range 1800-2100 are represented as !YEAR;\n",
    "        - Other digits are represented as !DIGITS\n",
    "\n",
    "This function returns a TAG for every type of normalization task but not the normalized word.\n",
    "\n",
    "The other references to normalization that can be found inside NLTK deal with mathematical operations to make two different datas comparable, and in a few cases to put some strings into lower case.\n",
    "\n",
    "### Freeling\n",
    "\n",
    "The FreeLing package consists of a library providing language analysis services (such as morphological analysis, date recognition, PoS tagging, etc.). It was made by TALP Research Center and the Universitat Politecnica de Catalunya (see http://nlp.lsi.upc.edu/freeling).\n",
    "\n",
    "The FreeLing API divides the normalization task into modules that implement the detection of some patterns: Numbers, Punctuation, Dates, Multiword, Name Entity and Quantity.\n",
    "\n",
    "As you can review in: _~/freeling-3.1/src/include/freeling/morfo/tokenizer.h_ and _~/freeling-3.1/src/libfreeling/tokenizer.cc_ there are some basical rules you can use to analyse the logic of the normalization process inside _FreeLing_. In fact we have some of those rules in here (taken from _~/freeling-3.1/data/en/tokenizer.dat_).\n",
    "\n",
    "    <RegExps>\n",
    "    INDEX_SEQUENCE   0  (\\.{4,}|-{2,}|\\*{2,}|_{2,}|/{2,})\n",
    "    INITIALS1 \t 1  ([A-Z](\\.[A-Z])+)(\\.\\.\\.)\n",
    "    INITIALS2 \t 0  ([A-Z]\\.)+\n",
    "    TIMES            0  (([01]?[0-9]|2[0-4]):[0-5][0-9])\n",
    "    NAMES_CODES\t 0  ({ALPHA}|{SYMNUM})*[0-9]({ALPHA}|[0-9]|{SYMNUM}+{ALPHANUM})*\n",
    "    THREE_DOTS \t 0  (\\.\\.\\.)\n",
    "    QUOTES\t         0  (``|<<|>>|'')\n",
    "    MAILS \t         0  {ALPHANUM}+([\\._]{ALPHANUM}+)*@{ALPHANUM}+([\\._]{ALPHANUM}+)*\n",
    "    URLS1 \t         0  ((mailto:|(news|http|https|ftp|ftps)://)[\\w\\.\\-]+|^(www(\\.[\\w\\-]+)+))\n",
    "    URLS2            1  ([\\w\\.\\-]+\\.(com|org|net))[\\s]\n",
    "    CONTRACT_0a      1  (i'(m|d|ll|ve))({NOALPHANUM}|$) CI\n",
    "    CONTRACT_0b      1  ((you|we|they|who)'(d|ll|ve|re))({NOALPHANUM}|$) CI\n",
    "    CONTRACT_0c      1  ((he|she|it|that|there)'(d|ll|s))({NOALPHANUM}|$) CI\n",
    "    CONTRACT_0d      1  ((let|what|where|how|who)'s)({NOALPHANUM}|$) CI\n",
    "    CONTRACT1        1  ({ALPHA}+)('([sdm]|ll|ve|re)({NOALPHANUM}|$)) CI\n",
    "    CONTRACT2        1  ('([sdm]|ll|ve|re|tween))({NOALPHANUM}|$) CI\n",
    "    KEEP_COMPOUNDS   0  {ALPHA}+(['_\\-\\+]{ALPHA}+)+\n",
    "    *ABREVIATIONS1   0  (({ALPHA}+\\.)+)(?!\\.\\.)\n",
    "    WORD             0  {ALPHANUM}+[\\+]*\n",
    "    OTHERS_C         0  {OTHERS}\n",
    "    </RegExps>\n",
    "\n",
    "In this case, the process takes into account the same **preprocess** module referenced before. The disadvantage of Freeling is the huge amount of code (+300 Mb) necessary for the complete installation. On the other hand, it has a wonderful performance, is implemented in C++ and has support for many languages, Spanish being the most supported by this platform.\n",
    "\n",
    "### Pattern\n",
    "\n",
    "It is a Belgium application for data science and some task of natural language processing. It was made by the Computational Linguistic and Psycholinguistics Research Center (CLIPS), and has support for various languages as English and Spanish (see http://www.clips.uantwerpen.be/pattern or http://github.com/clips/pattern). Currently, its main problem is that there is no version for Python 3. \n",
    "\n",
    "~/pattern/text/en/wordnet/__init__.py:\n",
    "\n",
    "    def normalize(word):\n",
    "       57:     \"\"\" Normalizes the word for synsets() or Sentiwordnet[] by removing accents,\n",
    "       58          since WordNet does not take unicode.\n",
    "       \n",
    "As you can see this function only helps the English Wordnet implementation to deal with its accent incompatibilities.\n",
    "\n",
    "### Other python libs in pypi repository\n",
    "\n",
    "* __Normalization__\n",
    "* __Normalizr__\n",
    "\n",
    "### A curiosity in Sklearn Library\n",
    "\n",
    "You can find a normalize method inside this library.\n",
    "\n",
    "    from sklearn.preprocessing import normalize\n",
    "    \n",
    "But, basically, this is to _\"Normalize samples individually to unit norm.\"_ (taken from ~/sklearn/preprocessing/data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4 Text Normalization Collection\n",
    "\n",
    "Use the _02.1_preprocessDocList.py_ script to complete this task.\n",
    "\n",
    "Details of the above script:\n",
    "\n",
    "- You may pass pair file path, src-path, susp-path and out-path, as parameters of the script.\n",
    "- It will start reading *preProcessedDocDict* file to handle the list of previously preprocessed documents.\n",
    "\n",
    "    * in depth: the algorithm contains a dict named \"preProcessedDocDict\" to optimize the preprocess flow as we can have the same susp or src doc in many cases; these are preprocessed just the first time.\n",
    "\n",
    "The below box shows how to execute the script for the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The script will use as default the data folder as working directory\n",
      "/home/abelma/01b_Paraph/data\n",
      "Preprocessed cases:  1000 Valid cases:  1000\n",
      "Preprocessed cases:  2000 Valid cases:  2000\n",
      "tiempo total:  24.462116241455078\n"
     ]
    }
   ],
   "source": [
    "%run scripts/02.1_preprocessCaseList orig/pairs orig/src/ orig/susp/ norm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* The normalization process was conceptualized. \n",
    "* Then the different rules defined by the author were specified as steps in the implemented normalization process.\n",
    "* Different real natural language text situations need to be analysed to implement different rules that can process them. \n",
    "* The python classic libraries don't implement a similar normalization process as defined in this notebook, mainly due to the paradigm of converting text into numerical vectors.\n",
    "* The selected process has many similarities with the normalization process detailed in FreeLing platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "* Indurkhya, N. & Damerau, F. J. Herbrich, R. & Graepel, T. (Eds.) Handbook of Natural Language Processing CRC Press, 2010.\n",
    "<a id='Indurkhya2010'></a>\n",
    "\n",
    "* FreeLing User Manual, 2013.\n",
    "\n",
    "* Perkins, Jacob. Python 3 Text Processing with NLTK 3 Cookbook, Packt Publishing, 2014.\n",
    "\n",
    "* Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper. 2009. O’Reilly Media, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Many of the grammatical concept list on this tutorial script are not complete, for example Abbreviation is only superficially implemented. Pick up a grammatical concept, analyse the preprocess mudule implementation and try to improve it.\n",
    "\n",
    "2. Date recognition is not implemented in this tutorial scripts, made a quick RegExp based implementation and compared it with FreeLing Date recognition rules.\n",
    "\n",
    "3. The preprocess punctuation_filter method is the first step to correct the effects of some punctuations chars on the normalization process. Use a mathematical pdf book and first convert to text (we suggest using python-pdfminer) and then keep the result using the original implementation and then review all the regular expressions and try new ones and repeat the preprocess. After that compare the results. Save this files for future analysis in the alignment process.\n",
    "\n",
    "4. Try to implement a new normalization flow different to showed here, and compare the results counting the number of sentences. These ones will be usefull in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
