{
 "metadata": {
  "name": "",
  "signature": "sha256:f292fc0ab42a048c0d32c7ad080419c6849c3d9ab8793dcace7ea519f2c625ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Evaluating the Best Normalization after Alignment\n",
      "\n",
      "The objetive of this notebook is to show you across a measure if we had a good *Normalization* process based on Plagiarism Detection Corpus tags values."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Problem of Controling Fixed Sentence Boundaries\n",
      "\n",
      "For plagiarism detection or text reuse analysis you will must select from two whole text document the similar fragments. However the techniques of text reuse some times does not need a sentence limit to make a similarity calculation, they can use tokens or even chars to delimit the boundaries of compared fragments. Another reality is that in artificially generated cases the algorithm can capture inside the fragment just a chunk of the beginning or ending sentence.\n",
      "\n",
      "But in real scenarios humans take fragments compose by complete ideas (sentences) and then, modified or not, they use it in a new document.\n",
      "\n",
      "As it was explained [before](02.1-Normalizing-Text-Corpus.ipynb) the normalization process intend to normalize tokens and delete end of sentece ambiguities or to define very well the end of sentence dots. **Alert:** The sentence tokenization is an open problem in NLP, this is an indirect way to show the quality of normalization made by the author.\n",
      "\n",
      "Then for future text reuse experiments it will be measure the cuality of sentence division based on its pertinence to the annotated fragment into the plag case XML doc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "../"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Math Formalization of Normalized Sentences % in a Case\n",
      "(based on Case XML Information)\n",
      "\n",
      "After [Alignment Process](02.2c-Jaccard-Align-Preproc-to-Original-Sent.ipynb) a <font color='#F84825'>new text structure</font> was obtained. Open the file [suspicious-document00007.txt](files/data/aligned/susp/suspicious-document00007.txt) to see that previous-process result structure in the form <font color='#F84825'>\n",
      "   $(id_K,normalized-sentence_K,original\\,offset_{sentence\\,K},original\\,offset+length_{sentence\\,K})$\n",
      "</font>        \n",
      "\n",
      "The fragment attributes are showed below for the same document in the xml file [suspicious-document00007-source-document00382.xml](files/data/PAN-PC-2013/orig/03-random-obfuscation/suspicious-document00007-source-document00382.xml)\n",
      "        \n",
      "<body>\n",
      "<pre style='color:#1f1c1b;background-color:#ffffff;'>\n",
      "<b>&lt;document</b><span style='color:#006e28;'> reference=</span><span style='color:#aa0000;'>&quot;suspicious-document00007.txt&quot;</span><b>&gt;</b>\n",
      "<b>&lt;feature</b><span style='color:#006e28;'> name=</span><span style='color:#aa0000;'>&quot;plagiarism&quot;</span><span style='color:#006e28;'> obfuscation=</span><span style='color:#aa0000;'>&quot;random&quot;</span><span style='color:#006e28;'> obfuscation_degree=</span><span style='color:#aa0000;'>&quot;0.4694788492120119&quot;</span><span style='color:#006e28;'> source_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> source_offset=</span><span style='color:#aa0000;'>&quot;0&quot;</span><span style='color:#006e28;'> source_reference=</span><span style='color:#aa0000;'>&quot;source-document00382.txt&quot;</span><span style='color:#006e28;'> this_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> this_offset=</span><span style='color:#aa0000;'>&quot;9449&quot;</span><span style='color:#006e28;'> type=</span><span style='color:#aa0000;'>&quot;artificial&quot;</span> <b>/&gt;</b>\n",
      "<b>&lt;/document</b><b>&gt;</b>\n",
      "</pre>\n",
      "</body>\n",
      "        \n",
      "Having this attributes values in advance. How do we calculate the percent of a sentence inside the fragment?\n",
      "\n",
      "Making a mathematical reasoning it can be found that in the three cases indicated in figure 1 the general formula to solve this problem will be:\n",
      "\n",
      "$$\\%\\,of\\,sentence_{K}\\,that\\,belong\\,to\\,the\\,fragment_X = \\frac{min(L_K,L_X) - max(Offset_K,Offset_X)}{L_K-Offset_K}$$\n",
      "\n",
      "Where $L_K = Offset_K + Length_K$, that means the position of the last character that belongs to te $sentence_K$.\n",
      "\n",
      "<center><strong>Elaborated diagram to show possible situations after sentence normalization</strong></center></br>\n",
      "<table border=0 cellspacing=10> \n",
      "    <caption align=\"bottom\"> </br><em>Figura 2.4.1: The three possible sentence pertinence to the fragment</em>\n",
      "    </caption> \n",
      "<tr align=\"center\">\n",
      "    <th> <img src=\"imgs/PercentSentBelongFragment.jpg\" height=200px width=300px alt=\"*\" \n",
      "        align=\"center\"> </th>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "To avoid the problem that many single documents can appear in more than one case we need to keep the $\\%\\,of\\,sentence_{K}\\,that\\,belong\\,to\\,the\\,fragment_X$ related to the sentence. Creating one new text per case based on a new structure (all values are numerical, ideal to load it in a numpy array):\n",
      "\n",
      "    /norm/quality/suspicious-document00007-source-document00382.xml\n",
      "   $(id_{sentence_P\\,susp},offset_{sentence_P},offset+length_{sentence_P},\\%\\,sentence_{P}\\, \\in\\,susp_{fragment\\,X},id_{fragment\\,X})$\n",
      "   \n",
      "   $\\hspace{2cm}\\vdots$ \n",
      "   \n",
      "   $(id_{sentence_Q\\,src},offset_{sentence_Q},offset+length_{sentence_Q},\\%\\,sentence_{Q}\\, \\in\\,src_{fragment\\,X},id_{fragment\\,X})$\n",
      "   \n",
      "   $\\hspace{2cm}\\vdots$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementation\n",
      "\n",
      "TODO: aqu\u00ed va el algoritmo y la explicaci\u00f3n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scripts import PANXml\n",
      "\n",
      "def calcPercent(actualFile, Ox, Lx, fragmentID, thresh):  \n",
      "    percent = [];case_percent = []\n",
      "    with open(actualFile) as doc:\n",
      "        for num,line in enumerate(doc):\n",
      "            ID,sent,Ok,lenk = line.split('\\t')\n",
      "            Ok = int(Ok)\n",
      "            Lk = int(lenk)\n",
      "            if Ok < Lx and Lk > Ox:\n",
      "                perc = (min(Lk,Lx)-max(Ok,Ox))/float(Lk-Ok)\n",
      "                if perc > thresh:\n",
      "                    percent.append(ID+'\\t'+str(Ok)+'\\t'+lenk[:-1]+'\\t'+str(perc)+'\\t'+str(fragmentID+1)+actualFile[-9:-4])\n",
      "                    case_percent.append(perc)\n",
      "    return percent, case_percent\n",
      "\n",
      "def calc_sentPercentCase(inputfileName,alignedCollectionPath, xmlColecctionPath,threshold=0.3,printout=False):\n",
      "    susp, src = inputfileName.split()\n",
      "    case = PANXml(xmlColecctionPath+susp[:-4]+'-'+src[:-4]+'.xml')\n",
      "    result = []; case_perc = []\n",
      "    \n",
      "    for i,file in enumerate([alignedCollectionPath+'susp/'+susp,alignedCollectionPath+'src/'+src]):\n",
      "        for fragmentID,fragment in enumerate(case.fragmentList):\n",
      "            #The non-plagiarism cases have an empty XML, the next line is to avoid thems\n",
      "            if len(case.fragmentList) > 0:\n",
      "                if i == 0:\n",
      "                    Ox = int(fragment.suspOffset)\n",
      "                    Lx = Ox+int(fragment.suspLength)\n",
      "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
      "                    result.extend(a);case_perc.extend(b)\n",
      "                else:\n",
      "                    Ox = int(fragment.srcOffset)\n",
      "                    Lx = Ox+int(fragment.srcLength)\n",
      "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
      "                    result.extend(a);case_perc.extend(b)\n",
      "                if printout:\n",
      "                    print('Case %d, Offset=%d, Length=%d' % (fragmentID, Ox, Lx-Ox))\n",
      "\n",
      "    #Write in a single doc the percent result line by line for both docs of the case\n",
      "    report = open(alignedCollectionPath+'quality/'+inputfileName, 'w')\n",
      "    if printout:\n",
      "        print('Sents\\tOffset\\tLength\\t%InFrag\\tFragID')\n",
      "    for line in result:\n",
      "        if printout:\n",
      "            print(line)\n",
      "        report.write(line+'\\n')\n",
      "    report.close()\n",
      "    \n",
      "    data = np.array(case_perc,dtype=np.float64)\n",
      "    #print('data:',case_perc)\n",
      "    return float(data.mean()), case_perc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing for a case with only one pair of fragments"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inputFile = 'suspicious-document00122.txt source-document01169.txt'\n",
      "alignedCollectionPath = '../align/'\n",
      "xmlColecctionPath = '../orig/paraph/'\n",
      "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
      "print(\"Case Percent Quality =\",A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Case 0, Offset=0, Length=556\n",
        "Case 0, Offset=0, Length=999\n",
        "Sents\tOffset\tLength\t%InFrag\tFragID\n",
        "0\t0\t514\t1.0\t100122\n",
        "1\t514\t555\t1.0\t100122\n",
        "0\t0\t40\t1.0\t101169\n",
        "1\t40\t69\t1.0\t101169\n",
        "2\t69\t1000\t0.9989258861439313\t101169\n",
        "Case Percent Quality = 0.9997851772287862\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing for a case with more than one pair of fragments"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inputFile = 'suspicious-document00070.txt source-document01504.txt'\n",
      "alignedCollectionPath = '../align/'\n",
      "xmlColecctionPath = '../orig/paraph/'\n",
      "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
      "print(\"Case Percent Quality =\",A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Case 0, Offset=3505, Length=184\n",
        "Case 1, Offset=4727, Length=311\n",
        "Case 0, Offset=211, Length=387\n",
        "Case 1, Offset=1021, Length=422\n",
        "Sents\tOffset\tLength\t%InFrag\tFragID\n",
        "30\t3503\t3594\t0.978021978021978\t100070\n",
        "31\t3594\t3627\t1.0\t100070\n",
        "32\t3627\t3653\t1.0\t100070\n",
        "33\t3653\t3690\t0.972972972972973\t100070\n",
        "42\t4725\t4819\t0.9787234042553191\t200070\n",
        "43\t4819\t4949\t1.0\t200070\n",
        "44\t4949\t5037\t1.0\t200070\n",
        "4\t211\t349\t1.0\t101504\n",
        "5\t349\t422\t1.0\t101504\n",
        "6\t422\t499\t1.0\t101504\n",
        "7\t499\t598\t1.0\t101504\n",
        "13\t1021\t1101\t1.0\t201504\n",
        "14\t1101\t1171\t1.0\t201504\n",
        "15\t1171\t1183\t1.0\t201504\n",
        "16\t1183\t1276\t1.0\t201504\n",
        "17\t1276\t1444\t0.9940476190476191\t201504\n",
        "Case Percent Quality = 0.9952353733936181\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementation for the whole Collection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Generate the percent for all sentences in every preprocessed case\n",
      "#Load all cases in pairs\n",
      "alignedCollectionPath = '../align/'\n",
      "xmlColecctionPath = '../orig/paraph/'\n",
      "\n",
      "import time\n",
      "\n",
      "norm_percent = []\n",
      "sent_perc = []\n",
      "threshold=0.3\n",
      "\n",
      "with open('../align/align_pairs') as doc:\n",
      "    init = time.time()\n",
      "    for line in doc:\n",
      "        line = line[:-1]\n",
      "        perct, sent = calc_sentPercentCase(line, alignedCollectionPath, \n",
      "                                           xmlColecctionPath,threshold)\n",
      "        norm_percent.append(perct)\n",
      "        sent_perc.extend(sent)\n",
      "    timef = time.time() - init\n",
      "    print(timef)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.7511546611785889\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python3/dist-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
        "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(norm_percent)\n",
      "quality_norm_vector = np.array(norm_percent,dtype=np.float64)\n",
      "print(quality_norm_vector.shape)\n",
      "print(quality_norm_vector[800:850])\n",
      "print('Total useful sentences:',len(sent_perc))\n",
      "quality_sent_norm = np.array(sent_perc,dtype=np.float64)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1445,)\n",
        "[        nan         nan         nan         nan         nan         nan\n",
        "         nan         nan         nan         nan         nan         nan\n",
        "         nan         nan         nan         nan         nan         nan\n",
        "         nan         nan         nan         nan         nan         nan\n",
        "         nan         nan         nan         nan         nan         nan\n",
        "         nan         nan         nan         nan  0.99618056  0.99696953\n",
        "  0.89050748  0.96716298  1.          1.          0.93700246  0.93842185\n",
        "  0.99747475  0.99585104  0.99547051  0.96577381  0.99399674  0.99604722\n",
        "  0.99179626  0.9954023 ]\n",
        "Total useful sentences: 7739\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sentence Normalization Quality Measure\n",
      "\n",
      "$Quality = \\frac{\\sum_{k=0}^n (\\%\\,of\\,sentence_k\\,\\,inside\\,fragment>\\mu)}{total\\,sentences\\,with\\,\\% > \\mu}$ \n",
      "\n",
      "Where $n+1$ is the total sentences of the analised fragment and $\\mu$ is the minimum percent of a sentence inside the fragment to consider it that belong to the fragment case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import DataFrame\n",
      "miu = 0.9\n",
      "miu_vector = []\n",
      "miu2 = 0.5\n",
      "miu_vector2 = []\n",
      "\n",
      "Quality = quality_norm_vector.mean()\n",
      "print ('Quality Type:',type(Quality),'\\nQuality mean:',Quality,\n",
      "       '.\\nNew data type is needed: DataFrame (DF).') #this is for show that np-array do not handle NaN values, DataFrame object convertion is needed\n",
      "\n",
      "P = DataFrame(quality_norm_vector, columns=['percent'])\n",
      "Qn = P.mean()\n",
      "print('DF Quality based on cases-quality-average: %.4f' % (Qn['percent']))\n",
      "\n",
      "print('Total sentences:',len(sent_perc))\n",
      "Q = DataFrame(quality_sent_norm, columns=['percent'])\n",
      "Qs = Q.mean()\n",
      "print('DF Quality based on total sentences with percent > %.2f inside de case: %.4f' % (threshold,Qs['percent']))\n",
      "\n",
      "for sent in quality_sent_norm:\n",
      "    if sent > miu:\n",
      "        miu_vector.append(sent)\n",
      "\n",
      "for sent in quality_sent_norm:\n",
      "    if sent < miu2:\n",
      "        miu_vector2.append(sent)\n",
      "        \n",
      "print('# of normalized sentences with +%.2f%% of its size' % (miu*100),\n",
      "      'inside its respectively case-frag: %d' % (len(miu_vector)))\n",
      "print('# de oraciones norm con +%.2f%% de su tama\u00f1o fuera del caso: %d' % \n",
      "      (100-miu2*100,len(miu_vector2)))\n",
      "print('Quality based on miu: %.4f' % (len(miu_vector)/len(Q)))\n",
      "print('Quality based on miu2: %.4f' % (1.0-len(miu_vector2)/len(Q)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Quality Type: <class 'numpy.float64'> \n",
        "Quality mean: nan .\n",
        "New data type is needed: DataFrame (DF).\n",
        "DF Quality based on cases-quality-average: 0.9773\n",
        "Total sentences: 7739\n",
        "DF Quality based on total sentences with percent > 0.30 inside de case: 0.9837\n",
        "# of normalized sentences with +90.00% of its size inside its respectively case-frag: 7450\n",
        "# de oraciones norm con +50.00% de su tama\u00f1o fuera del caso: 73\n",
        "Quality based on miu: 0.9627\n",
        "Quality based on miu2: 0.9906\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Conclusions\n",
      "\n",
      "El indicador de calidad de la normalizaci\u00f3n es alto. Significa que un 96%  de las oraciones normalizadas obtenidas tienen m\u00e1s del 90% de su longitud dentro del caso definido en el XML. Y menos de un 3% est\u00e1n tienen menos del 30% de su tama\u00f1o dentro del caso."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Questions\n",
      "\n",
      "1. Implemente otras medidas de calidad basado en lo aprendido.\n",
      "\n",
      "2. Implemente las ecuaciones de calidad descritas y no formuladas en este ep\u00edgrafe.\n",
      "\n",
      "3. Combine algunas de las medidas evaluadas, pruebe luego con dos versiones diferentes de la normalizaci\u00f3n e interprete el resultado. Sugerencia elimine el reconocimiento de abreviaturas o contracciones."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# References and Resources"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}