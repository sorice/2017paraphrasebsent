{
 "metadata": {
  "name": "",
  "signature": "sha256:300c5580c6f6ff48485a6b7b0b06a4aa2029b7f40f1bbf526604094764bd2947"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sentence Alignment Algorithm Phase\n",
      "\n",
      "The objective of this phase is to get a new text structure with normalized sentences and its original related by the offset and length properties. E.g.:\n",
      "\n",
      "<a id='Aligned_Text_Structure'></a>\n",
      "*suspicious-document00XYZ.txt*\n",
      "\n",
      "   $(id_K,normalized-sentence_K,original-offset_{sentence\\,K},original-length_{sentence\\,K})$\n",
      "\n",
      "This is useful for example when you are working in a real plagiarism detection or text-reuse detection applications and you need to show to the users the similarity result between to preprocessed fragments or sentences in its original form (pdf, html, etc).  And, in addition, is very safe to work with duplicated and transformed objects very well related to the originals, after have them you can do subsequent transformations to the copy-object and never loss the original length and position of this sentence or fragment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Alignment Algorithm based on Smith-Waterman Distance\n",
      "\n",
      "El algoritmo final necesita de 4 funciones auxiliares:\n",
      "\n",
      "1. Funci\u00f3n de reemplazo de caracteres en el texto original con los cuales Smith-Waterman falla (*Ej: los saltos de l\u00ednea*) (**normilize**)\n",
      "2. Funci\u00f3n para recorrer la lista de oraciones del texto preprocesado (**getSentA**).\n",
      "3. Funci\u00f3n para recorrer todos los segmentos del texto original terminados en '.' desde un punto inicial *offsetB*, y teniendo como datos el punto anterior y el punto siguiente (**getSentB**).\n",
      "4. Jaccard function to validate Smith-Watermna alignment (**Jaccard**)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Original Text Normalization Function\n",
      "\n",
      "To improve the precision of Smith-Waterman algorithm is necessary to convert some punctuation situation.\n",
      "The main feature here is that whole regular expressions and functiones used does not change the length of the original text. The [previous](02.1-Normalizing-Text-Corpus.ipynb) normalization process it doest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('/home/abelm')\n",
      "from preprocess.methods import add_text_end_dot, abbrev_recognition_support, del_contiguous_point_support \n",
      "from preprocess.methods import contiguos_string_recognition_support\n",
      "\n",
      "def normalize(text_orig):\n",
      "    replacement_patterns = [(r'[:](?=\\s*?\\n)','##1'),\n",
      "                            (r'\\xc2|\\xa0',' '),\n",
      "                            (r'(\\w\\s*?):(?=\\s+?[A-Z]+?)|(\\w\\s*?):(?=\\s*?\"+?[A-Z]+?)','\\g<1>##2'),\n",
      "                            (r'[?!]','##3'),\n",
      "                            (r'(\\w+?)(\\n)(?=[\"$%()*+&,-/;:\u00bf\u00a1<=>@[\\\\]^`{|}~\\t\\s]*(?=.*[A-Z0-9]))','\\g<1>##4'), # any alphanumeric char\n",
      "                            # follow by \\n follow by any number of point sign follow by a capital letter, replace by alphanumerig+.\n",
      "                            (r'(\\w+?)(\\n)(?=[\"$%()*+&,-/;:\u00bf\u00a1<=>@[\\\\]^`{|}~\\t\\s\\n]*(?=[a-zA-Z0-9]))','\\g<1>##5'),# any alphanumeric char\n",
      "                            # follow by \\n follow by any number of point sign follow by a letter, replace by alphanumerig+.\n",
      "                            (r'[:](?=\\s*?)(?=[\"$%()*+&,-/;:\u00bf\u00a1<=>@[\\\\]^`{|}~\\t\\s]*[A-Z]+?)','##6'),\n",
      "                            (r'(\\w+?\\s*?)\\|','\\g<1>##7'),\n",
      "                            (r'\\n(?=\\s*?[A-Z]+?)','##8'),\n",
      "                            (r'##\\d','apdbx'),\n",
      "                            ]\n",
      "    \n",
      "    for (pattern, repl) in replacement_patterns:\n",
      "            (text_orig, count) = re.subn(pattern, repl, text_orig)\n",
      "    \n",
      "    text_orig = del_contiguous_point_support(text_orig)\n",
      "    text_orig = contiguos_string_recognition_support(text_orig)\n",
      "    text_orig = abbrev_recognition_support(text_orig)\n",
      "    text_orig = re.sub(r'apdbx+','.', text_orig)\n",
      "    text_orig = add_text_end_dot(text_orig)#append . final si el \u00faltimo caracter no tiene punto, evita un ciclo infinito al final.\n",
      "    return text_orig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Get Sentence A\n",
      "\n",
      "Get all sentences in preprocessed text. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def getSentA(text):\n",
      "    offset = 0\n",
      "    for i in re.finditer('\\.',text):\n",
      "        sentA = text[offset:i.end()]\n",
      "        yield sentA, offset, i.end()\n",
      "        offset = i.end()+1\n",
      "\n",
      "preprocessed_text = open('../norm/susp/suspicious-document00184.txt').read()\n",
      "        \n",
      "for i,(sentA, offset, length) in enumerate(getSentA(preprocessed_text)):\n",
      "    print (i,sentA, offset, length)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 The is a 5 acre 20 000 m located in the of located in . 0 55\n",
        "1 The zoo is operated by the in partnership with the . 56 108\n",
        "2 Queens Zoo zoo New York City borough Queens Flushing Meadows_Corona Park Wildlife Conservation Society New York City Department of Parks and Recreation . 109 262\n",
        "3 The Zoo opened on with the ceremonial ribbon cut by . 263 316\n",
        "4 The zoo is home mostly to animals native to North America . 317 376\n",
        "5 The Queens Zoo is the only one of the five zoos in New York City to exhibit . 377 454\n",
        "6 The zoo was constructed on the site of the and the zoo is aviary is a designed by and used during the 1964 Fair . 455 568\n",
        "7 October 26 1968 Robert Moses 2 Spectacled Bears 1964 New York World is Fair geodesic dome Buckminster Fuller 1  . 569 682\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Get Sentence B\n",
      "\n",
      "<center><strong>Diagrama simplificado getSentB function.</strong></center></br>\n",
      "<table border=0 cellspacing=10> \n",
      "    <caption align=\"bottom\"> </br><em>Figura 2.3.1: Esquema del Algoritmo Get Sentence B</em>\n",
      "    </caption> \n",
      "<tr align=\"center\">\n",
      "    <th> <img src=\"imgs/getSentB.jpg\" height=800px width=1200px alt=\"*\" \n",
      "        align=\"center\"> </th>\n",
      "    <th> </th>\n",
      "    <td> \n",
      "        <p> El algoritmo comienza a buscar en el texto original el caracter '.'  m\u00e1s cercano al offsetB (*$\\|\\overrightarrow{sB}\\|=0 \\Longrightarrow sentLength = 0$*).\n",
      "        Una vez encontrado denota como punto previo este nuevo punto para una nueva llamada (*$sentLength = \\|\\overrightarrow{sB}\\|+1$*), \n",
      "        luego calcula el segmento, y define el punto siguiente como $Offset + \\|\\overrightarrow{sB}\\|$ *(len segmento definido por el . encontrado)*.\n",
      "        En la siguiente corrida el algoritmo encontrar\u00e1 el '.' m\u00e1s cercano a partir de **sB** \n",
      "        calculando el nuevo punto siguiente(*sB`*). Y as\u00ed sucesivamente\n",
      "        ante cada llamada en la funci\u00f3n *getSentB*. Es en la funci\u00f3n de alineaci\u00f3n donde se establece la condici\u00f3n \n",
      "        de parada cuando el score de Smith-Waterman es m\u00e1ximo entre el *$\\|\\overrightarrow{sB^{n`}}\\|$ y *length sentA*. \n",
      "        </p>\n",
      "    </td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getSentB(text2, offsetB, nextPoint,sentLength):\n",
      "    posB = text2[offsetB+sentLength:].find('.')\n",
      "    sentLength += posB+1\n",
      "    sentB = text2[offsetB:offsetB+sentLength]\n",
      "    nextPoint = offsetB + sentLength\n",
      "    return sentB, nextPoint, sentLength\n",
      "\n",
      "text_orig = open('../susp/suspicious-document00184.txt').read()\n",
      "\n",
      "offsetB = 0;nextPoint = 0;sentLength=0\n",
      "\n",
      "for i in range(text_orig.count('.')):\n",
      "    sentB, nextPoint, sentLength = getSentB(text_orig,offsetB,nextPoint,sentLength)\n",
      "    print('i',i,'sentB:',sentB)\n",
      "    print('offsetB:', offsetB, 'nextPoint:', nextPoint, 'sentLength', sentLength)\n",
      "    print('***************')\n",
      "    if i == 2: #This is a cuting point see the explanation below\n",
      "        offsetB = nextPoint\n",
      "        nextPoint = 0\n",
      "        sentLength = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i 0 sentB: The is a 5 acre (20,000\u00a0m ) located in the of , located in .\n",
        "offsetB: 0 nextPoint: 60 sentLength 60\n",
        "***************\n",
        "i 1 sentB: The is a 5 acre (20,000\u00a0m ) located in the of , located in . The zoo is operated by the in partnership with the .\n",
        "offsetB: 0 nextPoint: 113 sentLength 113\n",
        "***************\n",
        "i 2 sentB: The is a 5 acre (20,000\u00a0m ) located in the of , located in . The zoo is operated by the in partnership with the .Queens Zoo zoo New York City borough Queens Flushing Meadows-Corona Park Wildlife Conservation Society New York City Department of Parks and Recreation\n",
        "The Zoo opened on , , with the ceremonial ribbon cut by .\n",
        "offsetB: 0 nextPoint: 322 sentLength 322\n",
        "***************\n",
        "i 3 sentB:  The zoo is home mostly to animals native to North America.\n",
        "offsetB: 322 nextPoint: 381 sentLength 59\n",
        "***************\n",
        "i 4 sentB:  The zoo is home mostly to animals native to North America. The Queens Zoo is the only one of the five zoos in New York City to exhibit .\n",
        "offsetB: 322 nextPoint: 459 sentLength 137\n",
        "***************\n",
        "i 5 sentB:  The zoo is home mostly to animals native to North America. The Queens Zoo is the only one of the five zoos in New York City to exhibit . The zoo was constructed on the site of the , and the zoo's aviary is a , designed by and used during the 1964 Fair.\n",
        "offsetB: 322 nextPoint: 575 sentLength 253\n",
        "***************\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note:** as you can prove later in the alingment algorithm, when sentA match with sentB the value of the offset parameter is equalized to *nextPoint*. Then the algorithm start to look for the next sentB from the final position (*last* **nextPoint**) of previous sentB. In the above example 322 is at the same time the las nextPoint(& len) of $sentB_1$, and the offset of $sentB_2$, this one start with the string: *The zoo is home mostly...*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Jaccard Function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jaccard(text1,text2):\n",
      "    sentA1 = re.sub(r'[!\"#$%&()\\'*+,-/:;<=>?@\\\\^_`{|}~.\\[\\]]',' ', text1)\n",
      "    sentB1 = re.sub(r'[!\"#$%&()\\'*+,-/:;<=>?@\\\\^_`{|}~.\\[\\]]',' ', text2)\n",
      "    setA = set(sentA1.split())\n",
      "    setB = set(sentB1.split())\n",
      "    if len(setA.union(setB)) == 0:\n",
      "        return 0, sentA1, sentB1\n",
      "    else:\n",
      "        return len(setA.intersection(setB))/float(len(setA.union(setB))), sentA1, sentB1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Alignment Pipeline\n",
      "\n",
      "This tutorial reuse a py3 implementation of swalign module. It contains the implementation of Smith-Waterman distance.\n",
      "The piple line show inside the *alignSentences* function use the previous four functions and the swalign object. This module\n",
      "can make attractive visualization of the correspondese between both sentences, valid for educational porpouses.\n",
      "\n",
      "**Nota:** To see more results uncomment the *print* orders."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import swalign\n",
      "import re\n",
      "\n",
      "match = 2\n",
      "mismatch = -1\n",
      "\n",
      "scoring = swalign.NucleotideScoringMatrix(match, mismatch)\n",
      "sw = swalign.LocalAlignment(scoring)\n",
      "\n",
      "def alignSentences(preproc_text, orig_text):\n",
      "    sentenceList=[]\n",
      "    offsetB = 0\n",
      "    \n",
      "    norm_orig_text = normalize(orig_text)\n",
      "    \n",
      "    #if preproc_text.count('.') > norm_orig_text.count('.'):\n",
      "    #    raise Exception(\"Preprocess Error: number of preproc periods most be less or equal than normalize original text periods.\")\n",
      "        \n",
      "    for i, (sentA, offsetA, lengthA) in enumerate(getSentA(preproc_text)):\n",
      "        print(1)\n",
      "        maxScore =-1; score = 0\n",
      "        prevPoint = 0#len(sentA)-2\n",
      "        nextPoint = 0\n",
      "        iqualScore = 0;prevFrag='';jaccard_measure = 0; X = {()}; Y={()}\n",
      "        prev_jaccard_measure = 1.0\n",
      "        \n",
      "        #S\u00ed llegamos a la \u00faltima oraci\u00f3n entonces\n",
      "        if i == preproc_text.count('.')-1: \n",
      "            lengMax = len(norm_orig_text)\n",
      "            tuple = (i, sentA, offsetB, lengMax)\n",
      "            sentenceList.append(tuple)\n",
      "            break\n",
      "        \n",
      "        #S\u00ed no es la \u00faltima oraci\u00f3n compara hasta encontrar el score max.\n",
      "        while(score >= maxScore):\n",
      "            prev_jaccard_measure = jaccard_measure; prev_setA = X; prev_setB = Y\n",
      "            lengMax = nextPoint\n",
      "            maxScore = score\n",
      "            \n",
      "            #Get sentence B and prepare it to calc distances\n",
      "            sentB, nextPoint, prevPoint = getSentB(norm_orig_text, offsetB, nextPoint, prevPoint)\n",
      "            sentB = sentB.replace('\\n',' ') #avoid some bugs on swalign function\n",
      "            \n",
      "            #Calc measures Smith-Watterman and Jaccard\n",
      "            alignment = sw.align(sentA[-round(len(sentA)*0.5):], sentB[-round(len(sentA)*0.5):])\n",
      "            jaccard_measure, X, Y = jaccard ( sentA , sentB) #Second measure only to lookfor errors\n",
      "            score = alignment.score\n",
      "            matches = alignment.matches\n",
      "            \n",
      "            #if i>-1:\n",
      "            #    print ('jaccard_measure:',jaccard_measure)\n",
      "            #    print('-----offsetB',offsetB,'---- from pos', prevPoint, '-----to pos',nextPoint)\n",
      "            #    print('i:',i,'score:',score,'maxScore:',maxScore, 'matches:',matches)\n",
      "                #print('sentB:\\n',sentB)\n",
      "            #    print('frag-sentA:',sentA[-round(len(sentA)*0.5):],'\\nfrag-sentB:',sentB[-round(len(sentA)*0.5):],'\\n')\n",
      "            \n",
      "            #Repeated sentence exception src00014\n",
      "            if prevFrag == sentB[-round(len(sentA)*0.5):]:\n",
      "                #print ('=================Repeated sentence')\n",
      "                break\n",
      "            #keep the previous fragment to know if the next sent is the same as before. \n",
      "            #SmithWaterman move forward to the next sentence.\n",
      "            prevFrag = sentB[-round(len(sentA)*0.5):] \n",
      "            \n",
      "            #Short sentence exceptions\n",
      "            if len(sentA) < 14:\n",
      "                maxScore = score\n",
      "                lengMax = nextPoint\n",
      "                break\n",
      "                \n",
      "            #Infinite loop exception\n",
      "            if score == maxScore:\n",
      "                iqualScore += 1\n",
      "            if iqualScore == 20:\n",
      "                break\n",
      "            \n",
      "        tuple = (i, sentA, offsetB, lengMax)\n",
      "        sentenceList.append(tuple)\n",
      "        \n",
      "        if i > 0:\n",
      "            print ('jaccard_measure:',prev_jaccard_measure)\n",
      "            #print (prev_setA,'<--->',prev_setB)\n",
      "            print('#############RESULTADO de la ORACI\u00d3N :', i)\n",
      "            print('score max:',maxScore, 'offsetB:', offsetB, 'lengthB:',lengMax-offsetB)\n",
      "            print('sentB:',text_orig[offsetB:lengMax])\n",
      "            print('sentA:',sentA)\n",
      "            print('\\n***************')\n",
      "        \n",
      "        offsetB = lengMax\n",
      "\n",
      "    return sentenceList\n",
      "\n",
      "text_orig = open('../susp/suspicious-document00017.txt').read()\n",
      "preproc_text = open('../norm/susp/suspicious-document00017.txt').read()\n",
      "\n",
      "sentenceList = alignSentences(preproc_text,text_orig)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 1\n",
        "score max: 36 offsetB: 37 lengthB: 52\n",
        "sentB: Alama Highway Patrol Door Seal\n",
        "681 (as of 2004) [3]\n",
        "\n",
        "sentA: Alama Highway Patrol Door Seal 681 as of 2004 3 .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 2\n",
        "score max: 19 offsetB: 89 lengthB: 31\n",
        "sentB: Civilians\n",
        "587 (as of 2004) [4]\n",
        "\n",
        "sentA: Civilians 587 as of 2004 4 .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 3\n",
        "score max: 15 offsetB: 120 lengthB: 17\n",
        "sentB: Agency executive\n",
        "\n",
        "sentA: Agency executive .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 4\n",
        "score max: 32 offsetB: 137 lengthB: 36\n",
        "sentB: Major Roscoe Howell, Division Chief\n",
        "\n",
        "sentA: Major Roscoe Howell Division Chief .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 5\n",
        "score max: 13 offsetB: 173 lengthB: 14\n",
        "sentB: Parent agency\n",
        "\n",
        "sentA: Parent agency .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 0\n",
        "#############RESULTADO de la ORACI\u00d3N : 6\n",
        "score max: 9 offsetB: 187 lengthB: 10\n",
        "sentB: Footnotes\n",
        "\n",
        "sentA: Footnotes .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 7\n",
        "score max: 79 offsetB: 197 lengthB: 82\n",
        "sentB: Division of the country, over which the agency has usual operational jurisdiction.\n",
        "sentA: Division of the country over which the agency has usual operational jurisdiction .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 8\n",
        "score max: 17 offsetB: 279 lengthB: 20\n",
        "sentB:  Divisional agency:\n",
        "\n",
        "sentA: Divisional agency .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 9\n",
        "score max: 73 offsetB: 299 lengthB: 81\n",
        "sentB: The is a division of the and is the agency for , which has anywhere in the state.\n",
        "sentA: The is a division of the and is the agency for which has anywhere in the state .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 10\n",
        "score max: 185 offsetB: 380 lengthB: 189\n",
        "sentB:  It was created to protect the lives, property and constitutional rights of people in Alabama.Alabama Highway Patrol Alabama Department of Public Safety highway patrol Alabama jurisdiction\n",
        "\n",
        "sentA: It was created to protect the lives property and constitutional rights of people in Alabama_Alabama Highway Patrol Alabama Department of Public Safety highway patrol Alabama jurisdiction .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 11\n",
        "score max: 149 offsetB: 569 lengthB: 153\n",
        "sentB: In 1971, the became the first U.S. police organization to use down-sized vehicles for regular highway patrol duties when they purchased 132 AMC Javelins.\n",
        "sentA: In 1971 the became the first U_S_ police organization to use down_sized vehicles for regular highway patrol duties when they purchased 132 AMC Javelins .\n",
        "\n",
        "***************\n",
        "1\n",
        "jaccard_measure: 1.0\n",
        "#############RESULTADO de la ORACI\u00d3N : 12\n",
        "score max: 113 offsetB: 722 lengthB: 117\n",
        "sentB:  This pre-dated, among others, the Camaros and Mustangs used by other departments years later.Alabama Highway Patrol\n",
        "\n",
        "sentA: This pre_dated among others the Camaros and Mustangs used by other departments years later_Alabama Highway Patrol .\n",
        "\n",
        "***************\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.22162294387817383"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Testing the Alignment result"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ('oraci\u00f3n preprocesada:', sentenceList[0][1])\n",
      "print ('oraci\u00f3n original:', text_orig[sentenceList[0][2]:sentenceList[0][3]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "oraci\u00f3n preprocesada: The is a 5 acre 20 000 m located in the of located in .\n",
        "oraci\u00f3n original: The is a 5 acre (20,000\u00a0m ) located in the of , located in .\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting the Tagged Aligned Text of Whole Collection\n",
      "\n",
      "Here we have again the *file* **pairs** which contains all normalized cases, inside the normalizated collection (../norm), then we need the route to original files and finally the output directory.\n",
      "\n",
      "Remember\u00edng the [begining definition](#Aligned_Text_Structure) of aligned text structure for future phases.\n",
      "\n",
      "*suspicious-document00007.txt*\n",
      "\n",
      "   $(id_K,normalized-sentence_K,original-offset_{sentence\\,K},original-length_{sentence\\,K})$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Alert!!! -> If you try to run this block based on SmithWaterman\n",
      "%run scripts/02.3_alignNormalizedCaseList_obsolete.py ../norm/pairs2 ../src/ ../susp/ ../align/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "FileNotFoundError",
       "evalue": "[Errno 2] No such file or directory: '../norm/pairs2'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m/media/ALMACEN/Doctorado/00_plag_algh/doc/scripts/02.3_alignNormalizedCaseList.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0moutdir\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mcases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0malignedDocList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../norm/pairs2'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Conclusions\n",
      "\n",
      "The objective of this process was accomplished. The obtained transformed text contains the proposed structure.\n",
      "But some performance problems can appear. My laptop crash two times trying to process the whole list of 1400 cases.\n",
      "One exceptional document could be the src/source-document00281.txt. These one contains a long fragment of words at the \n",
      "beginning without any dot punctuation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Excersices\n",
      "\n",
      "1. The comparison between alignment text in 2.2_notebook and the algh presented here can show better performance. Implement a different alignment pipeline using another faster technique and less source code.\n",
      "\n",
      "2. Make a list of different punctuation situations that can appear in real text provided by pdftotext conversors and program the regular expresion for both sides the normalization process and for normalize function.\n",
      "\n",
      "3. Create a different idea to align original and preprocessed sentences. Sugestion, take a look to cross-lingual alignment implemented in nltk.\n",
      "\n",
      "4. The next code can be added to the alignment pipeline to improve the alignment of long sentences. Evaluate it and propose the best k to maintain the precision.\n",
      "\n",
      "        #Optimization for very long sentences alignment\n",
      "        if len(sentA) > 500:\n",
      "            k = 0.1\n",
      "        else: k=0.5\n",
      "        \n",
      "5. The pairs list provided for this tutorial contains 1400 cases in which the aligment based in Smith-Waterman and Jaccard makes a perfect score. If you test this algorithm in PAN-PC pairs file the result will be very different, some cases will be bad aligned. Make the necessary changes to the algorithm to write in the preprocessDocList file a \"False\" value if the text is bad aligned."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}