{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Best Normalization after Alignment\n",
    "\n",
    "The objetive of this notebook is to show you across a measure if we had a good *Normalization* process based on Plagiarism Detection Corpus tags values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Controling Fixed Sentence Boundaries\n",
    "\n",
    "For plagiarism detection or text reuse analysis you will must select from two whole text document the similar fragments. However the techniques of text reuse some times does not need a sentence limit to make a similarity calculation, they can use tokens or even chars to delimit the boundaries of compared fragments. Another reality is that in artificially generated cases the algorithm can capture inside the fragment just a chunk of the beginning or ending sentence.\n",
    "\n",
    "But in real scenarios humans take fragments compose by complete ideas (sentences) and then, modified or not, they use it in a new document.\n",
    "\n",
    "As it was explained [before](02.1-Normalizing-Text-Corpus.ipynb) the normalization process intend to normalize tokens and delete end of sentece ambiguities or to define very well the end of sentence dots. **Alert:** The sentence tokenization is an open problem in NLP, this is an indirect way to show the quality of normalization made by the author.\n",
    "\n",
    "Then for future text reuse experiments it will be measure the cuality of sentence division based on its pertinence to the annotated fragment into the plag case XML doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Formalization of Normalized Sentences % in a Case\n",
    "(based on Case XML Information)\n",
    "\n",
    "After [Alignment Process](02.2c-Jaccard-Align-Preproc-to-Original-Sent.ipynb) a <font color='#F84825'>new text structure</font> was obtained. Open the file [suspicious-document00007.txt](files/data/aligned/susp/suspicious-document00007.txt) to see that previous-process result structure in the form <font color='#F84825'>\n",
    "   $(id_K,normalized-sentence_K,original\\,offset_{sentence\\,K},original\\,offset+length_{sentence\\,K})$\n",
    "</font>        \n",
    "\n",
    "The fragment attributes are showed below for the same document in the xml file [suspicious-document00007-source-document00382.xml](files/data/PAN-PC-2013/orig/03-random-obfuscation/suspicious-document00007-source-document00382.xml)\n",
    "        \n",
    "<body>\n",
    "<pre style='color:#1f1c1b;background-color:#ffffff;'>\n",
    "<b>&lt;document</b><span style='color:#006e28;'> reference=</span><span style='color:#aa0000;'>&quot;suspicious-document00007.txt&quot;</span><b>&gt;</b>\n",
    "<b>&lt;feature</b><span style='color:#006e28;'> name=</span><span style='color:#aa0000;'>&quot;plagiarism&quot;</span><span style='color:#006e28;'> obfuscation=</span><span style='color:#aa0000;'>&quot;random&quot;</span><span style='color:#006e28;'> obfuscation_degree=</span><span style='color:#aa0000;'>&quot;0.4694788492120119&quot;</span><span style='color:#006e28;'> source_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> source_offset=</span><span style='color:#aa0000;'>&quot;0&quot;</span><span style='color:#006e28;'> source_reference=</span><span style='color:#aa0000;'>&quot;source-document00382.txt&quot;</span><span style='color:#006e28;'> this_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> this_offset=</span><span style='color:#aa0000;'>&quot;9449&quot;</span><span style='color:#006e28;'> type=</span><span style='color:#aa0000;'>&quot;artificial&quot;</span> <b>/&gt;</b>\n",
    "<b>&lt;/document</b><b>&gt;</b>\n",
    "</pre>\n",
    "</body>\n",
    "        \n",
    "Having this attributes values in advance. How do we calculate the percent of a sentence inside the fragment?\n",
    "\n",
    "Making a mathematical reasoning it can be found that in the three cases indicated in figure 1 the general formula to solve this problem will be:\n",
    "\n",
    "$$\\%\\,of\\,sentence_{K}\\,that\\,belong\\,to\\,the\\,fragment_X = \\frac{min(L_K,L_X) - max(Offset_K,Offset_X)}{L_K-Offset_K}$$\n",
    "\n",
    "Where $L_K = Offset_K + Length_K$, that means the position of the last character that belongs to te $sentence_K$.\n",
    "\n",
    "<center><strong>Elaborated diagram to show possible situations after sentence normalization</strong></center></br>\n",
    "<table border=0 cellspacing=10> \n",
    "    <caption align=\"bottom\"> </br><em>Figura 2.4.1: The three possible sentence pertinence to the fragment</em>\n",
    "    </caption> \n",
    "<tr align=\"center\">\n",
    "    <th> <img src=\"imgs/PercentSentBelongFragment.jpg\" height=200px width=300px alt=\"*\" \n",
    "        align=\"center\"> </th>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "To avoid the problem that one single documents can appear in more than one pair we need to keep the $\\%\\,of\\,sentence_{K}\\,that\\,belong\\,to\\,the\\,fragment_X$ related to the sentence. Creating one new text per case based on a new structure (all values are numerical, ideal to load it in a numpy array):\n",
    "\n",
    "    /norm/quality/suspicious-document00007-source-document00382.xml\n",
    "   $(id_{sentence_P\\,susp},offset_{sentence_P},offset+length_{sentence_P},\\%\\,sentence_{P}\\, \\in\\,susp_{fragment\\,X},id_{fragment\\,X})$\n",
    "   \n",
    "   $\\hspace{2cm}\\vdots$ \n",
    "   \n",
    "   $(id_{sentence_Q\\,src},offset_{sentence_Q},offset+length_{sentence_Q},\\%\\,sentence_{Q}\\, \\in\\,src_{fragment\\,X},id_{fragment\\,X})$\n",
    "   \n",
    "   $\\hspace{2cm}\\vdots$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "TODO: aquí va el gráfico del algoritmo y la explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts import PANXml\n",
    "\n",
    "def calcPercent(file, fragmentOffset, fragmentLen, fragmentID, thresh):  \n",
    "    percent = [];case_percent = []\n",
    "    with open(file) as doc:\n",
    "        for num,line in enumerate(doc):\n",
    "            ID,sent,offsetk,lenk = line.split('\\t')\n",
    "            Offsetk = int(offsetk)\n",
    "            Lenk = int(lenk)\n",
    "            if Offsetk < fragmentLen and Lenk > fragmentOff:\n",
    "                \n",
    "                perc = (min(Lenk,fragmentLen)-max(Offsetk,fragmentOffset))/float(Lenk-Offsetk)\n",
    "                \n",
    "                if perc > thresh:\n",
    "                    percent.append(ID+'\\t'\n",
    "                                   +str(Offsetk)+'\\t'+lenk[:-1]+'\\t'\n",
    "                                   +str(perc)+'\\t'\n",
    "                                   +str(fragmentID+1)\n",
    "                                   +file[-9:-4])\n",
    "                    \n",
    "                    case_percent.append(perc)\n",
    "    \n",
    "    return percent, case_percent\n",
    "\n",
    "def calc_sentPercentCase(inputfileName,alignedCollectionPath, xmlColecctionPath,threshold=0.3,printout=False):\n",
    "    susp, src = inputfileName.split()\n",
    "    case = PANXml(xmlColecctionPath+susp[:-4]+'-'+src[:-4]+'.xml')\n",
    "    result = []; case_perc = []\n",
    "    \n",
    "    for i,file in enumerate([alignedCollectionPath+'susp/'+susp,alignedCollectionPath+'src/'+src]):\n",
    "        for fragmentID,fragment in enumerate(case.fragmentList):\n",
    "            #The non-plagiarism cases have an empty XML, the next line is to avoid thems\n",
    "            if len(case.fragmentList) > 0:\n",
    "                if i == 0:\n",
    "                    Ox = int(fragment.suspOffset)\n",
    "                    Lx = Ox+int(fragment.suspLength)\n",
    "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
    "                    result.extend(a);case_perc.extend(b)\n",
    "                    \n",
    "                else:\n",
    "                    Ox = int(fragment.srcOffset)\n",
    "                    Lx = Ox+int(fragment.srcLength)\n",
    "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
    "                    result.extend(a);case_perc.extend(b)\n",
    "                if printout:\n",
    "                    print('Case %d, Offset=%d, Length=%d' % (fragmentID, Ox, Lx-Ox))\n",
    "\n",
    "    #Write in a single doc the percent result line by line for both docs of the case\n",
    "    report = open(alignedCollectionPath+'quality/'+inputfileName, 'w')\n",
    "    if printout:\n",
    "        print('Sents\\tOffset\\tLength\\t%InFrag\\tFragID')\n",
    "    for line in result:\n",
    "        if printout:\n",
    "            print(line)\n",
    "        report.write(line+'\\n')\n",
    "    report.close()\n",
    "    \n",
    "    data = np.array(case_perc,dtype=np.float64)\n",
    "    #print('data:',case_perc)\n",
    "    return float(data.mean()), case_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for a case with only one pair of fragments\n",
    "\n",
    "Please create \"quality\" folder inside aligned folder before to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0, Offset=0, Length=556\n",
      "Case 0, Offset=0, Length=999\n",
      "Sents\tOffset\tLength\t%InFrag\tFragID\n",
      "0\t0\t514\t1.0\t100122\n",
      "1\t514\t556\t1.0\t100122\n",
      "0\t0\t40\t1.0\t101169\n",
      "1\t40\t70\t1.0\t101169\n",
      "2\t70\t1002\t0.9967811158798283\t101169\n",
      "Case Percent Quality = 0.9993562231759657\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'suspicious-document00122.txt source-document01169.txt'\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
    "print(\"Case Percent Quality =\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for a case with more than one pair of fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0, Offset=3505, Length=184\n",
      "Case 1, Offset=4727, Length=311\n",
      "Case 0, Offset=211, Length=387\n",
      "Case 1, Offset=1021, Length=422\n",
      "Sents\tOffset\tLength\t%InFrag\tFragID\n",
      "30\t3473\t3562\t0.6404494382022472\t100070\n",
      "31\t3562\t3593\t1.0\t100070\n",
      "32\t3593\t3620\t1.0\t100070\n",
      "33\t3620\t3655\t1.0\t100070\n",
      "34\t3655\t3755\t0.34\t100070\n",
      "42\t4684\t4777\t0.5376344086021505\t200070\n",
      "43\t4777\t4904\t1.0\t200070\n",
      "44\t4904\t4992\t1.0\t200070\n",
      "4\t211\t346\t1.0\t101504\n",
      "5\t346\t418\t1.0\t101504\n",
      "6\t418\t497\t1.0\t101504\n",
      "7\t497\t596\t1.0\t101504\n",
      "13\t1019\t1096\t0.974025974025974\t201504\n",
      "14\t1096\t1168\t1.0\t201504\n",
      "15\t1168\t1182\t1.0\t201504\n",
      "16\t1182\t1275\t1.0\t201504\n",
      "17\t1275\t1443\t1.0\t201504\n",
      "Case Percent Quality = 0.9113005776959042\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'suspicious-document00070.txt source-document01504.txt'\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
    "print(\"Case Percent Quality =\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for the whole Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/test1/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6947212219238281\n"
     ]
    }
   ],
   "source": [
    "#Generate the percent for all sentences in every preprocessed case\n",
    "#Load all cases in pairs\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "\n",
    "import time\n",
    "\n",
    "norm_percent = []\n",
    "sent_perc = []\n",
    "threshold=0.3\n",
    "\n",
    "init = time.time()\n",
    "for line in open('data/aligned/align_pairs'):\n",
    "    perct, sent = calc_sentPercentCase(line, alignedCollectionPath, \n",
    "                                       xmlColecctionPath,threshold)\n",
    "    norm_percent.append(perct)\n",
    "    sent_perc.extend(sent)\n",
    "    timef = time.time() - init\n",
    "print(timef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "[       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.9596732  0.86314415\n",
      " 0.89054233 0.94825782 0.83321233 0.95089359 0.95363974 0.83758948\n",
      " 0.81468753 0.8639324  0.89908607]\n",
      "Total useful sentences: 13354\n"
     ]
    }
   ],
   "source": [
    "len(norm_percent)\n",
    "quality_norm_vector = np.array(norm_percent,dtype=np.float64)\n",
    "print(quality_norm_vector.shape)\n",
    "print(quality_norm_vector[990:1011])\n",
    "print('Total useful sentences:',len(sent_perc))\n",
    "quality_sent_norm = np.array(sent_perc,dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9252346674340438"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_norm_vector[1000:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Normalization Quality Measure\n",
    "\n",
    "$Quality = \\frac{\\sum_{k=0}^n (\\%\\,of\\,sentence_k\\,\\,inside\\,fragment>\\mu)}{total\\,sentences\\,with\\,\\% > \\mu}$ \n",
    "\n",
    "Where $n+1$ is the total sentences of the analised fragment and $\\mu$ is the minimum percent of a sentence inside the fragment to consider it that belong to the fragment case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Type: <class 'numpy.float64'> \n",
      "Quality mean: nan .\n",
      "New data type is needed: DataFrame (DF).\n",
      "DF Quality based on cases-quality-average: 0.9252\n",
      "Total sentences: 13354\n",
      "DF Quality based on total sentences with percent > 0.30 inside de case: 0.9388\n",
      "# of normalized sentences with +90.00% of its size inside its respectively case-frag: 11245\n",
      "# de oraciones norm con +50.00% de su tamaño fuera del caso: 584\n",
      "Quality based on miu: 0.8421\n",
      "Quality based on miu2: 0.9563\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "miu = 0.9\n",
    "miu_vector = []\n",
    "miu2 = 0.5\n",
    "miu_vector2 = []\n",
    "\n",
    "Quality = quality_norm_vector.mean()\n",
    "#this is for show that np-array do not handle NaN values, \n",
    "#then DataFrame object convertion is needed\n",
    "print ('Quality Type:',type(Quality),'\\nQuality mean:',Quality,\n",
    "       '.\\nNew data type is needed: DataFrame (DF).') \n",
    "\n",
    "P = DataFrame(quality_norm_vector, columns=['percent'])\n",
    "Qn = P.mean()\n",
    "print('DF Quality based on cases-quality-average: %.4f' % (Qn['percent']))\n",
    "\n",
    "print('Total sentences:',len(sent_perc))\n",
    "Q = DataFrame(quality_sent_norm, columns=['percent'])\n",
    "Qs = Q.mean()\n",
    "print('DF Quality based on total sentences with percent > %.2f inside de case: %.4f' % (threshold,Qs['percent']))\n",
    "\n",
    "for sent in quality_sent_norm:\n",
    "    if sent > miu:\n",
    "        miu_vector.append(sent)\n",
    "\n",
    "for sent in quality_sent_norm:\n",
    "    if sent < miu2:\n",
    "        miu_vector2.append(sent)\n",
    "        \n",
    "print('# of normalized sentences with +%.2f%% of its size' % (miu*100),\n",
    "      'inside its respectively case-frag: %d' % (len(miu_vector)))\n",
    "print('# de oraciones norm con +%.2f%% de su tamaño fuera del caso: %d' % \n",
    "      (100-miu2*100,len(miu_vector2)))\n",
    "print('Quality based on miu: %.4f' % (len(miu_vector)/len(Q)))\n",
    "print('Quality based on miu2: %.4f' % (1.0-len(miu_vector2)/len(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percent\n",
       "0  0.529412\n",
       "1  1.000000\n",
       "2  1.000000\n",
       "3  0.986667\n",
       "4  1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Quality factor of this new data is good. Means that the normalization process redefine sentences with more of the half of its length in a 95%. 84% has more than 90% of the sentences inside the fragment defined by xml. Only 7% has less than 30% outside the fragment defined by corpus's xmls files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Develop new quality measures based on learned here.\n",
    "\n",
    "2. Describe formulas and algorithms in Excersises.notebook.\n",
    "\n",
    "3. Change the distance measure, and a different version of _preprocess_ lib and interprete the result. Test the contractions and abbreviations replacement and analyse the result.\n",
    "\n",
    "4. Implement the normalization with nltk, pattern or spacy, and analyse the results. Comment the difference for future experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
