{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Best Normalization after Alignment\n",
    "\n",
    "The objetive of this notebook is to show you across a measure if we had a good *Normalization* process based on Plagiarism Detection Corpus tags values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Controling Fixed Sentence Boundaries\n",
    "\n",
    "For plagiarism detection or text reuse analysis you will must select from two whole text document the similar fragments. However the techniques of text reuse some times does not need a sentence limit to make a similarity calculation, they can use tokens or even chars to delimit the boundaries of compared fragments. Another reality is that in artificially generated cases the algorithm can capture inside the fragment just a chunk of the beginning or ending sentence.\n",
    "\n",
    "But in real scenarios humans take fragments compose by complete ideas (sentences) and then, modified or not, they use it in a new document.\n",
    "\n",
    "As it was explained [before](02.1-Normalizing-Text-Corpus.ipynb) the normalization process intend to normalize tokens and delete end of sentece ambiguities or to define very well the end of sentence dots. **Alert:** The sentence tokenization is an open problem in NLP, this is an indirect way to show the quality of normalization made by the author.\n",
    "\n",
    "Then for future text reuse experiments it will be measure the cuality of sentence division based on its pertinence to the annotated fragment into the plag case XML doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Formalization of Normalized Sentences % in a Case\n",
    "(based on Case XML Information)\n",
    "\n",
    "After [Alignment Process](02.2c-Jaccard-Align-Preproc-to-Original-Sent.ipynb) a <font color='#F84825'>new text structure</font> was obtained. Open the file [suspicious-document00007.txt](files/data/aligned/susp/suspicious-document00007.txt) to see that previous-process result structure in the form <font color='#F84825'>\n",
    "   $(id_K,normalized-sentence_K,original\\,offset_{sentence\\,K},original\\,offset+length_{sentence\\,K})$\n",
    "</font>        \n",
    "\n",
    "The fragment attributes are showed below for the same document in the xml file [suspicious-document00007-source-document00382.xml](files/data/PAN-PC-2013/orig/03-random-obfuscation/suspicious-document00007-source-document00382.xml)\n",
    "        \n",
    "<body>\n",
    "<pre style='color:#1f1c1b;background-color:#ffffff;'>\n",
    "<b>&lt;document</b><span style='color:#006e28;'> reference=</span><span style='color:#aa0000;'>&quot;suspicious-document00007.txt&quot;</span><b>&gt;</b>\n",
    "<b>&lt;feature</b><span style='color:#006e28;'> name=</span><span style='color:#aa0000;'>&quot;plagiarism&quot;</span><span style='color:#006e28;'> obfuscation=</span><span style='color:#aa0000;'>&quot;random&quot;</span><span style='color:#006e28;'> obfuscation_degree=</span><span style='color:#aa0000;'>&quot;0.4694788492120119&quot;</span><span style='color:#006e28;'> source_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> source_offset=</span><span style='color:#aa0000;'>&quot;0&quot;</span><span style='color:#006e28;'> source_reference=</span><span style='color:#aa0000;'>&quot;source-document00382.txt&quot;</span><span style='color:#006e28;'> this_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> this_offset=</span><span style='color:#aa0000;'>&quot;9449&quot;</span><span style='color:#006e28;'> type=</span><span style='color:#aa0000;'>&quot;artificial&quot;</span> <b>/&gt;</b>\n",
    "<b>&lt;/document</b><b>&gt;</b>\n",
    "</pre>\n",
    "</body>\n",
    "        \n",
    "Having this attributes values in advance. How do we calculate the percent of a sentence inside the fragment?\n",
    "\n",
    "Making a mathematical reasoning it can be found that in the three cases indicated in figure 1 the general formula to solve this problem will be:\n",
    "\n",
    "$$\\%\\,of\\,sentence_{K}\\,that\\,belong\\,to\\,the\\,fragment_X = \\frac{min(L_K,L_X) - max(Offset_K,Offset_X)}{L_K-Offset_K}$$\n",
    "\n",
    "Where $L_K = Offset_K + Length_K$, that means the position of the last character that belongs to te $sentence_K$.\n",
    "\n",
    "<center><strong>Elaborated diagram to show possible situations after sentence normalization</strong></center></br>\n",
    "<table border=0 cellspacing=10> \n",
    "    <caption align=\"bottom\"> </br><em>Figura 2.4.1: The three possible sentence pertinence to the fragment</em>\n",
    "    </caption> \n",
    "<tr align=\"center\">\n",
    "    <th> <img src=\"imgs/PercentSentBelongFragment.jpg\" height=200px width=300px alt=\"*\" \n",
    "        align=\"center\"> </th>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "To avoid the problem that one single documents can appear in more than one pair we need to keep the $\\%\\,of\\,sentence_{K}\\,that\\,belong\\,to\\,the\\,fragment_X$ related to the sentence. Creating one new text per case based on a new structure (all values are numerical, ideal to load it in a numpy array):\n",
    "\n",
    "    /norm/quality/suspicious-document00007-source-document00382.xml\n",
    "   $(id_{sentence_P\\,susp},offset_{sentence_P},offset+length_{sentence_P},\\%\\,sentence_{P}\\, \\in\\,susp_{fragment\\,X},id_{fragment\\,X})$\n",
    "   \n",
    "   $\\hspace{2cm}\\vdots$ \n",
    "   \n",
    "   $(id_{sentence_Q\\,src},offset_{sentence_Q},offset+length_{sentence_Q},\\%\\,sentence_{Q}\\, \\in\\,src_{fragment\\,X},id_{fragment\\,X})$\n",
    "   \n",
    "   $\\hspace{2cm}\\vdots$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "TODO: aquí va el gráfico del algoritmo y la explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts import PANXml_Reader\n",
    "\n",
    "def calcPercent(file, fragmentOffset, fragmentLen, fragmentID, thresh):  \n",
    "    percent = [];case_percent = []\n",
    "    with open(file) as doc:\n",
    "        for num,line in enumerate(doc):\n",
    "            ID,sent,sentOffset,sentLen = line.split('\\t')\n",
    "            sentOffset = int(sentOffset)\n",
    "            sentLen = int(sentLen) + sentOffset\n",
    "            if sentOffset < fragmentLen and sentLen > fragmentOffset:\n",
    "                \n",
    "                perc = (min(sentLen,fragmentLen)-max(sentOffset,fragmentOffset))/float(sentLen-sentOffset)\n",
    "                \n",
    "                #if perc >= 0:\n",
    "                percent.append(ID+'\\t'\n",
    "                               +str(sentOffset)+'\\t'+str(sentLen)+'\\t'\n",
    "                               +str(perc)+'\\t'\n",
    "                               +str(fragmentID+1)+file[-9:-4])\n",
    "\n",
    "                case_percent.append(perc)\n",
    "    \n",
    "    return percent, case_percent\n",
    "\n",
    "def calc_sentPercentCase(inputfileName,alignedCollectionPath, \n",
    "                         xmlColecctionPath,\n",
    "                         threshold=0.3,\n",
    "                         printout=False):\n",
    "    \n",
    "    susp, src = inputfileName.split()\n",
    "    xmlDoc = PANXml_Reader(xmlColecctionPath+susp[:-4]+'-'+src[:-4]+'.xml')\n",
    "    fragmentList = xmlDoc.parser()\n",
    "    result = []; case_perc = []\n",
    "    \n",
    "    #The non-plagiarism cases have an empty XML, the next line is to avoid thems\n",
    "    if len(fragmentList) > 0:\n",
    "        files = [alignedCollectionPath+'susp/'+susp,alignedCollectionPath+'src/'+src]\n",
    "        for i,file in enumerate(files):\n",
    "            for fragmentID,fragment in enumerate(fragmentList):\n",
    "                if i == 0: #if the case is the susp case\n",
    "                    Ox = int(fragment.suspOffset)\n",
    "                    Lx = Ox+int(fragment.suspLength)\n",
    "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
    "                    result.extend(a);case_perc.extend(b)\n",
    "                    \n",
    "                else: #if the case is the src case\n",
    "                    Ox = int(fragment.srcOffset)\n",
    "                    Lx = Ox+int(fragment.srcLength)\n",
    "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
    "                    result.extend(a);case_perc.extend(b)\n",
    "                if printout:\n",
    "                    print('Case %d, Offset=%d, Length=%d' % (fragmentID, Ox, Lx-Ox))\n",
    "\n",
    "        #Write in a single doc the percent result line by line for both docs of the case\n",
    "        report = open(alignedCollectionPath+'quality/'+inputfileName, 'w')\n",
    "        if printout:\n",
    "            print('Sents\\tOffset\\tLength\\t%InFrag\\tFragID')\n",
    "        for line in result:\n",
    "            if printout:\n",
    "                print(line)\n",
    "            report.write(line+'\\n')\n",
    "        report.close()\n",
    "\n",
    "        data = np.array(case_perc,dtype=np.float64)\n",
    "\n",
    "        return float(data.mean()), case_perc\n",
    "    \n",
    "    else:\n",
    "        return 0.0, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for a case with only one pair of fragments\n",
    "\n",
    "Please create \"quality\" folder inside aligned folder before to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0, Offset=1021, Length=502\n",
      "Case 0, Offset=0, Length=687\n",
      "Sents\tOffset\tLength\t%InFrag\tFragID\n",
      "12\t972\t1064\t0.4673913043478261\t100427\n",
      "13\t1064\t1188\t1.0\t100427\n",
      "14\t1188\t1413\t1.0\t100427\n",
      "15\t1413\t1503\t1.0\t100427\n",
      "16\t1503\t1526\t0.8695652173913043\t100427\n",
      "0\t0\t65\t1.0\t101618\n",
      "1\t65\t106\t1.0\t101618\n",
      "2\t106\t137\t1.0\t101618\n",
      "3\t137\t267\t1.0\t101618\n",
      "4\t267\t420\t1.0\t101618\n",
      "5\t420\t584\t1.0\t101618\n",
      "6\t584\t663\t1.0\t101618\n",
      "7\t663\t687\t1.0\t101618\n",
      "Case Percent Quality = 0.9489966555183946\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'suspicious-document00427.txt source-document01618.txt'\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
    "print(\"Case Percent Quality =\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for a case with more than one pair of fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0, Offset=5824, Length=272\n",
      "Case 1, Offset=21870, Length=279\n",
      "Case 0, Offset=0, Length=801\n",
      "Case 1, Offset=879, Length=358\n",
      "Sents\tOffset\tLength\t%InFrag\tFragID\n",
      "51\t5824\t5896\t1.0\t101064\n",
      "52\t5896\t5935\t1.0\t101064\n",
      "53\t5935\t5998\t1.0\t101064\n",
      "54\t5998\t6066\t1.0\t101064\n",
      "55\t6066\t6097\t0.967741935483871\t101064\n",
      "240\t21747\t21966\t0.4383561643835616\t201064\n",
      "241\t21966\t22118\t1.0\t201064\n",
      "242\t22118\t22150\t0.96875\t201064\n",
      "0\t0\t49\t1.0\t100671\n",
      "1\t49\t210\t1.0\t100671\n",
      "2\t210\t470\t1.0\t100671\n",
      "3\t470\t602\t1.0\t100671\n",
      "4\t602\t710\t1.0\t100671\n",
      "5\t710\t802\t0.9891304347826086\t100671\n",
      "7\t879\t1127\t1.0\t200671\n",
      "8\t1127\t1141\t1.0\t200671\n",
      "9\t1141\t1190\t1.0\t200671\n",
      "10\t1190\t1239\t0.9591836734693877\t200671\n",
      "Case Percent Quality = 0.9623979004510793\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'suspicious-document01064.txt source-document00671.txt'\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
    "print(\"Case Percent Quality =\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for the whole Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5250451564788818\n"
     ]
    }
   ],
   "source": [
    "#Generate the percent for all sentences in every preprocessed case\n",
    "#Load all cases in pairs\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "\n",
    "import time\n",
    "\n",
    "norm_percent = []\n",
    "sent_perc = []\n",
    "threshold=0.3\n",
    "\n",
    "init = time.time()\n",
    "for line in open('data/aligned/aligned_pairs'):\n",
    "    perct, sent = calc_sentPercentCase(line[:-1], alignedCollectionPath, \n",
    "                                       xmlColecctionPath,threshold)\n",
    "    norm_percent.append(perct)\n",
    "    sent_perc.extend(sent)\n",
    "    timef = time.time() - init\n",
    "print(timef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.93982923 0.99764652 0.7652065  0.96741266 0.89646236\n",
      " 0.89776727 0.93668272 0.94802847 0.99578731 0.99778318 0.99827709\n",
      " 0.76641118 0.96892604 0.99805388]\n",
      "[0.         0.         0.         ... 0.98975591 0.88768371 0.99013629]\n",
      "Total useful sentences: 13376\n"
     ]
    }
   ],
   "source": [
    "len(norm_percent)\n",
    "quality_norm_vector = np.array(norm_percent,dtype=np.float64)\n",
    "print(quality_norm_vector[990:1011])\n",
    "print(quality_norm_vector)\n",
    "print('Total useful sentences:',len(sent_perc))\n",
    "quality_sent_norm = np.array(sent_perc,dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673992915065034"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_norm_vector[1000:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Normalization Quality Measure\n",
    "\n",
    "$Quality = \\frac{\\sum_{k=0}^n (\\%\\,of\\,sentence_k\\,\\,inside\\,fragment>\\mu)}{total\\,sentences\\,with\\,\\% > \\mu}$ \n",
    "\n",
    "Where $n+1$ is the total sentences of the analised fragment and $\\mu$ is the minimum percent of a sentence inside the fragment to consider it that belong to the fragment case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Type: <class 'numpy.float64'> \n",
      "Quality mean: 0.4840843570093619 .\n",
      "New data type is needed: DataFrame (DF).\n",
      "DF Quality based on cases-quality-average: 0.4841\n",
      "Total sentences: 13376\n",
      "DF Quality based on total sentences with percent > 0.30 inside de case: 0.9734\n",
      "# of normalized sentences with +90.00% of its size inside its respectively case-frag: 12794\n",
      "# de oraciones norm con +50.00% de su tamaño fuera del caso: 318\n",
      "Quality based on miu: 0.9565\n",
      "Quality based on miu2: 0.9762\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "miu = 0.9\n",
    "miu_vector = []\n",
    "miu2 = 0.5\n",
    "miu_vector2 = []\n",
    "\n",
    "Quality = quality_norm_vector.mean()\n",
    "#this is for show that np-array do not handle NaN values, \n",
    "#then DataFrame object convertion is needed\n",
    "print ('Quality Type:',type(Quality),'\\nQuality mean:',Quality,\n",
    "       '.\\nNew data type is needed: DataFrame (DF).') \n",
    "\n",
    "P = DataFrame(quality_norm_vector, columns=['percent'])\n",
    "Qn = P.mean()\n",
    "print('DF Quality based on cases-quality-average: %.4f' % (Qn['percent']))\n",
    "\n",
    "print('Total sentences:',len(sent_perc))\n",
    "Q = DataFrame(quality_sent_norm, columns=['percent'])\n",
    "Qs = Q.mean()\n",
    "print('DF Quality based on total sentences with percent > %.2f inside de case: %.4f' % (threshold,Qs['percent']))\n",
    "\n",
    "for sent in quality_sent_norm:\n",
    "    if sent > miu:\n",
    "        miu_vector.append(sent)\n",
    "\n",
    "for sent in quality_sent_norm:\n",
    "    if sent < miu2:\n",
    "        miu_vector2.append(sent)\n",
    "        \n",
    "print('# of normalized sentences with +%.2f%% of its size' % (miu*100),\n",
    "      'inside its respectively case-frag: %d' % (len(miu_vector)))\n",
    "print('# de oraciones norm con +%.2f%% de su tamaño fuera del caso: %d' % \n",
    "      (100-miu2*100,len(miu_vector2)))\n",
    "print('Quality based on miu: %.4f' % (len(miu_vector)/len(Q)))\n",
    "print('Quality based on miu2: %.4f' % (1.0-len(miu_vector2)/len(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.243421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percent\n",
       "0  1.000000\n",
       "1  1.000000\n",
       "2  1.000000\n",
       "3  1.000000\n",
       "4  0.243421"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Data obtained with aligner1.0 is good. Quality factor means that the normalization process redefine the 95% of the sentences with more of the half of its length inside the fragment. 84% has more than 90% of the sentences inside the fragment defined by xml. Only 7% has less than 30% outside the fragment defined by corpus's xmls files.\n",
    "\n",
    "With aligner2.0 all the numbers are excellent. 95% of the sentences has 90% of its length inside the fragment. Only 2.6% has less than 30% out of the fragment. The implementation of the second version of jaccard and aligner based on chars and words comparations was good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Develop new quality measures based on learned here.\n",
    "\n",
    "2. Describe formulas and algorithms in Excersises.notebook.\n",
    "\n",
    "3. Change the distance measure, and a different version of _preprocess_ lib and interprete the result. Test the contractions and abbreviations replacement and analyse the result.\n",
    "\n",
    "4. Implement the normalization with nltk, pattern or spacy, and analyse the results. Comment the difference for future experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
