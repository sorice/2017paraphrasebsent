{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for SciPy LA Habana 2017. Source and license info is on [github repository](http://github.com/sorice/simtext_scipyla2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Best Normalization after Alignment\n",
    "\n",
    "The objetive of this notebook is to evaluate the quality of the normalization-alignment process using the fragment boundaries defined by the Plagiarism Detection Corpus tags values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Controlling Fixed Sentence Boundaries\n",
    "\n",
    "For plagiarism detection or text reuse analysis, you must select the similar fragments from two documents. However, the techniques of text reuse sometimes do not need a sentence limit to make a similarity calculation, they can use tokens or even chars to delimit the boundaries of compared fragments. Another reality is that in artificially generated cases the algorithm can capture inside the fragment just a chunk of the beginning or ending sentence.\n",
    "\n",
    "But in real scenarios humans take fragments composed by complete ideas (sentences) and then, modified or not, they use them in a new document.\n",
    "\n",
    "As it was explained [before](02.1-Normalizing-Text-Corpus.ipynb), the normalization process intends to normalize tokens and delete end of sentence ambiguities or to define very well the end of sentence dots. **Alert:** The sentence tokenization is an open problem in NLP, this is an indirect way to show the quality of normalization made by the author.\n",
    "\n",
    "Then, for future text reuse experiments it will be measured the quality of sentence division based on its pertinence to the annotated fragment into the plag case XML doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Formalization of Normalized Sentences % in a Case\n",
    "(based on Case XML Information)\n",
    "\n",
    "After [Alignment Process](02.2-Jaccard-Align-Preproc-to-Original-Sent.ipynb) a <font color='#F84825'>new text structure</font> was obtained. Open the file [suspicious-document00007.txt](files/data/aligned/susp/suspicious-document00007.txt) to see the resulting text structure of the previous process in the form <font color='#F84825'>\n",
    "   $(id_K,normalized-sentence_K,original\\,offset_{sentence\\,K},original\\,offset+length_{sentence\\,K})$\n",
    "</font>        \n",
    "\n",
    "The fragment attributes are shown below for the same document in the xml file [suspicious-document00007-source-document00382.xml](files/data/PAN-PC-2013/orig/03-random-obfuscation/suspicious-document00007-source-document00382.xml)\n",
    "        \n",
    "<body>\n",
    "<pre style='color:#1f1c1b;background-color:#ffffff;'>\n",
    "<b>&lt;document</b><span style='color:#006e28;'> reference=</span><span style='color:#aa0000;'>&quot;suspicious-document00007.txt&quot;</span><b>&gt;</b>\n",
    "<b>&lt;feature</b><span style='color:#006e28;'> name=</span><span style='color:#aa0000;'>&quot;plagiarism&quot;</span><span style='color:#006e28;'> obfuscation=</span><span style='color:#aa0000;'>&quot;random&quot;</span><span style='color:#006e28;'> obfuscation_degree=</span><span style='color:#aa0000;'>&quot;0.4694788492120119&quot;</span><span style='color:#006e28;'> source_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> source_offset=</span><span style='color:#aa0000;'>&quot;0&quot;</span><span style='color:#006e28;'> source_reference=</span><span style='color:#aa0000;'>&quot;source-document00382.txt&quot;</span><span style='color:#006e28;'> this_length=</span><span style='color:#aa0000;'>&quot;453&quot;</span><span style='color:#006e28;'> this_offset=</span><span style='color:#aa0000;'>&quot;9449&quot;</span><span style='color:#006e28;'> type=</span><span style='color:#aa0000;'>&quot;artificial&quot;</span> <b>/&gt;</b>\n",
    "<b>&lt;/document</b><b>&gt;</b>\n",
    "</pre>\n",
    "</body>\n",
    "        \n",
    "Having this attributes values in advance, how do we calculate the percentage of a sentence inside the fragment?\n",
    "\n",
    "Making a mathematical reasoning, it can be found that in the three cases indicated in figure 1 the general formula to solve this problem will be:\n",
    "\n",
    "$$\\%\\,of\\,sentence_{K}\\,that\\,belongs\\,to\\,the\\,fragment_X = \\frac{min(L_K,L_X) - max(Offset_K,Offset_X)}{L_K-Offset_K}$$\n",
    "\n",
    "Where $L_K = Offset_K + Length_K$. That means the position of the last character that belongs to the $sentence_K$.\n",
    "\n",
    "<body> \n",
    "    <br>\n",
    "<center><strong>Diagram that shows possible situations after Sentence Normalization</strong></center></br>\n",
    "<table border=0 cellspacing=10> \n",
    "    <caption align=\"bottom\"> </br><em>Figure 2.4.1: Percentage of a sentence belonging to a fragment. Three different possibilities.</em>\n",
    "    </caption> \n",
    "<tr align=\"center\">\n",
    "    <th> <img src=\"imgs/PercentSentBelongFragment.jpg\" height=200px width=300px alt=\"*\" \n",
    "        align=\"center\"> </th>\n",
    "</tr>\n",
    "</table>\n",
    "</body>\n",
    "\n",
    "To avoid the performance problem of repeating these calculations for one single document (which appears in more than one pair) we need to keep the $\\%\\,of\\,sentence_{K}\\,that\\,belongs\\,to\\,the\\,fragment_X$ related to the sentence, thus creating one new text per case based on a new structure (all values are numerical, ideal to load them in a numpy array):\n",
    "\n",
    "    /norm/quality/suspicious-document00007-source-document00382.xml\n",
    "   $(id_{sentence_P\\,susp},offset_{sentence_P},offset+length_{sentence_P},\\%\\,sentence_{P}\\, \\in\\,susp_{fragment\\,X},id_{fragment\\,X})$\n",
    "   \n",
    "   $\\hspace{2cm}\\vdots$ \n",
    "   \n",
    "   $(id_{sentence_Q\\,src},offset_{sentence_Q},offset+length_{sentence_Q},\\%\\,sentence_{Q}\\, \\in\\,src_{fragment\\,X},id_{fragment\\,X})$\n",
    "   \n",
    "   $\\hspace{2cm}\\vdots$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts import PANXml_Reader\n",
    "\n",
    "def calcPercent(file, fragmentOffset, fragmentLen, fragmentID, thresh):  \n",
    "    percent = [];case_percent = []\n",
    "    with open(file) as doc:\n",
    "        for num,line in enumerate(doc):\n",
    "            ID,sent,sentOffset,sentLen = line.split('\\t')\n",
    "            sentOffset = int(sentOffset)\n",
    "            sentLen = int(sentLen) + sentOffset\n",
    "            if sentOffset < fragmentLen and sentLen > fragmentOffset:\n",
    "                \n",
    "                perc = (min(sentLen,fragmentLen)-max(sentOffset,fragmentOffset))/float(sentLen-sentOffset)\n",
    "                \n",
    "                #if perc >= 0:\n",
    "                percent.append(ID+'\\t'\n",
    "                               +str(sentOffset)+'\\t'+str(sentLen)+'\\t'\n",
    "                               +str(perc)+'\\t'\n",
    "                               +str(fragmentID+1)+file[-9:-4])\n",
    "\n",
    "                case_percent.append(perc)\n",
    "    \n",
    "    return percent, case_percent\n",
    "\n",
    "def calc_sentPercentCase(inputfileName,alignedCollectionPath, \n",
    "                         xmlColecctionPath,\n",
    "                         threshold=0.3,\n",
    "                         printout=False):\n",
    "    \n",
    "    susp, src = inputfileName.split()\n",
    "    xmlDoc = PANXml_Reader(xmlColecctionPath+susp[:-4]+'-'+src[:-4]+'.xml')\n",
    "    fragmentList = xmlDoc.parser()\n",
    "    result = []; case_perc = []\n",
    "    \n",
    "    #The non-plagiarism cases have an empty XML, the next line is to avoid thems\n",
    "    if len(fragmentList) > 0:\n",
    "        files = [alignedCollectionPath+'susp/'+susp,alignedCollectionPath+'src/'+src]\n",
    "        for i,file in enumerate(files):\n",
    "            for fragmentID,fragment in enumerate(fragmentList):\n",
    "                if i == 0: #if the case is the susp case\n",
    "                    Ox = int(fragment.suspOffset)\n",
    "                    Lx = Ox+int(fragment.suspLength)\n",
    "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
    "                    result.extend(a);case_perc.extend(b)\n",
    "                    \n",
    "                else: #if the case is the src case\n",
    "                    Ox = int(fragment.srcOffset)\n",
    "                    Lx = Ox+int(fragment.srcLength)\n",
    "                    a,b = calcPercent(file,Ox,Lx,fragmentID,threshold)\n",
    "                    result.extend(a);case_perc.extend(b)\n",
    "                if printout:\n",
    "                    print('Case %d, Offset=%d, Length=%d' % (fragmentID, Ox, Lx-Ox))\n",
    "\n",
    "        #Write in a single doc the percent result line by line for both docs of the case\n",
    "        report = open(alignedCollectionPath+'quality/'+inputfileName, 'w')\n",
    "        if printout:\n",
    "            print('Sents\\tOffset\\tLength\\t%InFrag\\tFragID')\n",
    "        for line in result:\n",
    "            if printout:\n",
    "                print(line)\n",
    "            report.write(line+'\\n')\n",
    "        report.close()\n",
    "\n",
    "        data = np.array(case_perc,dtype=np.float64)\n",
    "\n",
    "        return float(data.mean()), case_perc\n",
    "    \n",
    "    else:\n",
    "        return 0.0, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for a case with only one pair of fragments\n",
    "\n",
    "Please create \"quality\" folder inside aligned folder before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0, Offset=1021, Length=502\n",
      "Case 0, Offset=0, Length=687\n",
      "Sents\tOffset\tLength\t%InFrag\tFragID\n",
      "12\t972\t1064\t0.4673913043478261\t100427\n",
      "13\t1064\t1188\t1.0\t100427\n",
      "14\t1188\t1413\t1.0\t100427\n",
      "15\t1413\t1503\t1.0\t100427\n",
      "16\t1503\t1526\t0.8695652173913043\t100427\n",
      "0\t0\t65\t1.0\t101618\n",
      "1\t65\t106\t1.0\t101618\n",
      "2\t106\t137\t1.0\t101618\n",
      "3\t137\t267\t1.0\t101618\n",
      "4\t267\t420\t1.0\t101618\n",
      "5\t420\t584\t1.0\t101618\n",
      "6\t584\t663\t1.0\t101618\n",
      "7\t663\t687\t1.0\t101618\n",
      "Case Percent Quality = 0.9489966555183946\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'suspicious-document00427.txt source-document01618.txt'\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
    "print(\"Case Percent Quality =\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for a case with more than one pair of fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0, Offset=5824, Length=272\n",
      "Case 1, Offset=21870, Length=279\n",
      "Case 0, Offset=0, Length=801\n",
      "Case 1, Offset=879, Length=358\n",
      "Sents\tOffset\tLength\t%InFrag\tFragID\n",
      "51\t5824\t5896\t1.0\t101064\n",
      "52\t5896\t5935\t1.0\t101064\n",
      "53\t5935\t5998\t1.0\t101064\n",
      "54\t5998\t6066\t1.0\t101064\n",
      "55\t6066\t6097\t0.967741935483871\t101064\n",
      "240\t21747\t21966\t0.4383561643835616\t201064\n",
      "241\t21966\t22118\t1.0\t201064\n",
      "242\t22118\t22150\t0.96875\t201064\n",
      "0\t0\t49\t1.0\t100671\n",
      "1\t49\t210\t1.0\t100671\n",
      "2\t210\t470\t1.0\t100671\n",
      "3\t470\t602\t1.0\t100671\n",
      "4\t602\t710\t1.0\t100671\n",
      "5\t710\t802\t0.9891304347826086\t100671\n",
      "7\t879\t1127\t1.0\t200671\n",
      "8\t1127\t1141\t1.0\t200671\n",
      "9\t1141\t1190\t1.0\t200671\n",
      "10\t1190\t1239\t0.9591836734693877\t200671\n",
      "Case Percent Quality = 0.9623979004510793\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'suspicious-document01064.txt source-document00671.txt'\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "A, B = calc_sentPercentCase(inputFile, alignedCollectionPath, xmlColecctionPath,printout=True)\n",
    "print(\"Case Percent Quality =\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for the whole Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5250451564788818\n"
     ]
    }
   ],
   "source": [
    "#Generate the percent for all sentences in every preprocessed case\n",
    "#Load all cases in pairs\n",
    "alignedCollectionPath = 'data/aligned/'\n",
    "xmlColecctionPath = 'data/orig/xml/'\n",
    "\n",
    "import time\n",
    "\n",
    "norm_percent = []\n",
    "sent_perc = []\n",
    "threshold=0.3\n",
    "\n",
    "init = time.time()\n",
    "for line in open('data/aligned/aligned_pairs'):\n",
    "    perct, sent = calc_sentPercentCase(line[:-1], alignedCollectionPath, \n",
    "                                       xmlColecctionPath,threshold)\n",
    "    norm_percent.append(perct)\n",
    "    sent_perc.extend(sent)\n",
    "    timef = time.time() - init\n",
    "print(timef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.93982923 0.99764652 0.7652065  0.96741266 0.89646236\n",
      " 0.89776727 0.93668272 0.94802847 0.99578731 0.99778318 0.99827709\n",
      " 0.76641118 0.96892604 0.99805388]\n",
      "[0.         0.         0.         ... 0.98975591 0.88768371 0.99013629]\n",
      "Total useful sentences: 13376\n"
     ]
    }
   ],
   "source": [
    "len(norm_percent)\n",
    "quality_norm_vector = np.array(norm_percent,dtype=np.float64)\n",
    "print(quality_norm_vector[990:1011])\n",
    "print(quality_norm_vector)\n",
    "print('Total useful sentences:',len(sent_perc))\n",
    "quality_sent_norm = np.array(sent_perc,dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673992915065034"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_norm_vector[1000:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Normalization Quality Measure\n",
    "\n",
    "$Quality = \\frac{\\sum_{k=0}^n (\\%\\,of\\,sentence_k\\,\\,inside\\,fragment>\\mu)}{total\\,sentences\\,with\\,\\% > \\mu}$ \n",
    "\n",
    "Where $n+1$ is the total sentences of the analised fragment, and $\\mu$ is the minimum percentage of a sentence inside the fragment which classifies that sentence as belonging to the case fragment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Type: <class 'numpy.float64'> \n",
      "Quality mean: 0.4840843570093619 .\n",
      "New data type is needed: DataFrame (DF).\n",
      "DF Quality based on cases-quality-average: 0.4841\n",
      "Total sentences: 13376\n",
      "DF Quality based on total sentences with percent > 0.30 inside de case: 0.9734\n",
      "# of normalized sentences with +90.00% of its size inside its respectively case-frag: 12794\n",
      "# de oraciones norm con +50.00% de su tamaÃ±o fuera del caso: 318\n",
      "Quality based on miu: 0.9565\n",
      "Quality based on miu2: 0.9762\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "miu = 0.9\n",
    "miu_vector = []\n",
    "miu2 = 0.5\n",
    "miu_vector2 = []\n",
    "\n",
    "Quality = quality_norm_vector.mean()\n",
    "#this is to show that np-array does not handle NaN values, \n",
    "#then DataFrame object convertion is needed\n",
    "print ('Quality Type:',type(Quality),'\\nQuality mean:',Quality,\n",
    "       '.\\nNew data type is needed: DataFrame (DF).') \n",
    "\n",
    "P = DataFrame(quality_norm_vector, columns=['percent'])\n",
    "Qn = P.mean()\n",
    "print('DF Quality based on cases-quality-average: %.4f' % (Qn['percent']))\n",
    "\n",
    "print('Total sentences:',len(sent_perc))\n",
    "Q = DataFrame(quality_sent_norm, columns=['percent'])\n",
    "Qs = Q.mean()\n",
    "print('DF Quality based on total sentences with percent > %.2f inside de case: %.4f' % (threshold,Qs['percent']))\n",
    "\n",
    "for sent in quality_sent_norm:\n",
    "    if sent > miu:\n",
    "        miu_vector.append(sent)\n",
    "\n",
    "for sent in quality_sent_norm:\n",
    "    if sent < miu2:\n",
    "        miu_vector2.append(sent)\n",
    "        \n",
    "print('# of normalized sentences with +%.2f%% of its size' % (miu*100),\n",
    "      'inside its respectively case-frag: %d' % (len(miu_vector)))\n",
    "print('# of norm sentences with +%.2f%% of its size outside of the case: %d' % \n",
    "      (100-miu2*100,len(miu_vector2)))\n",
    "print('Quality based on miu: %.4f' % (len(miu_vector)/len(Q)))\n",
    "print('Quality based on miu2: %.4f' % (1.0-len(miu_vector2)/len(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.243421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percent\n",
       "0  1.000000\n",
       "1  1.000000\n",
       "2  1.000000\n",
       "3  1.000000\n",
       "4  0.243421"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The data obtained with aligner1.0 is good. The _quality factor_ means that the normalization process redefines the 95% of the sentences with more of the half of their length inside the fragment. The 84% has more than 90% of the sentences inside the fragment defined by the xml document. Only 7% has less than 30% outside the fragment defined by corpus's xml files.\n",
    "\n",
    "With aligner2.0 all the numbers are better. 95% of the sentences has 90% of its length inside the fragment. Only 2.6% has less than 30% out of the fragment. The implementation of the second version of jaccard and aligner based on chars and word comparisons was good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Develop new quality measures based on what was learned here.\n",
    "\n",
    "2. Describe their formulas and algorithms.\n",
    "\n",
    "3. Change the distance measure, and a different version of _preprocess_ lib and interpret  the result. Test the contractions and abbreviations replacement and analyse the result.\n",
    "\n",
    "4. Implement the normalization with nltk, pattern or spacy, and analyse the results. Comment on the differences for future experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
