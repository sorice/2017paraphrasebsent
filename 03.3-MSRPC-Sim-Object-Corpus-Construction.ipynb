{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for Paper *Paraphrase Beyond Sentence*. Source and license info is on [GitHub](https://github.com/sorice/2017paraphrasebsent/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Corpus of Similarity Objects based on MSRPC\n",
    "\n",
    "After having a good set of text similarity functions, the next urgent resource is the corpus of similarity objects to use into machine learning sklearn python library. Most frecuently format inside sklearn datasets is csv. \n",
    "\n",
    "The ARFF formats used here, is for __Weka__ software, made in Java. The original intention was to compare performance in both platforms, but this objective never was accomplished, also to compare the results of the stage of \"feature extraction\" by Weka and Sklearn methods.\n",
    "\n",
    "The next set of cells implement the generation of a corpus of similarity vectors. This cells have been converted into script module functions for future uses in the following notebooks, but as the main objective of this collection is to teach, we let the first successfull implementation for student questions and quiz implementations.\n",
    "\n",
    "**Note:** The above mention functions are *script.datasets.msrpc_to_csv*,*script.datasets.msrpc_to_arff*.\n",
    "\n",
    "**Note:** Textsim package can be download from [github](https://github.com/sorice/textsim).\n",
    "\n",
    "__2020 Note:__ The MSRPC arff files used in the next cells were obtained with a JAVA software of Las Tunas University in Cuba, this software is not longer available. So the last cell is dedicated to obtain the same similarity vectors or some extended ones based only in python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.utils import Bunch\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series, read_csv, read_table\n",
    "import textsim\n",
    "from textsim.utils import calc_all\n",
    "import arff\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading original MSRPC\n",
    "\n",
    "The __objective__ of these sections are to teach how to program the loader of a corpus, similar to sklearn.\n",
    "\n",
    "The original Microsoft Research Paraphrase Corpus is organized in this schema:\n",
    "\n",
    "__class__, __ID of String A__, __ID of String B__, __String A__, __String B__\n",
    "\n",
    "This corpus can be downloaded in [http://research.microsoft.com/en-us/downloads](http://research.microsoft.com/en-us/downloads) also could be found in many repositories in github like [THIS](https://github.com/wasiahmad/paraphrase_identification/tree/master/datasets/msr-paraphrase-corpus)\n",
    "\n",
    "Originaly MSRPC was divided in two subsets: test and train. The author of this collection did a unic file (_data/msrpc.txt_) for future experiments with different configurations. It is possible to split this unic file in test/train subsets for __model evaluation__ using different strategies like _stratified Kfold_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1725/1725 [06:41<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 401.63975954055786\n",
      "Exceptions: 54 \n",
      "Values: [16, 35, 48, 143, 176, 217, 228, 251, 279, 281, 322, 378, 408, 429, 430, 446, 475, 525, 572, 576, 588, 662, 697, 701, 731, 745, 766, 773, 824, 843, 897, 924, 953, 1003, 1014, 1019, 1039, 1059, 1077, 1151, 1186, 1209, 1216, 1236, 1413, 1425, 1467, 1470, 1527, 1607, 1663, 1666, 1677, 1717]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Read Paraphrase Corpus\n",
    "df = read_table('data/MSRPC-2004/msr_paraphrase_test.txt',sep='\\t')\n",
    "data = []\n",
    "distances = []\n",
    "exceptions = []\n",
    "\n",
    "ti = time.time()\n",
    "#Open vector ARFF similarity feature corpus\n",
    "with open('data/MSRPC-2004/msrpc_test_textsim-42fb.arff','w') as corpus: \n",
    "    corpus.write('@relation paraphrase\\n\\n')\n",
    "    for distance in sorted(textsim.__all_distances__.keys()):\n",
    "        corpus.write('@attribute '+distance+' numeric\\n')\n",
    "        distances.append(distance)\n",
    "    corpus.write('@attribute '+'id'+' integer\\n')\n",
    "    distances.append('id')\n",
    "    corpus.write('@attribute class {yes,no}\\n\\n')\n",
    "    distances.append('class')\n",
    "    corpus.write('@data\\n')\n",
    "    \n",
    "    for row in tqdm(range(len(df))):\n",
    "        clase, ide1, ide2, sent1, sent2 = df.xs(row)\n",
    "        try:\n",
    "            obj = calc_all(sent1,sent2)[2:]\n",
    "            sec = ''\n",
    "            for item in obj:\n",
    "                if str(item) == 'nan':\n",
    "                    sec += '?,'\n",
    "                else:\n",
    "                    sec += str(item)+','\n",
    "            sec += str(row) #append id for future analysis after classification\n",
    "            obj.append(row)\n",
    "            if clase:\n",
    "                corpus.write(sec+',yes\\n')\n",
    "                obj.append('yes')\n",
    "            else:\n",
    "                corpus.write(sec+',no\\n')\n",
    "                obj.append('no')\n",
    "            data.append(obj)\n",
    "            \n",
    "        except:\n",
    "            exceptions.append(row)\n",
    "\n",
    "tf = time.time()-ti\n",
    "print('Total time:',tf)\n",
    "print('Exceptions:',len(exceptions),'\\nValues:',exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Runs*\n",
    "\n",
    "1. time = 379.49773502349854, valid_data = 1671\n",
    "2. time = 381, valid_data = 1671\n",
    "3. time = 415.97, valid_data = 1671 (last version that generate a correct MSRP ARFF file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Data Generation\n",
    "\n",
    "Only exec the next block if you exec first the above block, the __data__ object must exist to execute well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_csv(file_path='data/MSRPC-2004/msrpc_test_textsim-42f.csv'):\n",
    "    with open(file_path,'w') as corpus:\n",
    "        corpus.write(str(len(data))+',')\n",
    "        corpus.write(str(len(distances)-1)+',')\n",
    "        corpus.write('Paraph,Non,')\n",
    "        for distance in distances:\n",
    "            corpus.write(distance+',')\n",
    "        corpus.write('\\n')\n",
    "        for instance in data:\n",
    "            corpus.write(str(instance)[1:-1]+'\\n')\n",
    "    return\n",
    "            \n",
    "corpus_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcial conclusions\n",
    "\n",
    "* The csv corpus estandard wich is loaded with Bunch, must have some metadatas in the first two row, like, the len of cases, the amount of features, etc.\n",
    "* Is very important to have distances in cython, because the process could be much much fast.\n",
    "* Do not attempt to do this in a laptop, without the cython implementation. Is to slow!\n",
    "* A GPU version could be more suitable to get results in minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating complete MSRPC from txt to csv\n",
    "\n",
    "This step is only needed once. Is a slow process so the recommedation is to doit in a powerfull machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.datasets import msrpc_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 826.1204106807709\n"
     ]
    }
   ],
   "source": [
    "ti = time.time()\n",
    "msrpc_to_csv('data/MSRPC-2004/msrpc.txt')\n",
    "tf = time.time()-ti\n",
    "print('Total time:',tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playful Programming\n",
    "\n",
    "### Example of Parallel version of Corpus construction code\n",
    "\n",
    "Code of parallel_process funct geted from [Dans Shiebler Blog](http://danshiebler.com/2016-09-14-parallel-progress-bar/).\n",
    "This version code only generates CSV format. For ARFF format see __scripts.parallel_msrpc_arff.py__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import textsim\n",
    "from textsim.utils import calc_all\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MSRPC.txt\n",
    "\n",
    "Like in __scripts.datasets.msrpc_to_csv()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSRPC loaded in  13.015718698501587  seconds\n"
     ]
    }
   ],
   "source": [
    "#Read Paraphrase Corpus in TXT format\n",
    "file_path = 'data/MSRPC-2004/msrpc.txt'\n",
    "\n",
    "ti = time.time()\n",
    "df = DataFrame(columns=['class','id1','id2','sent1', 'sent2'])\n",
    "loading_except = []\n",
    "\n",
    "with open(file_path) as corp:\n",
    "    count = 0\n",
    "    for row in corp:\n",
    "        try:\n",
    "            obj = row.split('\\t')\n",
    "            if count == 0: #do not process the line 0\n",
    "                count+=1\n",
    "                pass\n",
    "            else: #do distance calculation in the rest\n",
    "                df = df.append(Series(obj, index=df.columns), ignore_index=True)\n",
    "                count+=1\n",
    "        except:\n",
    "            loading_except.append(count)\n",
    "            \n",
    "tm = time.time()-ti\n",
    "print('MSRPC loaded in ', tm, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.80k/5.80k [05:21<00:00, 18.0it/s]\n",
      "5798it [00:00, 634502.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 324.66868019104004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from scripts.parallel import parallel_process, return_obj\n",
    "import time\n",
    "\n",
    "distances = []\n",
    "exceptions = []\n",
    "ti = time.time()\n",
    "#Open target file for new corpus & write\n",
    "with open('data/MSRPC-2004/parallel_msrpc.csv','w') as corpus:    \n",
    "    for distance in sorted(textsim.__all_distances__.keys()):\n",
    "        distances.append(distance)\n",
    "    distances.append('id')\n",
    "    distances.append('class')\n",
    "    corpus.write(','.join(str(elem) for elem in distances)+'\\n')\n",
    "\n",
    "#Parallel trick for this problem\n",
    "arr = [{'row':i, 'sent1':df.xs(i)[3], 'sent2':df.xs(i)[4], 'clase':df.xs(i)[0]} for i in range(len(df))]\n",
    "exceptions = parallel_process(arr, return_obj, use_kwargs=True)\n",
    "    \n",
    "tf = time.time()-ti\n",
    "print('Total time:',tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptions: 15 \n",
      "Values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>48</th>\n",
       "      <th>143</th>\n",
       "      <th>697</th>\n",
       "      <th>1019</th>\n",
       "      <th>1470</th>\n",
       "      <th>2695</th>\n",
       "      <th>2826</th>\n",
       "      <th>3087</th>\n",
       "      <th>3394</th>\n",
       "      <th>3658</th>\n",
       "      <th>3920</th>\n",
       "      <th>3990</th>\n",
       "      <th>4308</th>\n",
       "      <th>4403</th>\n",
       "      <th>4451</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>2695.0</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>3087.0</td>\n",
       "      <td>3394.0</td>\n",
       "      <td>3658.0</td>\n",
       "      <td>3920.0</td>\n",
       "      <td>3990.0</td>\n",
       "      <td>4308.0</td>\n",
       "      <td>4403.0</td>\n",
       "      <td>4451.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   48     143    697     1019    1470    2695    2826    3087    3394    3658  \\\n",
       "0  48.0  143.0  697.0  1019.0  1470.0  2695.0  2826.0  3087.0  3394.0  3658.0   \n",
       "\n",
       "     3920    3990    4308    4403    4451  \n",
       "0  3920.0  3990.0  4308.0  4403.0  4451.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_excepts = DataFrame(exceptions)\n",
    "df_excepts = df_excepts.dropna()\n",
    "print('Exceptions:',len(df_excepts),'\\nValues:')\n",
    "df_excepts.T #Transpose only to get a better visualization of exceptions in one row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original exceptions in msrpc_test subcorpus (len = 1725 cases)\n",
    "\n",
    "    exceptions = [16, 35, 48, 143, 176, 217, 228, 251, 279, 281, 322, 378, 408, 429, 430, 446, 475, 525, 572, 576, 588, 662, 697, 701, 731, 745, 766, 773, 824, 843, 897, 924, 953, 1003, 1014, 1019, 1039, 1059, 1077, 1151, 1186, 1209, 1216, 1236, 1413, 1425, 1467, 1470, 1527, 1607, 1663, 1666, 1677, 1717]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 143, 697, 1019, 1470, 2695, 2826, 3087, 3394, 3658, 3920, 3990, 4308, 4403, 4451]\n"
     ]
    }
   ],
   "source": [
    "print(list(df_excepts.index)) #to get the same values but in int type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* tqdm library is excellent for visualizing bar progress of single core\n",
    "  and parallel code solutions, and let you know exactly how many\n",
    "  instances still pendent in your actual cell running.\n",
    "* Original parallel solution in DELL i7 first generation laptop = speed up 1.7x (6.7 minutes are now 4 minutes), tested only in msrpc_test subcorpus len = 1725 cases.\n",
    "* Average speed up in second parallel solution in HP i7 8th generation laptop = 2.2x (E.g. 826 seconds in non-parallel are = in this solution to 376 seconds).\n",
    "* The change in reading method of msrpc.txt show a big gaining in exceptions handling: \n",
    "    - 54 in msrpc_test using __read_csv__ method.\n",
    "    - 15 in the whole corpus using __open().read().split('\\t')__.\n",
    "* Cython most improve the distances calculation time.\n",
    "* The numbers shows that DELL i7 1thsG is 0.5x slower than HP 8thG, with the same amount of jobs = 4.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Read pandas.io module to get a strong idea about how to play with in/out operations.\n",
    "* Read scipy.io module to undestand arff format reading implemented iside this library.\n",
    "* Implement cython measures, because knowledge and corpus measures are very slow. The firt one based on Wornet are very complicated to use in a 10K cases corpus. The second needs big corpus and Neural Network training for get some results.\n",
    "* It is important to review the distances or algorithms implemented in SpaCy to calculate if it is more suitable to use some of those, and avoid other packages distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Make a parallel version of msrpc_to_csv function.\n",
    "2. Make a GPU compatible version of msrpc_to_csv function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "<a id='Scipy2012'></a>\n",
    "* [Scipy2012] Scipy Community, Manual \"SciPy Reference Guide\", 2012."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
